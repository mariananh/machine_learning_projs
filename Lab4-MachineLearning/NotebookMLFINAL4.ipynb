{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-26T17:35:17.463848Z",
     "iopub.status.busy": "2024-10-26T17:35:17.463209Z",
     "iopub.status.idle": "2024-10-26T17:35:18.761279Z",
     "shell.execute_reply": "2024-10-26T17:35:18.760250Z",
     "shell.execute_reply.started": "2024-10-26T17:35:17.463789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/crater-segmentation/Ytrain2_b.npy\n",
      "/kaggle/input/crater-segmentation/Xtrain2_b.npy\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T17:35:19.431744Z",
     "iopub.status.busy": "2024-10-26T17:35:19.430746Z",
     "iopub.status.idle": "2024-10-26T17:35:19.558191Z",
     "shell.execute_reply": "2024-10-26T17:35:19.557228Z",
     "shell.execute_reply.started": "2024-10-26T17:35:19.431689Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.load('Xtrain2_b.npy')\n",
    "y = np.load('Ytrain2_b.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T17:35:20.839116Z",
     "iopub.status.busy": "2024-10-26T17:35:20.838766Z",
     "iopub.status.idle": "2024-10-26T17:35:20.844054Z",
     "shell.execute_reply": "2024-10-26T17:35:20.843154Z",
     "shell.execute_reply.started": "2024-10-26T17:35:20.839080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(547, 2304) (547, 2304)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-26T17:35:23.594055Z",
     "iopub.status.busy": "2024-10-26T17:35:23.593697Z",
     "iopub.status.idle": "2024-10-26T18:40:55.157357Z",
     "shell.execute_reply": "2024-10-26T18:40:55.156468Z",
     "shell.execute_reply.started": "2024-10-26T17:35:23.594018Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n",
      "Starting data preparation...\n",
      "Training set size: (437, 2304), Test set size: (110, 2304)\n",
      "Initializing model and processor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5f7ca7d65d47e1ae12ae6c4060779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b511bfada1314464afae9ae7ba8e4261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/70.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66c730368724a2b96b805f503a96f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/179M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29/458524710.py:156: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 0.7261\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.83 GB\n",
      "GPU 1 memory allocated: 0.01 GB\n",
      "Epoch: 0, Batch: 5, Loss: 0.4513\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 10, Loss: 0.5543\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 15, Loss: 0.4136\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 20, Loss: 0.4891\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 25, Loss: 0.4840\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 30, Loss: 0.3865\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 35, Loss: 0.5339\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 40, Loss: 0.4538\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 45, Loss: 0.3010\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 0, Batch: 50, Loss: 0.3315\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.84 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Average Training Loss: 0.4499\n",
      "Validation Balanced Accuracy: 0.7450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 0.3712\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 5, Loss: 0.5598\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 10, Loss: 0.3997\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 15, Loss: 0.4874\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 20, Loss: 0.2994\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 25, Loss: 0.3282\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 30, Loss: 0.2656\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 35, Loss: 0.3113\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 40, Loss: 0.4000\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 45, Loss: 0.3646\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 1, Batch: 50, Loss: 0.3761\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Training Loss: 0.3689\n",
      "Validation Balanced Accuracy: 0.7545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 0, Loss: 0.2753\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 5, Loss: 0.3546\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 10, Loss: 0.4254\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 15, Loss: 0.2760\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 20, Loss: 0.3517\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 25, Loss: 0.2939\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 30, Loss: 0.4247\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 35, Loss: 0.3917\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 40, Loss: 0.2862\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 45, Loss: 0.3140\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 2, Batch: 50, Loss: 0.3643\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Average Training Loss: 0.3291\n",
      "Validation Balanced Accuracy: 0.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 0, Loss: 0.4628\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 5, Loss: 0.2947\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 10, Loss: 0.2696\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 15, Loss: 0.2186\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 20, Loss: 0.3052\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 25, Loss: 0.3098\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 30, Loss: 0.3534\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 35, Loss: 0.3300\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 40, Loss: 0.4096\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 45, Loss: 0.3025\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 3, Batch: 50, Loss: 0.2416\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Average Training Loss: 0.3109\n",
      "Validation Balanced Accuracy: 0.7841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Batch: 0, Loss: 0.2991\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 5, Loss: 0.2654\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 10, Loss: 0.2897\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 15, Loss: 0.3117\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 20, Loss: 0.3131\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 25, Loss: 0.3357\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 30, Loss: 0.4900\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 35, Loss: 0.2081\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 40, Loss: 0.2478\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 45, Loss: 0.3746\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 4, Batch: 50, Loss: 0.2449\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Average Training Loss: 0.2999\n",
      "Validation Balanced Accuracy: 0.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Batch: 0, Loss: 0.2300\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 5, Loss: 0.4140\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 10, Loss: 0.2165\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 15, Loss: 0.3265\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 20, Loss: 0.3028\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 25, Loss: 0.2638\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 30, Loss: 0.3101\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 35, Loss: 0.2592\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 40, Loss: 0.2994\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 45, Loss: 0.3593\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 5, Batch: 50, Loss: 0.2435\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Average Training Loss: 0.3038\n",
      "Validation Balanced Accuracy: 0.7938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Batch: 0, Loss: 0.2138\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 5, Loss: 0.3610\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 10, Loss: 0.2038\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 15, Loss: 0.1880\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 20, Loss: 0.5354\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 25, Loss: 0.3677\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 30, Loss: 0.3416\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 35, Loss: 0.2223\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 40, Loss: 0.2582\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 45, Loss: 0.3051\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 6, Batch: 50, Loss: 0.3540\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Average Training Loss: 0.2898\n",
      "Validation Balanced Accuracy: 0.7904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Batch: 0, Loss: 0.2527\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 5, Loss: 0.2298\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 10, Loss: 0.2564\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 15, Loss: 0.2272\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 20, Loss: 0.3271\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 25, Loss: 0.1518\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 30, Loss: 0.2217\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 35, Loss: 0.1836\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 40, Loss: 0.1958\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 45, Loss: 0.3776\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 7, Batch: 50, Loss: 0.2824\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Average Training Loss: 0.2659\n",
      "Validation Balanced Accuracy: 0.8048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Batch: 0, Loss: 0.1750\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 5, Loss: 0.3590\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 10, Loss: 0.2163\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 15, Loss: 0.2332\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 20, Loss: 0.2019\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 25, Loss: 0.1935\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 30, Loss: 0.2897\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 35, Loss: 0.2250\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 40, Loss: 0.2321\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 45, Loss: 0.2064\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 8, Batch: 50, Loss: 0.2346\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Average Training Loss: 0.2519\n",
      "Validation Balanced Accuracy: 0.7941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 0, Loss: 0.2328\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 5, Loss: 0.2594\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 10, Loss: 0.1894\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 15, Loss: 0.1961\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 20, Loss: 0.2288\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 25, Loss: 0.1603\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 30, Loss: 0.2483\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 35, Loss: 0.2394\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 40, Loss: 0.3021\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 45, Loss: 0.1851\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 9, Batch: 50, Loss: 0.1812\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Average Training Loss: 0.2378\n",
      "Validation Balanced Accuracy: 0.7928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Batch: 0, Loss: 0.2207\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 5, Loss: 0.1790\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 10, Loss: 0.2036\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 15, Loss: 0.2062\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 20, Loss: 0.2943\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 25, Loss: 0.2005\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 30, Loss: 0.1409\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 35, Loss: 0.1897\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 40, Loss: 0.2058\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 45, Loss: 0.2054\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 10, Batch: 50, Loss: 0.1681\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Average Training Loss: 0.2216\n",
      "Validation Balanced Accuracy: 0.8132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Batch: 0, Loss: 0.1520\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 5, Loss: 0.2775\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 10, Loss: 0.2029\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 15, Loss: 0.2508\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 20, Loss: 0.1868\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 25, Loss: 0.1976\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 30, Loss: 0.2173\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 35, Loss: 0.2667\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 40, Loss: 0.1723\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 45, Loss: 0.2406\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 11, Batch: 50, Loss: 0.1917\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "Average Training Loss: 0.2071\n",
      "Validation Balanced Accuracy: 0.8095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Batch: 0, Loss: 0.1833\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 5, Loss: 0.2379\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 10, Loss: 0.1340\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 15, Loss: 0.1483\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 20, Loss: 0.1840\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 25, Loss: 0.1142\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 30, Loss: 0.2269\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 35, Loss: 0.2663\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 40, Loss: 0.2176\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 45, Loss: 0.2361\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 12, Batch: 50, Loss: 0.2143\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "Average Training Loss: 0.2007\n",
      "Validation Balanced Accuracy: 0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Batch: 0, Loss: 0.1982\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 5, Loss: 0.2125\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 10, Loss: 0.1657\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 15, Loss: 0.1704\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 20, Loss: 0.2015\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 25, Loss: 0.2362\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 30, Loss: 0.2283\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 35, Loss: 0.1602\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 40, Loss: 0.2772\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 45, Loss: 0.1805\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 13, Batch: 50, Loss: 0.2193\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "Average Training Loss: 0.1983\n",
      "Validation Balanced Accuracy: 0.8158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Batch: 0, Loss: 0.2334\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 5, Loss: 0.2096\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 10, Loss: 0.1846\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 15, Loss: 0.1877\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 20, Loss: 0.1525\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 25, Loss: 0.2986\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 30, Loss: 0.1995\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 35, Loss: 0.2094\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 40, Loss: 0.1405\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 45, Loss: 0.1810\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 14, Batch: 50, Loss: 0.1772\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "Average Training Loss: 0.1927\n",
      "Validation Balanced Accuracy: 0.8123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Batch: 0, Loss: 0.2350\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 5, Loss: 0.2354\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 10, Loss: 0.1837\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 15, Loss: 0.1729\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 20, Loss: 0.2163\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 25, Loss: 0.2420\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 30, Loss: 0.1636\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 35, Loss: 0.2521\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 40, Loss: 0.1795\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 45, Loss: 0.1745\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 15, Batch: 50, Loss: 0.2028\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "Average Training Loss: 0.1998\n",
      "Validation Balanced Accuracy: 0.8137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Batch: 0, Loss: 0.1767\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 5, Loss: 0.1599\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 10, Loss: 0.1960\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 15, Loss: 0.2628\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 20, Loss: 0.2031\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 25, Loss: 0.1514\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 30, Loss: 0.2204\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 35, Loss: 0.1691\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 40, Loss: 0.1301\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 45, Loss: 0.1902\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 16, Batch: 50, Loss: 0.2255\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n",
      "Average Training Loss: 0.1912\n",
      "Validation Balanced Accuracy: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Batch: 0, Loss: 0.1477\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 5, Loss: 0.1794\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 10, Loss: 0.2498\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 15, Loss: 0.2111\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 20, Loss: 0.2012\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 25, Loss: 0.2128\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 30, Loss: 0.1766\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 35, Loss: 0.1564\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 40, Loss: 0.1658\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 45, Loss: 0.1447\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 17, Batch: 50, Loss: 0.1973\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n",
      "Average Training Loss: 0.1867\n",
      "Validation Balanced Accuracy: 0.8041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Batch: 0, Loss: 0.1519\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 5, Loss: 0.1865\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 10, Loss: 0.1703\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 15, Loss: 0.1657\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 20, Loss: 0.1553\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 25, Loss: 0.1644\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 30, Loss: 0.1377\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 35, Loss: 0.1869\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 40, Loss: 0.1541\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 45, Loss: 0.1468\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 18, Batch: 50, Loss: 0.1794\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n",
      "Average Training Loss: 0.1696\n",
      "Validation Balanced Accuracy: 0.8155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Batch: 0, Loss: 0.1689\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 5, Loss: 0.1574\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 10, Loss: 0.1699\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 15, Loss: 0.1947\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 20, Loss: 0.1692\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 25, Loss: 0.1600\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 30, Loss: 0.1960\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 35, Loss: 0.2668\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 40, Loss: 0.1520\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 45, Loss: 0.1481\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 19, Batch: 50, Loss: 0.2258\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n",
      "Average Training Loss: 0.1707\n",
      "Validation Balanced Accuracy: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Batch: 0, Loss: 0.1639\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 5, Loss: 0.2175\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 10, Loss: 0.1219\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 15, Loss: 0.1294\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 20, Loss: 0.1514\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 25, Loss: 0.1345\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 30, Loss: 0.1648\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 35, Loss: 0.1660\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 40, Loss: 0.1703\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 45, Loss: 0.1658\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 20, Batch: 50, Loss: 0.1211\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\n",
      "Average Training Loss: 0.1575\n",
      "Validation Balanced Accuracy: 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Batch: 0, Loss: 0.1339\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 5, Loss: 0.1815\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 10, Loss: 0.1307\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 15, Loss: 0.2019\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 20, Loss: 0.1769\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 25, Loss: 0.1151\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 30, Loss: 0.1644\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 35, Loss: 0.1052\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 40, Loss: 0.2273\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 45, Loss: 0.1667\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 21, Batch: 50, Loss: 0.1599\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\n",
      "Average Training Loss: 0.1491\n",
      "Validation Balanced Accuracy: 0.8186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Batch: 0, Loss: 0.1682\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 5, Loss: 0.1193\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 10, Loss: 0.2329\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 15, Loss: 0.1917\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 20, Loss: 0.1314\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 25, Loss: 0.1123\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 30, Loss: 0.1124\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 35, Loss: 0.1121\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 40, Loss: 0.1352\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 45, Loss: 0.1506\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 22, Batch: 50, Loss: 0.1171\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22\n",
      "Average Training Loss: 0.1424\n",
      "Validation Balanced Accuracy: 0.8195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Batch: 0, Loss: 0.1772\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 5, Loss: 0.1390\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 10, Loss: 0.1613\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 15, Loss: 0.1384\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 20, Loss: 0.1667\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 25, Loss: 0.1207\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 30, Loss: 0.1162\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 35, Loss: 0.1252\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 40, Loss: 0.1795\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 45, Loss: 0.1757\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 23, Batch: 50, Loss: 0.1971\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23\n",
      "Average Training Loss: 0.1425\n",
      "Validation Balanced Accuracy: 0.8193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Batch: 0, Loss: 0.1258\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 5, Loss: 0.1637\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 10, Loss: 0.1420\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 15, Loss: 0.1423\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 20, Loss: 0.1496\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 25, Loss: 0.1597\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 30, Loss: 0.1509\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 35, Loss: 0.1476\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 40, Loss: 0.1251\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 45, Loss: 0.1548\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 24, Batch: 50, Loss: 0.1361\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24\n",
      "Average Training Loss: 0.1365\n",
      "Validation Balanced Accuracy: 0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Batch: 0, Loss: 0.1165\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 5, Loss: 0.1180\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 10, Loss: 0.1301\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 15, Loss: 0.1394\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 20, Loss: 0.1048\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 25, Loss: 0.1274\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 30, Loss: 0.1388\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 35, Loss: 0.1325\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 40, Loss: 0.1506\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 45, Loss: 0.1120\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 25, Batch: 50, Loss: 0.1205\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25\n",
      "Average Training Loss: 0.1331\n",
      "Validation Balanced Accuracy: 0.8197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Batch: 0, Loss: 0.1016\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 5, Loss: 0.1217\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 10, Loss: 0.0999\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 15, Loss: 0.1174\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 20, Loss: 0.1222\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 25, Loss: 0.1194\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 30, Loss: 0.1736\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 35, Loss: 0.1478\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 40, Loss: 0.1151\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 45, Loss: 0.1071\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 26, Batch: 50, Loss: 0.1330\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26\n",
      "Average Training Loss: 0.1312\n",
      "Validation Balanced Accuracy: 0.8187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Batch: 0, Loss: 0.1312\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 5, Loss: 0.1523\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 10, Loss: 0.1308\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 15, Loss: 0.1675\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 20, Loss: 0.1260\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 25, Loss: 0.1073\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 30, Loss: 0.1206\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 35, Loss: 0.1506\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 40, Loss: 0.1308\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 45, Loss: 0.1201\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 27, Batch: 50, Loss: 0.1200\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27\n",
      "Average Training Loss: 0.1271\n",
      "Validation Balanced Accuracy: 0.8180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Batch: 0, Loss: 0.1473\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 5, Loss: 0.1271\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 10, Loss: 0.1500\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 15, Loss: 0.1147\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 20, Loss: 0.1138\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 25, Loss: 0.1405\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 30, Loss: 0.1648\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 35, Loss: 0.1335\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 40, Loss: 0.1306\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 45, Loss: 0.1136\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 28, Batch: 50, Loss: 0.1462\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28\n",
      "Average Training Loss: 0.1252\n",
      "Validation Balanced Accuracy: 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Batch: 0, Loss: 0.1364\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 5, Loss: 0.1156\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 10, Loss: 0.1019\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 15, Loss: 0.1260\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 20, Loss: 0.1171\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 25, Loss: 0.1041\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 30, Loss: 0.1542\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 35, Loss: 0.1203\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 40, Loss: 0.1260\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 45, Loss: 0.0980\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 29, Batch: 50, Loss: 0.1146\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29\n",
      "Average Training Loss: 0.1211\n",
      "Validation Balanced Accuracy: 0.8162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Batch: 0, Loss: 0.1249\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 5, Loss: 0.1110\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 10, Loss: 0.1107\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 15, Loss: 0.1208\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 20, Loss: 0.0809\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 25, Loss: 0.1192\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 30, Loss: 0.1143\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 35, Loss: 0.1258\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 40, Loss: 0.1584\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 45, Loss: 0.1268\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 30, Batch: 50, Loss: 0.1092\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n",
      "Average Training Loss: 0.1215\n",
      "Validation Balanced Accuracy: 0.8184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Batch: 0, Loss: 0.1183\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 5, Loss: 0.1458\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 10, Loss: 0.1361\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 15, Loss: 0.0836\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 20, Loss: 0.1286\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 25, Loss: 0.1501\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 30, Loss: 0.1087\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 35, Loss: 0.1377\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 40, Loss: 0.1177\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 45, Loss: 0.1620\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 31, Batch: 50, Loss: 0.1415\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31\n",
      "Average Training Loss: 0.1205\n",
      "Validation Balanced Accuracy: 0.8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Batch: 0, Loss: 0.1261\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 5, Loss: 0.1349\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 10, Loss: 0.0812\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 15, Loss: 0.1070\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 20, Loss: 0.1444\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 25, Loss: 0.1169\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 30, Loss: 0.1145\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 35, Loss: 0.1144\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 40, Loss: 0.1083\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 45, Loss: 0.1413\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 32, Batch: 50, Loss: 0.1319\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32\n",
      "Average Training Loss: 0.1187\n",
      "Validation Balanced Accuracy: 0.8199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Batch: 0, Loss: 0.1139\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 5, Loss: 0.1161\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 10, Loss: 0.1068\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 15, Loss: 0.1274\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 20, Loss: 0.0817\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 25, Loss: 0.0903\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 30, Loss: 0.1429\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 35, Loss: 0.0982\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 40, Loss: 0.1163\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 45, Loss: 0.1241\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 33, Batch: 50, Loss: 0.1619\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33\n",
      "Average Training Loss: 0.1167\n",
      "Validation Balanced Accuracy: 0.8187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Batch: 0, Loss: 0.1410\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 5, Loss: 0.1445\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 10, Loss: 0.1170\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 15, Loss: 0.1082\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 20, Loss: 0.1404\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 25, Loss: 0.1242\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 30, Loss: 0.1266\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 35, Loss: 0.0978\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 40, Loss: 0.1008\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 45, Loss: 0.1053\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 34, Batch: 50, Loss: 0.1330\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34\n",
      "Average Training Loss: 0.1194\n",
      "Validation Balanced Accuracy: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Batch: 0, Loss: 0.1236\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 5, Loss: 0.1108\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 10, Loss: 0.0989\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 15, Loss: 0.1139\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 20, Loss: 0.1235\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 25, Loss: 0.0814\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 30, Loss: 0.1255\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 35, Loss: 0.1463\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 40, Loss: 0.1200\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 45, Loss: 0.1049\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 35, Batch: 50, Loss: 0.1338\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35\n",
      "Average Training Loss: 0.1227\n",
      "Validation Balanced Accuracy: 0.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Batch: 0, Loss: 0.1073\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 5, Loss: 0.1288\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 10, Loss: 0.1087\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 15, Loss: 0.0994\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 20, Loss: 0.1123\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 25, Loss: 0.1073\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 30, Loss: 0.1596\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 35, Loss: 0.1499\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 40, Loss: 0.1295\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 45, Loss: 0.1080\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 36, Batch: 50, Loss: 0.1317\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36\n",
      "Average Training Loss: 0.1241\n",
      "Validation Balanced Accuracy: 0.8199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Batch: 0, Loss: 0.1250\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 5, Loss: 0.1040\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 10, Loss: 0.1463\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 15, Loss: 0.1196\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 20, Loss: 0.0998\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 25, Loss: 0.1070\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 30, Loss: 0.0987\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 35, Loss: 0.1222\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 40, Loss: 0.1422\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 45, Loss: 0.1101\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 37, Batch: 50, Loss: 0.1493\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37\n",
      "Average Training Loss: 0.1223\n",
      "Validation Balanced Accuracy: 0.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Batch: 0, Loss: 0.1164\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 5, Loss: 0.1263\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 10, Loss: 0.0913\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 15, Loss: 0.1305\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 20, Loss: 0.1565\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 25, Loss: 0.1110\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 30, Loss: 0.1217\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 35, Loss: 0.1316\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 40, Loss: 0.1106\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 45, Loss: 0.1285\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 38, Batch: 50, Loss: 0.1242\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38\n",
      "Average Training Loss: 0.1219\n",
      "Validation Balanced Accuracy: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Batch: 0, Loss: 0.1208\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 5, Loss: 0.1088\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 10, Loss: 0.0942\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 15, Loss: 0.1060\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 20, Loss: 0.0845\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 25, Loss: 0.1251\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 30, Loss: 0.1180\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 35, Loss: 0.1115\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 40, Loss: 0.1126\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 45, Loss: 0.1362\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 39, Batch: 50, Loss: 0.1138\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39\n",
      "Average Training Loss: 0.1151\n",
      "Validation Balanced Accuracy: 0.8147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Batch: 0, Loss: 0.0994\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 5, Loss: 0.1304\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 10, Loss: 0.2272\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 15, Loss: 0.1157\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 20, Loss: 0.1099\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 25, Loss: 0.1382\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 30, Loss: 0.0903\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 35, Loss: 0.1120\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 40, Loss: 0.1040\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 45, Loss: 0.1010\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 40, Batch: 50, Loss: 0.0944\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40\n",
      "Average Training Loss: 0.1133\n",
      "Validation Balanced Accuracy: 0.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 0, Loss: 0.0959\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 5, Loss: 0.1273\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 10, Loss: 0.1155\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 15, Loss: 0.1501\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 20, Loss: 0.1041\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 25, Loss: 0.1147\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 30, Loss: 0.1195\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 35, Loss: 0.1032\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 40, Loss: 0.0828\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 45, Loss: 0.1369\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 41, Batch: 50, Loss: 0.1070\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41\n",
      "Average Training Loss: 0.1130\n",
      "Validation Balanced Accuracy: 0.8193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Batch: 0, Loss: 0.1066\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 5, Loss: 0.1229\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 10, Loss: 0.1121\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 15, Loss: 0.0846\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 20, Loss: 0.1143\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 25, Loss: 0.0972\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 30, Loss: 0.1050\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 35, Loss: 0.0876\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 40, Loss: 0.1085\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 45, Loss: 0.0835\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 42, Batch: 50, Loss: 0.1060\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42\n",
      "Average Training Loss: 0.1070\n",
      "Validation Balanced Accuracy: 0.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Batch: 0, Loss: 0.1136\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 5, Loss: 0.1113\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 10, Loss: 0.1062\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 15, Loss: 0.0894\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 20, Loss: 0.0841\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 25, Loss: 0.1057\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 30, Loss: 0.1101\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 35, Loss: 0.1392\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 40, Loss: 0.0887\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 45, Loss: 0.0999\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 43, Batch: 50, Loss: 0.1037\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43\n",
      "Average Training Loss: 0.1055\n",
      "Validation Balanced Accuracy: 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Batch: 0, Loss: 0.0969\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 5, Loss: 0.1062\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 10, Loss: 0.1120\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 15, Loss: 0.1215\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 20, Loss: 0.0656\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 25, Loss: 0.0991\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 30, Loss: 0.0944\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 35, Loss: 0.0928\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 40, Loss: 0.0899\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 45, Loss: 0.0602\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 44, Batch: 50, Loss: 0.0880\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44\n",
      "Average Training Loss: 0.1019\n",
      "Validation Balanced Accuracy: 0.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Batch: 0, Loss: 0.1053\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 5, Loss: 0.0774\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 10, Loss: 0.0984\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 15, Loss: 0.1213\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 20, Loss: 0.0905\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 25, Loss: 0.0875\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 30, Loss: 0.0881\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 35, Loss: 0.0969\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 40, Loss: 0.1125\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 45, Loss: 0.0982\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 45, Batch: 50, Loss: 0.1118\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n",
      "Average Training Loss: 0.0990\n",
      "Validation Balanced Accuracy: 0.8219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Batch: 0, Loss: 0.0827\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 5, Loss: 0.0896\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 10, Loss: 0.1121\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 15, Loss: 0.1021\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 20, Loss: 0.0906\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 25, Loss: 0.0956\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 30, Loss: 0.1128\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 35, Loss: 0.1105\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 40, Loss: 0.1174\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 45, Loss: 0.0883\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 46, Batch: 50, Loss: 0.1026\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46\n",
      "Average Training Loss: 0.0977\n",
      "Validation Balanced Accuracy: 0.8167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Batch: 0, Loss: 0.1223\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 5, Loss: 0.0710\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 10, Loss: 0.0873\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 15, Loss: 0.0843\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 20, Loss: 0.0946\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 25, Loss: 0.0893\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 30, Loss: 0.0929\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 35, Loss: 0.0919\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 40, Loss: 0.1022\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 45, Loss: 0.0957\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 47, Batch: 50, Loss: 0.0872\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47\n",
      "Average Training Loss: 0.0961\n",
      "Validation Balanced Accuracy: 0.8169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Batch: 0, Loss: 0.1122\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 5, Loss: 0.1050\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 10, Loss: 0.1070\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 15, Loss: 0.0867\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 20, Loss: 0.0994\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 25, Loss: 0.0987\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 30, Loss: 0.0884\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 35, Loss: 0.0998\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 40, Loss: 0.1101\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 45, Loss: 0.0749\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 48, Batch: 50, Loss: 0.1104\n",
      "Learning rates: [7.85124354122177e-06, 7.636367895343946e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48\n",
      "Average Training Loss: 0.0954\n",
      "Validation Balanced Accuracy: 0.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Batch: 0, Loss: 0.0909\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 5, Loss: 0.0750\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 10, Loss: 0.0846\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 15, Loss: 0.0825\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 20, Loss: 0.1151\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 25, Loss: 0.1219\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 30, Loss: 0.0900\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 35, Loss: 0.0863\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 40, Loss: 0.0805\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 45, Loss: 0.1210\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 49, Batch: 50, Loss: 0.0692\n",
      "Learning rates: [7.542957248827961e-06, 7.297252973710757e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49\n",
      "Average Training Loss: 0.0953\n",
      "Validation Balanced Accuracy: 0.8199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Batch: 0, Loss: 0.0922\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 5, Loss: 0.0841\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 10, Loss: 0.0910\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 15, Loss: 0.0969\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 20, Loss: 0.0809\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 25, Loss: 0.1135\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 30, Loss: 0.0917\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 35, Loss: 0.0823\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 40, Loss: 0.0846\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 45, Loss: 0.0941\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 50, Batch: 50, Loss: 0.0969\n",
      "Learning rates: [7.222075445642904e-06, 6.944282990207195e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50\n",
      "Average Training Loss: 0.0967\n",
      "Validation Balanced Accuracy: 0.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Batch: 0, Loss: 0.1078\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 5, Loss: 0.0866\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 10, Loss: 0.0753\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 15, Loss: 0.0949\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 20, Loss: 0.0816\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 25, Loss: 0.0818\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 30, Loss: 0.1117\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 35, Loss: 0.1069\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 40, Loss: 0.0922\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 45, Loss: 0.0838\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 51, Batch: 50, Loss: 0.0936\n",
      "Learning rates: [6.890576474687264e-06, 6.57963412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51\n",
      "Average Training Loss: 0.0938\n",
      "Validation Balanced Accuracy: 0.8228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, Batch: 0, Loss: 0.0789\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 5, Loss: 0.0879\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 10, Loss: 0.0882\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 15, Loss: 0.1075\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 20, Loss: 0.1168\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 25, Loss: 0.0814\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 30, Loss: 0.0764\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 35, Loss: 0.0802\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 40, Loss: 0.1026\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 45, Loss: 0.0779\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 52, Batch: 50, Loss: 0.0725\n",
      "Learning rates: [6.550504137351575e-06, 6.205554551086733e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52\n",
      "Average Training Loss: 0.0900\n",
      "Validation Balanced Accuracy: 0.8223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53, Batch: 0, Loss: 0.0873\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 5, Loss: 0.0954\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 10, Loss: 0.0778\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 15, Loss: 0.0822\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 20, Loss: 0.1100\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 25, Loss: 0.0983\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 30, Loss: 0.1090\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 35, Loss: 0.1021\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 40, Loss: 0.0799\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 45, Loss: 0.0821\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 53, Batch: 50, Loss: 0.0685\n",
      "Learning rates: [6.2039550926810394e-06, 5.8243506019491436e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53\n",
      "Average Training Loss: 0.0883\n",
      "Validation Balanced Accuracy: 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Batch: 0, Loss: 0.0875\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 5, Loss: 0.1170\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 10, Loss: 0.0975\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 15, Loss: 0.0760\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 20, Loss: 0.0956\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 25, Loss: 0.0695\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 30, Loss: 0.0673\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 35, Loss: 0.1013\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 40, Loss: 0.0668\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 45, Loss: 0.0785\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 54, Batch: 50, Loss: 0.0758\n",
      "Learning rates: [5.853065930775303e-06, 5.438372523852833e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54\n",
      "Average Training Loss: 0.0856\n",
      "Validation Balanced Accuracy: 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Batch: 0, Loss: 0.1091\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 5, Loss: 0.1010\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 10, Loss: 0.0837\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 15, Loss: 0.0950\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 20, Loss: 0.1095\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 25, Loss: 0.0900\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 30, Loss: 0.0687\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 35, Loss: 0.0799\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 40, Loss: 0.0728\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 45, Loss: 0.1046\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 55, Batch: 50, Loss: 0.0891\n",
      "Learning rates: [5.5e-06, 5.05e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55\n",
      "Average Training Loss: 0.0855\n",
      "Validation Balanced Accuracy: 0.8202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56, Batch: 0, Loss: 0.0984\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 5, Loss: 0.0779\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 10, Loss: 0.0929\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 15, Loss: 0.0804\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 20, Loss: 0.0778\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 25, Loss: 0.0743\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 30, Loss: 0.0714\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 35, Loss: 0.0775\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 40, Loss: 0.0944\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 45, Loss: 0.0809\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 56, Batch: 50, Loss: 0.0887\n",
      "Learning rates: [5.146934069224698e-06, 4.661627476147168e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56\n",
      "Average Training Loss: 0.0867\n",
      "Validation Balanced Accuracy: 0.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, Batch: 0, Loss: 0.1168\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 5, Loss: 0.0936\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 10, Loss: 0.0916\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 15, Loss: 0.0876\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 20, Loss: 0.0822\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 25, Loss: 0.0796\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 30, Loss: 0.0887\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 35, Loss: 0.1157\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 40, Loss: 0.0929\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 45, Loss: 0.0776\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 57, Batch: 50, Loss: 0.0756\n",
      "Learning rates: [4.796044907318962e-06, 4.2756493980508586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57\n",
      "Average Training Loss: 0.0834\n",
      "Validation Balanced Accuracy: 0.8246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58, Batch: 0, Loss: 0.0719\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 5, Loss: 0.0765\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 10, Loss: 0.0934\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 15, Loss: 0.0804\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 20, Loss: 0.0658\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 25, Loss: 0.1009\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 30, Loss: 0.0762\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 35, Loss: 0.0716\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 40, Loss: 0.0760\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 45, Loss: 0.0821\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 58, Batch: 50, Loss: 0.0925\n",
      "Learning rates: [4.4494958626484265e-06, 3.894445448913269e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58\n",
      "Average Training Loss: 0.0831\n",
      "Validation Balanced Accuracy: 0.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59, Batch: 0, Loss: 0.0735\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 5, Loss: 0.0962\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 10, Loss: 0.0829\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 15, Loss: 0.0716\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 20, Loss: 0.0739\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 25, Loss: 0.0802\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 30, Loss: 0.0760\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 35, Loss: 0.0938\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 40, Loss: 0.0687\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 45, Loss: 0.0613\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 59, Batch: 50, Loss: 0.0773\n",
      "Learning rates: [4.109423525312737e-06, 3.5203658778440106e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59\n",
      "Average Training Loss: 0.0815\n",
      "Validation Balanced Accuracy: 0.8184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, Batch: 0, Loss: 0.1082\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 5, Loss: 0.0827\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 10, Loss: 0.0657\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 15, Loss: 0.1036\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 20, Loss: 0.0801\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 25, Loss: 0.0793\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 30, Loss: 0.0783\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 35, Loss: 0.0721\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 40, Loss: 0.0788\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 45, Loss: 0.0859\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 60, Batch: 50, Loss: 0.0829\n",
      "Learning rates: [3.777924554357096e-06, 3.1557170097928055e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60\n",
      "Average Training Loss: 0.0825\n",
      "Validation Balanced Accuracy: 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Batch: 0, Loss: 0.0779\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 5, Loss: 0.0707\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 10, Loss: 0.0746\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 15, Loss: 0.0904\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 20, Loss: 0.0699\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 25, Loss: 0.0874\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 30, Loss: 0.0705\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 35, Loss: 0.0933\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 40, Loss: 0.0841\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 45, Loss: 0.0790\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 61, Batch: 50, Loss: 0.0667\n",
      "Learning rates: [3.45704275117204e-06, 2.8027470262892437e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61\n",
      "Average Training Loss: 0.0816\n",
      "Validation Balanced Accuracy: 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Batch: 0, Loss: 0.0988\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 5, Loss: 0.0984\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 10, Loss: 0.0756\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 15, Loss: 0.0805\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 20, Loss: 0.0624\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 25, Loss: 0.0952\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 30, Loss: 0.0721\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 35, Loss: 0.0770\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 40, Loss: 0.0693\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 45, Loss: 0.0814\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 62, Batch: 50, Loss: 0.0712\n",
      "Learning rates: [3.1487564587782306e-06, 2.4636321046560538e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62\n",
      "Average Training Loss: 0.0796\n",
      "Validation Balanced Accuracy: 0.8196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63, Batch: 0, Loss: 0.0740\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 5, Loss: 0.0690\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 10, Loss: 0.0851\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 15, Loss: 0.0665\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 20, Loss: 0.0809\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 25, Loss: 0.0732\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 30, Loss: 0.0680\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 35, Loss: 0.0847\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 40, Loss: 0.0816\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 45, Loss: 0.0970\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 63, Batch: 50, Loss: 0.0894\n",
      "Learning rates: [2.8549663646838717e-06, 2.1404630011522586e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63\n",
      "Average Training Loss: 0.0815\n",
      "Validation Balanced Accuracy: 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Batch: 0, Loss: 0.0710\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 5, Loss: 0.1007\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 10, Loss: 0.0934\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 15, Loss: 0.0727\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 20, Loss: 0.0732\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 25, Loss: 0.0841\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 30, Loss: 0.0785\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 35, Loss: 0.1109\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 40, Loss: 0.0763\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 45, Loss: 0.0725\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 64, Batch: 50, Loss: 0.0934\n",
      "Learning rates: [2.577483782514174e-06, 1.8352321607655915e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64\n",
      "Average Training Loss: 0.0792\n",
      "Validation Balanced Accuracy: 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Batch: 0, Loss: 0.0657\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 5, Loss: 0.0738\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 10, Loss: 0.0678\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 15, Loss: 0.0915\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 20, Loss: 0.0895\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 25, Loss: 0.0905\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 30, Loss: 0.0851\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 35, Loss: 0.0756\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 40, Loss: 0.0913\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 45, Loss: 0.0776\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 65, Batch: 50, Loss: 0.0772\n",
      "Learning rates: [2.3180194846605362e-06, 1.54982143312659e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65\n",
      "Average Training Loss: 0.0785\n",
      "Validation Balanced Accuracy: 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Batch: 0, Loss: 0.0788\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 5, Loss: 0.0712\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 10, Loss: 0.0845\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 15, Loss: 0.0655\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 20, Loss: 0.1048\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 25, Loss: 0.0932\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 30, Loss: 0.0815\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 35, Loss: 0.0840\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 40, Loss: 0.0856\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 45, Loss: 0.0652\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 66, Batch: 50, Loss: 0.0773\n",
      "Learning rates: [2.0781731547998605e-06, 1.285990470279847e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66\n",
      "Average Training Loss: 0.0793\n",
      "Validation Balanced Accuracy: 0.8216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Batch: 0, Loss: 0.0685\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 5, Loss: 0.0775\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 10, Loss: 0.0877\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 15, Loss: 0.0825\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 20, Loss: 0.0913\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 25, Loss: 0.0836\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 30, Loss: 0.0692\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 35, Loss: 0.0577\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 40, Loss: 0.0615\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 45, Loss: 0.0708\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 67, Batch: 50, Loss: 0.0719\n",
      "Learning rates: [1.8594235253127369e-06, 1.0453658778440109e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67\n",
      "Average Training Loss: 0.0784\n",
      "Validation Balanced Accuracy: 0.8206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Batch: 0, Loss: 0.1025\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 5, Loss: 0.0788\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 10, Loss: 0.0671\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 15, Loss: 0.1025\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 20, Loss: 0.0825\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 25, Loss: 0.0862\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 30, Loss: 0.0770\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 35, Loss: 0.0840\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 40, Loss: 0.0694\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 45, Loss: 0.0911\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 68, Batch: 50, Loss: 0.0762\n",
      "Learning rates: [1.6631192604065851e-06, 8.294311864472437e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68\n",
      "Average Training Loss: 0.0801\n",
      "Validation Balanced Accuracy: 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69, Batch: 0, Loss: 0.0723\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 5, Loss: 0.0801\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 10, Loss: 0.0830\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 15, Loss: 0.0870\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 20, Loss: 0.0936\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 25, Loss: 0.0924\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 30, Loss: 0.0716\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 35, Loss: 0.0788\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 40, Loss: 0.0873\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 45, Loss: 0.0638\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 69, Batch: 50, Loss: 0.1113\n",
      "Learning rates: [1.4904706411523449e-06, 6.395177052675795e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69\n",
      "Average Training Loss: 0.0796\n",
      "Validation Balanced Accuracy: 0.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70, Batch: 0, Loss: 0.0691\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 5, Loss: 0.0762\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 10, Loss: 0.0694\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 15, Loss: 0.0703\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 20, Loss: 0.0752\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 25, Loss: 0.0825\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 30, Loss: 0.0813\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 35, Loss: 0.0610\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 40, Loss: 0.0660\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 45, Loss: 0.0748\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 70, Batch: 50, Loss: 0.0721\n",
      "Learning rates: [1.3425421036992096e-06, 4.767963140691306e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70\n",
      "Average Training Loss: 0.0782\n",
      "Validation Balanced Accuracy: 0.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71, Batch: 0, Loss: 0.0710\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 5, Loss: 0.0695\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 10, Loss: 0.0621\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 15, Loss: 0.0716\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 20, Loss: 0.0888\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 25, Loss: 0.0609\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 30, Loss: 0.0673\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 35, Loss: 0.1001\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 40, Loss: 0.0690\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 45, Loss: 0.0690\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 71, Batch: 50, Loss: 0.0803\n",
      "Learning rates: [1.220245676671809e-06, 3.4227024433899e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71\n",
      "Average Training Loss: 0.0762\n",
      "Validation Balanced Accuracy: 0.8212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72, Batch: 0, Loss: 0.0677\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 5, Loss: 0.0724\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 10, Loss: 0.0986\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 15, Loss: 0.1294\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 20, Loss: 0.0760\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 25, Loss: 0.0820\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 30, Loss: 0.0776\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 35, Loss: 0.0734\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 40, Loss: 0.0701\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 45, Loss: 0.1051\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 72, Batch: 50, Loss: 0.0747\n",
      "Learning rates: [1.1243353582104554e-06, 2.36768894031501e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72\n",
      "Average Training Loss: 0.0788\n",
      "Validation Balanced Accuracy: 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73, Batch: 0, Loss: 0.0927\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 5, Loss: 0.0617\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 10, Loss: 0.0663\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 15, Loss: 0.0633\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 20, Loss: 0.0829\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 25, Loss: 0.0924\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 30, Loss: 0.0599\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 35, Loss: 0.0802\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 40, Loss: 0.0596\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 45, Loss: 0.0786\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 73, Batch: 50, Loss: 0.0734\n",
      "Learning rates: [1.0554024673218804e-06, 1.6094271405406859e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73\n",
      "Average Training Loss: 0.0769\n",
      "Validation Balanced Accuracy: 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, Batch: 0, Loss: 0.0923\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 5, Loss: 0.0740\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 10, Loss: 0.0958\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 15, Loss: 0.0652\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 20, Loss: 0.0888\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 25, Loss: 0.0720\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 30, Loss: 0.0808\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 35, Loss: 0.0695\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 40, Loss: 0.0818\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 45, Loss: 0.0737\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 74, Batch: 50, Loss: 0.0756\n",
      "Learning rates: [1.013871998200924e-06, 1.1525919802101657e-06]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74\n",
      "Average Training Loss: 0.0765\n",
      "Validation Balanced Accuracy: 0.8197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75, Batch: 0, Loss: 0.0783\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 5, Loss: 0.0814\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 10, Loss: 0.0811\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 15, Loss: 0.0907\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 20, Loss: 0.0649\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 25, Loss: 0.0974\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 30, Loss: 0.0587\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 35, Loss: 0.0850\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 40, Loss: 0.0625\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 45, Loss: 0.0768\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 75, Batch: 50, Loss: 0.1136\n",
      "Learning rates: [1e-05, 0.0001]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75\n",
      "Average Training Loss: 0.0850\n",
      "Validation Balanced Accuracy: 0.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76, Batch: 0, Loss: 0.0861\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 5, Loss: 0.0814\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 10, Loss: 0.0759\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 15, Loss: 0.0738\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 20, Loss: 0.0755\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 25, Loss: 0.1202\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 30, Loss: 0.1279\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 35, Loss: 0.0911\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 40, Loss: 0.0619\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 45, Loss: 0.0920\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 76, Batch: 50, Loss: 0.0778\n",
      "Learning rates: [9.996530663083255e-06, 9.996183729391579e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76\n",
      "Average Training Loss: 0.0871\n",
      "Validation Balanced Accuracy: 0.8199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77, Batch: 0, Loss: 0.0938\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 5, Loss: 0.0924\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 10, Loss: 0.0832\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 15, Loss: 0.1142\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 20, Loss: 0.1081\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 25, Loss: 0.0838\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 30, Loss: 0.0868\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 35, Loss: 0.0720\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 40, Loss: 0.0913\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 45, Loss: 0.0842\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 77, Batch: 50, Loss: 0.0913\n",
      "Learning rates: [9.986128001799077e-06, 9.984740801978984e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77\n",
      "Average Training Loss: 0.0854\n",
      "Validation Balanced Accuracy: 0.8231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78, Batch: 0, Loss: 0.0904\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 5, Loss: 0.0803\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 10, Loss: 0.0805\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 15, Loss: 0.0827\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 20, Loss: 0.0879\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 25, Loss: 0.0628\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 30, Loss: 0.0828\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 35, Loss: 0.0791\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 40, Loss: 0.0815\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 45, Loss: 0.0863\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 78, Batch: 50, Loss: 0.1023\n",
      "Learning rates: [9.96880805629717e-06, 9.965688861926886e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78\n",
      "Average Training Loss: 0.0828\n",
      "Validation Balanced Accuracy: 0.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79, Batch: 0, Loss: 0.0727\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 5, Loss: 0.0677\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 10, Loss: 0.0787\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 15, Loss: 0.0779\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 20, Loss: 0.0786\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 25, Loss: 0.0758\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 30, Loss: 0.0624\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 35, Loss: 0.0810\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 40, Loss: 0.0650\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 45, Loss: 0.0704\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 79, Batch: 50, Loss: 0.0943\n",
      "Learning rates: [9.94459753267812e-06, 9.939057285945933e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79\n",
      "Average Training Loss: 0.0806\n",
      "Validation Balanced Accuracy: 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Batch: 0, Loss: 0.0651\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 5, Loss: 0.0830\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 10, Loss: 0.0728\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 15, Loss: 0.0809\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 20, Loss: 0.0774\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 25, Loss: 0.0872\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 30, Loss: 0.0896\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 35, Loss: 0.0941\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 40, Loss: 0.0772\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 45, Loss: 0.0738\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 80, Batch: 50, Loss: 0.0587\n",
      "Learning rates: [9.913533761814537e-06, 9.90488713799599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80\n",
      "Average Training Loss: 0.0812\n",
      "Validation Balanced Accuracy: 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Batch: 0, Loss: 0.0621\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 5, Loss: 0.0730\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 10, Loss: 0.0857\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 15, Loss: 0.0700\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 20, Loss: 0.0799\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 25, Loss: 0.0766\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 30, Loss: 0.0636\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 35, Loss: 0.0609\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 40, Loss: 0.0609\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 45, Loss: 0.0766\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 81, Batch: 50, Loss: 0.1004\n",
      "Learning rates: [9.875664641789545e-06, 9.8632311059685e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81\n",
      "Average Training Loss: 0.0784\n",
      "Validation Balanced Accuracy: 0.8254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Batch: 0, Loss: 0.0819\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 5, Loss: 0.0720\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 10, Loss: 0.0726\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 15, Loss: 0.0787\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 20, Loss: 0.0705\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 25, Loss: 0.0854\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 30, Loss: 0.0745\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 35, Loss: 0.1020\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 40, Loss: 0.0639\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 45, Loss: 0.0978\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 82, Batch: 50, Loss: 0.0774\n",
      "Learning rates: [9.831048564041412e-06, 9.814153420445554e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82\n",
      "Average Training Loss: 0.0766\n",
      "Validation Balanced Accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83, Batch: 0, Loss: 0.0583\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 5, Loss: 0.0885\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 10, Loss: 0.0816\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 15, Loss: 0.0693\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 20, Loss: 0.0764\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 25, Loss: 0.0667\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 30, Loss: 0.0612\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 35, Loss: 0.0715\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 40, Loss: 0.0669\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 45, Loss: 0.0842\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 83, Batch: 50, Loss: 0.0689\n",
      "Learning rates: [9.779754323328192e-06, 9.757729755661011e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83\n",
      "Average Training Loss: 0.0740\n",
      "Validation Balanced Accuracy: 0.8154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84, Batch: 0, Loss: 0.0666\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 5, Loss: 0.0747\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 10, Loss: 0.0576\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 15, Loss: 0.0699\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 20, Loss: 0.0794\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 25, Loss: 0.0878\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 30, Loss: 0.0727\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 35, Loss: 0.0710\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 40, Loss: 0.0762\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 45, Loss: 0.0808\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 84, Batch: 50, Loss: 0.0806\n",
      "Learning rates: [9.72186101165118e-06, 9.694047112816297e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84\n",
      "Average Training Loss: 0.0744\n",
      "Validation Balanced Accuracy: 0.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85, Batch: 0, Loss: 0.0684\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 5, Loss: 0.0761\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 10, Loss: 0.0718\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 15, Loss: 0.0667\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 20, Loss: 0.0740\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 25, Loss: 0.0631\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 30, Loss: 0.0613\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 35, Loss: 0.0893\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 40, Loss: 0.0649\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 45, Loss: 0.0697\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 85, Batch: 50, Loss: 0.0779\n",
      "Learning rates: [9.65745789630079e-06, 9.623203685930869e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85\n",
      "Average Training Loss: 0.0727\n",
      "Validation Balanced Accuracy: 0.8202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86, Batch: 0, Loss: 0.0663\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 5, Loss: 0.0827\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 10, Loss: 0.0732\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 15, Loss: 0.0820\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 20, Loss: 0.0747\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 25, Loss: 0.0926\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 30, Loss: 0.0834\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 35, Loss: 0.0928\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 40, Loss: 0.0525\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 45, Loss: 0.0883\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 86, Batch: 50, Loss: 0.0785\n",
      "Learning rates: [9.586644282212866e-06, 9.545308710434153e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86\n",
      "Average Training Loss: 0.0745\n",
      "Validation Balanced Accuracy: 0.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87, Batch: 0, Loss: 0.0818\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 5, Loss: 0.0654\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 10, Loss: 0.0613\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 15, Loss: 0.0665\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 20, Loss: 0.0630\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 25, Loss: 0.0731\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 30, Loss: 0.0844\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 35, Loss: 0.0603\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 40, Loss: 0.0759\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 45, Loss: 0.0765\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 87, Batch: 50, Loss: 0.0962\n",
      "Learning rates: [9.509529358847657e-06, 9.460482294732421e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87\n",
      "Average Training Loss: 0.0718\n",
      "Validation Balanced Accuracy: 0.8258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88, Batch: 0, Loss: 0.0675\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 5, Loss: 0.0705\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 10, Loss: 0.0618\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 15, Loss: 0.0796\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 20, Loss: 0.0699\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 25, Loss: 0.0532\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 30, Loss: 0.0564\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 35, Loss: 0.0718\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 40, Loss: 0.0765\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 45, Loss: 0.0756\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 88, Batch: 50, Loss: 0.0866\n",
      "Learning rates: [9.426232031827589e-06, 9.368855235010347e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88\n",
      "Average Training Loss: 0.0697\n",
      "Validation Balanced Accuracy: 0.8181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89, Batch: 0, Loss: 0.1362\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 5, Loss: 0.0691\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 10, Loss: 0.0674\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 15, Loss: 0.0629\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 20, Loss: 0.0886\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 25, Loss: 0.0820\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 30, Loss: 0.0678\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 35, Loss: 0.0670\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 40, Loss: 0.0698\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 45, Loss: 0.0764\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 89, Batch: 50, Loss: 0.0759\n",
      "Learning rates: [9.336880739593415e-06, 9.270568813552756e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89\n",
      "Average Training Loss: 0.0733\n",
      "Validation Balanced Accuracy: 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90, Batch: 0, Loss: 0.0730\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 5, Loss: 0.0628\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 10, Loss: 0.0706\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 15, Loss: 0.0654\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 20, Loss: 0.0687\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 25, Loss: 0.0854\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 30, Loss: 0.0672\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 35, Loss: 0.0840\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 40, Loss: 0.0776\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 45, Loss: 0.0683\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 90, Batch: 50, Loss: 0.0731\n",
      "Learning rates: [9.241613255361455e-06, 9.1657745808976e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90\n",
      "Average Training Loss: 0.0701\n",
      "Validation Balanced Accuracy: 0.8228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Batch: 0, Loss: 0.0659\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 5, Loss: 0.0662\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 10, Loss: 0.0595\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 15, Loss: 0.0526\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 20, Loss: 0.0688\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 25, Loss: 0.0909\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 30, Loss: 0.0745\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 35, Loss: 0.0699\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 40, Loss: 0.0749\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 45, Loss: 0.0670\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 91, Batch: 50, Loss: 0.0508\n",
      "Learning rates: [9.140576474687265e-06, 9.05463412215599e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91\n",
      "Average Training Loss: 0.0703\n",
      "Validation Balanced Accuracy: 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Batch: 0, Loss: 0.0535\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 5, Loss: 0.0786\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 10, Loss: 0.0782\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 15, Loss: 0.0571\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 20, Loss: 0.0766\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 25, Loss: 0.0651\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 30, Loss: 0.1065\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 35, Loss: 0.0770\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 40, Loss: 0.0681\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 45, Loss: 0.0830\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 92, Batch: 50, Loss: 0.0732\n",
      "Learning rates: [9.033926188963353e-06, 8.937318807859687e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92\n",
      "Average Training Loss: 0.0699\n",
      "Validation Balanced Accuracy: 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93, Batch: 0, Loss: 0.0705\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 5, Loss: 0.0662\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 10, Loss: 0.0814\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 15, Loss: 0.0542\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 20, Loss: 0.0734\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 25, Loss: 0.0782\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 30, Loss: 0.0705\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 35, Loss: 0.0617\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 40, Loss: 0.0734\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 45, Loss: 0.0614\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 93, Batch: 50, Loss: 0.0779\n",
      "Learning rates: [8.92182684520014e-06, 8.814009529720155e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93\n",
      "Average Training Loss: 0.0685\n",
      "Validation Balanced Accuracy: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94, Batch: 0, Loss: 0.0585\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 5, Loss: 0.0671\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 10, Loss: 0.0616\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 15, Loss: 0.0716\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 20, Loss: 0.0605\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 25, Loss: 0.0576\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 30, Loss: 0.0511\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 35, Loss: 0.0612\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 40, Loss: 0.0576\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 45, Loss: 0.0721\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 94, Batch: 50, Loss: 0.0528\n",
      "Learning rates: [8.804451292460586e-06, 8.684896421706644e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94\n",
      "Average Training Loss: 0.0651\n",
      "Validation Balanced Accuracy: 0.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Batch: 0, Loss: 0.0755\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 5, Loss: 0.0697\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 10, Loss: 0.0852\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 15, Loss: 0.0576\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 20, Loss: 0.0529\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 25, Loss: 0.0607\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 30, Loss: 0.0642\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 35, Loss: 0.0746\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 40, Loss: 0.0747\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 45, Loss: 0.0630\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 95, Batch: 50, Loss: 0.0691\n",
      "Learning rates: [8.681980515339464e-06, 8.55017856687341e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95\n",
      "Average Training Loss: 0.0677\n",
      "Validation Balanced Accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96, Batch: 0, Loss: 0.0721\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 5, Loss: 0.0619\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 10, Loss: 0.0724\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 15, Loss: 0.0634\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 20, Loss: 0.0626\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 25, Loss: 0.0669\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 30, Loss: 0.0660\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 35, Loss: 0.0575\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 40, Loss: 0.0732\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 45, Loss: 0.0953\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 96, Batch: 50, Loss: 0.0567\n",
      "Learning rates: [8.554603354898239e-06, 8.410063690388063e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96\n",
      "Average Training Loss: 0.0671\n",
      "Validation Balanced Accuracy: 0.8247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97, Batch: 0, Loss: 0.0648\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 5, Loss: 0.0637\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 10, Loss: 0.0669\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 15, Loss: 0.0718\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 20, Loss: 0.0680\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 25, Loss: 0.0676\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 30, Loss: 0.0852\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 35, Loss: 0.0744\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 40, Loss: 0.0604\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 45, Loss: 0.0623\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 97, Batch: 50, Loss: 0.0602\n",
      "Learning rates: [8.422516217485828e-06, 8.26476783923441e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97\n",
      "Average Training Loss: 0.0665\n",
      "Validation Balanced Accuracy: 0.8229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Batch: 0, Loss: 0.0579\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 5, Loss: 0.0661\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 10, Loss: 0.0593\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 15, Loss: 0.0606\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 20, Loss: 0.0714\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 25, Loss: 0.0638\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 30, Loss: 0.0548\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 35, Loss: 0.0626\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 40, Loss: 0.0475\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 45, Loss: 0.0717\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 98, Batch: 50, Loss: 0.0694\n",
      "Learning rates: [8.285922771894254e-06, 8.114515049083679e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98\n",
      "Average Training Loss: 0.0662\n",
      "Validation Balanced Accuracy: 0.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_29/458524710.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, Batch: 0, Loss: 0.0579\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 5, Loss: 0.0550\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 10, Loss: 0.0691\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 15, Loss: 0.0660\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 20, Loss: 0.0517\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 25, Loss: 0.0751\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 30, Loss: 0.0868\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 35, Loss: 0.0813\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 40, Loss: 0.0645\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 45, Loss: 0.0517\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n",
      "Epoch: 99, Batch: 50, Loss: 0.0605\n",
      "Learning rates: [8.14503363531613e-06, 7.959536998847742e-05]\n",
      "GPU 0 memory allocated: 0.85 GB\n",
      "GPU 1 memory allocated: 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99\n",
      "Average Training Loss: 0.0656\n",
      "Validation Balanced Accuracy: 0.8247\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ColorJitter, Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Check GPU availability and set device\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "class CraterDataset(Dataset):\n",
    "    def __init__(self, images, masks, processor, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        image_rgb = np.repeat(image.reshape(48, 48, 1), 3, axis=-1)\n",
    "        \n",
    "        # Apply ColorJitter augmentation if transform exists\n",
    "        if self.transform and torch.rand(1) < 0.5:  # 50% chance to apply\n",
    "            image_tensor = torch.from_numpy(image_rgb).permute(2, 0, 1)\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "            image_rgb = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image_rgb, \n",
    "            segmentation_maps=mask, \n",
    "            return_tensors=\"pt\",\n",
    "            do_rescale=False\n",
    "        )\n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(),\n",
    "            'labels': inputs['labels'].squeeze()\n",
    "        }\n",
    "\n",
    "def calculate_validation_accuracy(model, X_test_normalized, y_test, processor):\n",
    "    resize_transform = Resize(\n",
    "        size=(48, 48),\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "        antialias=True\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(X_test_normalized), 48, 48))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, image in enumerate(X_test_normalized):\n",
    "            image_rgb = np.repeat(image.reshape(48, 48, 1), 3, axis=-1)\n",
    "            inputs = processor(\n",
    "                images=image_rgb,\n",
    "                return_tensors=\"pt\",\n",
    "                do_rescale=False\n",
    "            )\n",
    "            pixel_values = inputs['pixel_values'].cuda()\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            pred = probs[0, 1].cpu().numpy()\n",
    "            \n",
    "            if pred.shape != (48, 48):\n",
    "                pred_tensor = torch.from_numpy(pred).unsqueeze(0)\n",
    "                pred_resized = resize_transform(pred_tensor).squeeze().numpy()\n",
    "                predictions[i] = pred_resized\n",
    "            else:\n",
    "                predictions[i] = pred\n",
    "    \n",
    "    y_true_flat = y_test.reshape(-1, 48, 48).flatten()\n",
    "    y_pred_flat = predictions.flatten()\n",
    "    y_pred_flat = (y_pred_flat > 0.5).astype(int)\n",
    "    \n",
    "    return balanced_accuracy_score(y_true_flat, y_pred_flat)\n",
    "\n",
    "print(\"Starting data preparation...\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")\n",
    "\n",
    "# Reshape and normalize data\n",
    "X_train_reshaped = X_train.reshape(-1, 48, 48, 1)\n",
    "y_train_reshaped = y_train.reshape(-1, 48, 48)\n",
    "X_train_normalized = X_train_reshaped / 255.0\n",
    "\n",
    "# Reshape and normalize test data\n",
    "X_test_reshaped = X_test.reshape(-1, 48, 48, 1)\n",
    "X_test_normalized = X_test_reshaped / 255.0\n",
    "\n",
    "print(\"Initializing model and processor...\")\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b3\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "\n",
    "# Data augmentation\n",
    "color_jitter = ColorJitter(\n",
    "    brightness=0.2,\n",
    "    contrast=0.2,\n",
    "    saturation=0.2,\n",
    "    hue=0.1\n",
    ")\n",
    "\n",
    "# Create dataset and dataloader with augmentation\n",
    "train_dataset = CraterDataset(\n",
    "    X_train_normalized, \n",
    "    y_train_reshaped, \n",
    "    processor,\n",
    "    transform=color_jitter\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.module.segformer.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.module.decode_head.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=5,\n",
    "    T_mult=2,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "best_balanced_accuracy = 0\n",
    "patience = 100  # Changed to 20\n",
    "patience_counter = 0\n",
    "num_epochs = 100  # Changed to 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        pixel_values = batch['pixel_values'].cuda()\n",
    "        labels = batch['labels'].cuda()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss.mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            print(f\"Learning rates: {[group['lr'] for group in optimizer.param_groups]}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                memory_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "                print(f\"GPU {i} memory allocated: {memory_allocated:.2f} GB\")\n",
    "    \n",
    "    avg_loss = total_loss / batch_count\n",
    "    \n",
    "    # Calculate validation balanced accuracy\n",
    "    val_balanced_accuracy = calculate_validation_accuracy(model, X_test_normalized, y_test, processor)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Average Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Balanced Accuracy: {val_balanced_accuracy:.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping and model saving based on balanced accuracy\n",
    "    if val_balanced_accuracy > best_balanced_accuracy:\n",
    "        best_balanced_accuracy = val_balanced_accuracy\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'balanced_accuracy': val_balanced_accuracy,\n",
    "        }, 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs - No improvement in balanced accuracy\")\n",
    "            break\n",
    "    \n",
    "    # Regular checkpoint saving\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'balanced_accuracy': val_balanced_accuracy,\n",
    "        }, f'checkpoint_epoch_{epoch+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:01:48.927372Z",
     "iopub.status.busy": "2024-10-26T19:01:48.926998Z",
     "iopub.status.idle": "2024-10-26T19:01:52.272043Z",
     "shell.execute_reply": "2024-10-26T19:01:52.269457Z",
     "shell.execute_reply.started": "2024-10-26T19:01:48.927335Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29/2075488318.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pt')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.0.2.layer_norm_1.weight\", \"segformer.encoder.block.0.2.layer_norm_1.bias\", \"segformer.encoder.block.0.2.attention.self.query.weight\", \"segformer.encoder.block.0.2.attention.self.query.bias\", \"segformer.encoder.block.0.2.attention.self.key.weight\", \"segformer.encoder.block.0.2.attention.self.key.bias\", \"segformer.encoder.block.0.2.attention.self.value.weight\", \"segformer.encoder.block.0.2.attention.self.value.bias\", \"segformer.encoder.block.0.2.attention.self.sr.weight\", \"segformer.encoder.block.0.2.attention.self.sr.bias\", \"segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.2.attention.output.dense.weight\", \"segformer.encoder.block.0.2.attention.output.dense.bias\", \"segformer.encoder.block.0.2.layer_norm_2.weight\", \"segformer.encoder.block.0.2.layer_norm_2.bias\", \"segformer.encoder.block.0.2.mlp.dense1.weight\", \"segformer.encoder.block.0.2.mlp.dense1.bias\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.2.mlp.dense2.weight\", \"segformer.encoder.block.0.2.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.1.2.layer_norm_1.weight\", \"segformer.encoder.block.1.2.layer_norm_1.bias\", \"segformer.encoder.block.1.2.attention.self.query.weight\", \"segformer.encoder.block.1.2.attention.self.query.bias\", \"segformer.encoder.block.1.2.attention.self.key.weight\", \"segformer.encoder.block.1.2.attention.self.key.bias\", \"segformer.encoder.block.1.2.attention.self.value.weight\", \"segformer.encoder.block.1.2.attention.self.value.bias\", \"segformer.encoder.block.1.2.attention.self.sr.weight\", \"segformer.encoder.block.1.2.attention.self.sr.bias\", \"segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.2.attention.output.dense.weight\", \"segformer.encoder.block.1.2.attention.output.dense.bias\", \"segformer.encoder.block.1.2.layer_norm_2.weight\", \"segformer.encoder.block.1.2.layer_norm_2.bias\", \"segformer.encoder.block.1.2.mlp.dense1.weight\", \"segformer.encoder.block.1.2.mlp.dense1.bias\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.2.mlp.dense2.weight\", \"segformer.encoder.block.1.2.mlp.dense2.bias\", \"segformer.encoder.block.1.3.layer_norm_1.weight\", \"segformer.encoder.block.1.3.layer_norm_1.bias\", \"segformer.encoder.block.1.3.attention.self.query.weight\", \"segformer.encoder.block.1.3.attention.self.query.bias\", \"segformer.encoder.block.1.3.attention.self.key.weight\", \"segformer.encoder.block.1.3.attention.self.key.bias\", \"segformer.encoder.block.1.3.attention.self.value.weight\", \"segformer.encoder.block.1.3.attention.self.value.bias\", \"segformer.encoder.block.1.3.attention.self.sr.weight\", \"segformer.encoder.block.1.3.attention.self.sr.bias\", \"segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.3.attention.output.dense.weight\", \"segformer.encoder.block.1.3.attention.output.dense.bias\", \"segformer.encoder.block.1.3.layer_norm_2.weight\", \"segformer.encoder.block.1.3.layer_norm_2.bias\", \"segformer.encoder.block.1.3.mlp.dense1.weight\", \"segformer.encoder.block.1.3.mlp.dense1.bias\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.3.mlp.dense2.weight\", \"segformer.encoder.block.1.3.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.2.2.layer_norm_1.weight\", \"segformer.encoder.block.2.2.layer_norm_1.bias\", \"segformer.encoder.block.2.2.attention.self.query.weight\", \"segformer.encoder.block.2.2.attention.self.query.bias\", \"segformer.encoder.block.2.2.attention.self.key.weight\", \"segformer.encoder.block.2.2.attention.self.key.bias\", \"segformer.encoder.block.2.2.attention.self.value.weight\", \"segformer.encoder.block.2.2.attention.self.value.bias\", \"segformer.encoder.block.2.2.attention.self.sr.weight\", \"segformer.encoder.block.2.2.attention.self.sr.bias\", \"segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.2.attention.output.dense.weight\", \"segformer.encoder.block.2.2.attention.output.dense.bias\", \"segformer.encoder.block.2.2.layer_norm_2.weight\", \"segformer.encoder.block.2.2.layer_norm_2.bias\", \"segformer.encoder.block.2.2.mlp.dense1.weight\", \"segformer.encoder.block.2.2.mlp.dense1.bias\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.2.mlp.dense2.weight\", \"segformer.encoder.block.2.2.mlp.dense2.bias\", \"segformer.encoder.block.2.3.layer_norm_1.weight\", \"segformer.encoder.block.2.3.layer_norm_1.bias\", \"segformer.encoder.block.2.3.attention.self.query.weight\", \"segformer.encoder.block.2.3.attention.self.query.bias\", \"segformer.encoder.block.2.3.attention.self.key.weight\", \"segformer.encoder.block.2.3.attention.self.key.bias\", \"segformer.encoder.block.2.3.attention.self.value.weight\", \"segformer.encoder.block.2.3.attention.self.value.bias\", \"segformer.encoder.block.2.3.attention.self.sr.weight\", \"segformer.encoder.block.2.3.attention.self.sr.bias\", \"segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.3.attention.output.dense.weight\", \"segformer.encoder.block.2.3.attention.output.dense.bias\", \"segformer.encoder.block.2.3.layer_norm_2.weight\", \"segformer.encoder.block.2.3.layer_norm_2.bias\", \"segformer.encoder.block.2.3.mlp.dense1.weight\", \"segformer.encoder.block.2.3.mlp.dense1.bias\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.3.mlp.dense2.weight\", \"segformer.encoder.block.2.3.mlp.dense2.bias\", \"segformer.encoder.block.2.4.layer_norm_1.weight\", \"segformer.encoder.block.2.4.layer_norm_1.bias\", \"segformer.encoder.block.2.4.attention.self.query.weight\", \"segformer.encoder.block.2.4.attention.self.query.bias\", \"segformer.encoder.block.2.4.attention.self.key.weight\", \"segformer.encoder.block.2.4.attention.self.key.bias\", \"segformer.encoder.block.2.4.attention.self.value.weight\", \"segformer.encoder.block.2.4.attention.self.value.bias\", \"segformer.encoder.block.2.4.attention.self.sr.weight\", \"segformer.encoder.block.2.4.attention.self.sr.bias\", \"segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.4.attention.output.dense.weight\", \"segformer.encoder.block.2.4.attention.output.dense.bias\", \"segformer.encoder.block.2.4.layer_norm_2.weight\", \"segformer.encoder.block.2.4.layer_norm_2.bias\", \"segformer.encoder.block.2.4.mlp.dense1.weight\", \"segformer.encoder.block.2.4.mlp.dense1.bias\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.4.mlp.dense2.weight\", \"segformer.encoder.block.2.4.mlp.dense2.bias\", \"segformer.encoder.block.2.5.layer_norm_1.weight\", \"segformer.encoder.block.2.5.layer_norm_1.bias\", \"segformer.encoder.block.2.5.attention.self.query.weight\", \"segformer.encoder.block.2.5.attention.self.query.bias\", \"segformer.encoder.block.2.5.attention.self.key.weight\", \"segformer.encoder.block.2.5.attention.self.key.bias\", \"segformer.encoder.block.2.5.attention.self.value.weight\", \"segformer.encoder.block.2.5.attention.self.value.bias\", \"segformer.encoder.block.2.5.attention.self.sr.weight\", \"segformer.encoder.block.2.5.attention.self.sr.bias\", \"segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.5.attention.output.dense.weight\", \"segformer.encoder.block.2.5.attention.output.dense.bias\", \"segformer.encoder.block.2.5.layer_norm_2.weight\", \"segformer.encoder.block.2.5.layer_norm_2.bias\", \"segformer.encoder.block.2.5.mlp.dense1.weight\", \"segformer.encoder.block.2.5.mlp.dense1.bias\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.5.mlp.dense2.weight\", \"segformer.encoder.block.2.5.mlp.dense2.bias\", \"segformer.encoder.block.2.6.layer_norm_1.weight\", \"segformer.encoder.block.2.6.layer_norm_1.bias\", \"segformer.encoder.block.2.6.attention.self.query.weight\", \"segformer.encoder.block.2.6.attention.self.query.bias\", \"segformer.encoder.block.2.6.attention.self.key.weight\", \"segformer.encoder.block.2.6.attention.self.key.bias\", \"segformer.encoder.block.2.6.attention.self.value.weight\", \"segformer.encoder.block.2.6.attention.self.value.bias\", \"segformer.encoder.block.2.6.attention.self.sr.weight\", \"segformer.encoder.block.2.6.attention.self.sr.bias\", \"segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.6.attention.output.dense.weight\", \"segformer.encoder.block.2.6.attention.output.dense.bias\", \"segformer.encoder.block.2.6.layer_norm_2.weight\", \"segformer.encoder.block.2.6.layer_norm_2.bias\", \"segformer.encoder.block.2.6.mlp.dense1.weight\", \"segformer.encoder.block.2.6.mlp.dense1.bias\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.6.mlp.dense2.weight\", \"segformer.encoder.block.2.6.mlp.dense2.bias\", \"segformer.encoder.block.2.7.layer_norm_1.weight\", \"segformer.encoder.block.2.7.layer_norm_1.bias\", \"segformer.encoder.block.2.7.attention.self.query.weight\", \"segformer.encoder.block.2.7.attention.self.query.bias\", \"segformer.encoder.block.2.7.attention.self.key.weight\", \"segformer.encoder.block.2.7.attention.self.key.bias\", \"segformer.encoder.block.2.7.attention.self.value.weight\", \"segformer.encoder.block.2.7.attention.self.value.bias\", \"segformer.encoder.block.2.7.attention.self.sr.weight\", \"segformer.encoder.block.2.7.attention.self.sr.bias\", \"segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.7.attention.output.dense.weight\", \"segformer.encoder.block.2.7.attention.output.dense.bias\", \"segformer.encoder.block.2.7.layer_norm_2.weight\", \"segformer.encoder.block.2.7.layer_norm_2.bias\", \"segformer.encoder.block.2.7.mlp.dense1.weight\", \"segformer.encoder.block.2.7.mlp.dense1.bias\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.7.mlp.dense2.weight\", \"segformer.encoder.block.2.7.mlp.dense2.bias\", \"segformer.encoder.block.2.8.layer_norm_1.weight\", \"segformer.encoder.block.2.8.layer_norm_1.bias\", \"segformer.encoder.block.2.8.attention.self.query.weight\", \"segformer.encoder.block.2.8.attention.self.query.bias\", \"segformer.encoder.block.2.8.attention.self.key.weight\", \"segformer.encoder.block.2.8.attention.self.key.bias\", \"segformer.encoder.block.2.8.attention.self.value.weight\", \"segformer.encoder.block.2.8.attention.self.value.bias\", \"segformer.encoder.block.2.8.attention.self.sr.weight\", \"segformer.encoder.block.2.8.attention.self.sr.bias\", \"segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.8.attention.output.dense.weight\", \"segformer.encoder.block.2.8.attention.output.dense.bias\", \"segformer.encoder.block.2.8.layer_norm_2.weight\", \"segformer.encoder.block.2.8.layer_norm_2.bias\", \"segformer.encoder.block.2.8.mlp.dense1.weight\", \"segformer.encoder.block.2.8.mlp.dense1.bias\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.8.mlp.dense2.weight\", \"segformer.encoder.block.2.8.mlp.dense2.bias\", \"segformer.encoder.block.2.9.layer_norm_1.weight\", \"segformer.encoder.block.2.9.layer_norm_1.bias\", \"segformer.encoder.block.2.9.attention.self.query.weight\", \"segformer.encoder.block.2.9.attention.self.query.bias\", \"segformer.encoder.block.2.9.attention.self.key.weight\", \"segformer.encoder.block.2.9.attention.self.key.bias\", \"segformer.encoder.block.2.9.attention.self.value.weight\", \"segformer.encoder.block.2.9.attention.self.value.bias\", \"segformer.encoder.block.2.9.attention.self.sr.weight\", \"segformer.encoder.block.2.9.attention.self.sr.bias\", \"segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.9.attention.output.dense.weight\", \"segformer.encoder.block.2.9.attention.output.dense.bias\", \"segformer.encoder.block.2.9.layer_norm_2.weight\", \"segformer.encoder.block.2.9.layer_norm_2.bias\", \"segformer.encoder.block.2.9.mlp.dense1.weight\", \"segformer.encoder.block.2.9.mlp.dense1.bias\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.9.mlp.dense2.weight\", \"segformer.encoder.block.2.9.mlp.dense2.bias\", \"segformer.encoder.block.2.10.layer_norm_1.weight\", \"segformer.encoder.block.2.10.layer_norm_1.bias\", \"segformer.encoder.block.2.10.attention.self.query.weight\", \"segformer.encoder.block.2.10.attention.self.query.bias\", \"segformer.encoder.block.2.10.attention.self.key.weight\", \"segformer.encoder.block.2.10.attention.self.key.bias\", \"segformer.encoder.block.2.10.attention.self.value.weight\", \"segformer.encoder.block.2.10.attention.self.value.bias\", \"segformer.encoder.block.2.10.attention.self.sr.weight\", \"segformer.encoder.block.2.10.attention.self.sr.bias\", \"segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.10.attention.output.dense.weight\", \"segformer.encoder.block.2.10.attention.output.dense.bias\", \"segformer.encoder.block.2.10.layer_norm_2.weight\", \"segformer.encoder.block.2.10.layer_norm_2.bias\", \"segformer.encoder.block.2.10.mlp.dense1.weight\", \"segformer.encoder.block.2.10.mlp.dense1.bias\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.10.mlp.dense2.weight\", \"segformer.encoder.block.2.10.mlp.dense2.bias\", \"segformer.encoder.block.2.11.layer_norm_1.weight\", \"segformer.encoder.block.2.11.layer_norm_1.bias\", \"segformer.encoder.block.2.11.attention.self.query.weight\", \"segformer.encoder.block.2.11.attention.self.query.bias\", \"segformer.encoder.block.2.11.attention.self.key.weight\", \"segformer.encoder.block.2.11.attention.self.key.bias\", \"segformer.encoder.block.2.11.attention.self.value.weight\", \"segformer.encoder.block.2.11.attention.self.value.bias\", \"segformer.encoder.block.2.11.attention.self.sr.weight\", \"segformer.encoder.block.2.11.attention.self.sr.bias\", \"segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.11.attention.output.dense.weight\", \"segformer.encoder.block.2.11.attention.output.dense.bias\", \"segformer.encoder.block.2.11.layer_norm_2.weight\", \"segformer.encoder.block.2.11.layer_norm_2.bias\", \"segformer.encoder.block.2.11.mlp.dense1.weight\", \"segformer.encoder.block.2.11.mlp.dense1.bias\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.11.mlp.dense2.weight\", \"segformer.encoder.block.2.11.mlp.dense2.bias\", \"segformer.encoder.block.2.12.layer_norm_1.weight\", \"segformer.encoder.block.2.12.layer_norm_1.bias\", \"segformer.encoder.block.2.12.attention.self.query.weight\", \"segformer.encoder.block.2.12.attention.self.query.bias\", \"segformer.encoder.block.2.12.attention.self.key.weight\", \"segformer.encoder.block.2.12.attention.self.key.bias\", \"segformer.encoder.block.2.12.attention.self.value.weight\", \"segformer.encoder.block.2.12.attention.self.value.bias\", \"segformer.encoder.block.2.12.attention.self.sr.weight\", \"segformer.encoder.block.2.12.attention.self.sr.bias\", \"segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.12.attention.output.dense.weight\", \"segformer.encoder.block.2.12.attention.output.dense.bias\", \"segformer.encoder.block.2.12.layer_norm_2.weight\", \"segformer.encoder.block.2.12.layer_norm_2.bias\", \"segformer.encoder.block.2.12.mlp.dense1.weight\", \"segformer.encoder.block.2.12.mlp.dense1.bias\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.12.mlp.dense2.weight\", \"segformer.encoder.block.2.12.mlp.dense2.bias\", \"segformer.encoder.block.2.13.layer_norm_1.weight\", \"segformer.encoder.block.2.13.layer_norm_1.bias\", \"segformer.encoder.block.2.13.attention.self.query.weight\", \"segformer.encoder.block.2.13.attention.self.query.bias\", \"segformer.encoder.block.2.13.attention.self.key.weight\", \"segformer.encoder.block.2.13.attention.self.key.bias\", \"segformer.encoder.block.2.13.attention.self.value.weight\", \"segformer.encoder.block.2.13.attention.self.value.bias\", \"segformer.encoder.block.2.13.attention.self.sr.weight\", \"segformer.encoder.block.2.13.attention.self.sr.bias\", \"segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.13.attention.output.dense.weight\", \"segformer.encoder.block.2.13.attention.output.dense.bias\", \"segformer.encoder.block.2.13.layer_norm_2.weight\", \"segformer.encoder.block.2.13.layer_norm_2.bias\", \"segformer.encoder.block.2.13.mlp.dense1.weight\", \"segformer.encoder.block.2.13.mlp.dense1.bias\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.13.mlp.dense2.weight\", \"segformer.encoder.block.2.13.mlp.dense2.bias\", \"segformer.encoder.block.2.14.layer_norm_1.weight\", \"segformer.encoder.block.2.14.layer_norm_1.bias\", \"segformer.encoder.block.2.14.attention.self.query.weight\", \"segformer.encoder.block.2.14.attention.self.query.bias\", \"segformer.encoder.block.2.14.attention.self.key.weight\", \"segformer.encoder.block.2.14.attention.self.key.bias\", \"segformer.encoder.block.2.14.attention.self.value.weight\", \"segformer.encoder.block.2.14.attention.self.value.bias\", \"segformer.encoder.block.2.14.attention.self.sr.weight\", \"segformer.encoder.block.2.14.attention.self.sr.bias\", \"segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.14.attention.output.dense.weight\", \"segformer.encoder.block.2.14.attention.output.dense.bias\", \"segformer.encoder.block.2.14.layer_norm_2.weight\", \"segformer.encoder.block.2.14.layer_norm_2.bias\", \"segformer.encoder.block.2.14.mlp.dense1.weight\", \"segformer.encoder.block.2.14.mlp.dense1.bias\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.14.mlp.dense2.weight\", \"segformer.encoder.block.2.14.mlp.dense2.bias\", \"segformer.encoder.block.2.15.layer_norm_1.weight\", \"segformer.encoder.block.2.15.layer_norm_1.bias\", \"segformer.encoder.block.2.15.attention.self.query.weight\", \"segformer.encoder.block.2.15.attention.self.query.bias\", \"segformer.encoder.block.2.15.attention.self.key.weight\", \"segformer.encoder.block.2.15.attention.self.key.bias\", \"segformer.encoder.block.2.15.attention.self.value.weight\", \"segformer.encoder.block.2.15.attention.self.value.bias\", \"segformer.encoder.block.2.15.attention.self.sr.weight\", \"segformer.encoder.block.2.15.attention.self.sr.bias\", \"segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.15.attention.output.dense.weight\", \"segformer.encoder.block.2.15.attention.output.dense.bias\", \"segformer.encoder.block.2.15.layer_norm_2.weight\", \"segformer.encoder.block.2.15.layer_norm_2.bias\", \"segformer.encoder.block.2.15.mlp.dense1.weight\", \"segformer.encoder.block.2.15.mlp.dense1.bias\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.15.mlp.dense2.weight\", \"segformer.encoder.block.2.15.mlp.dense2.bias\", \"segformer.encoder.block.2.16.layer_norm_1.weight\", \"segformer.encoder.block.2.16.layer_norm_1.bias\", \"segformer.encoder.block.2.16.attention.self.query.weight\", \"segformer.encoder.block.2.16.attention.self.query.bias\", \"segformer.encoder.block.2.16.attention.self.key.weight\", \"segformer.encoder.block.2.16.attention.self.key.bias\", \"segformer.encoder.block.2.16.attention.self.value.weight\", \"segformer.encoder.block.2.16.attention.self.value.bias\", \"segformer.encoder.block.2.16.attention.self.sr.weight\", \"segformer.encoder.block.2.16.attention.self.sr.bias\", \"segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.16.attention.output.dense.weight\", \"segformer.encoder.block.2.16.attention.output.dense.bias\", \"segformer.encoder.block.2.16.layer_norm_2.weight\", \"segformer.encoder.block.2.16.layer_norm_2.bias\", \"segformer.encoder.block.2.16.mlp.dense1.weight\", \"segformer.encoder.block.2.16.mlp.dense1.bias\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.16.mlp.dense2.weight\", \"segformer.encoder.block.2.16.mlp.dense2.bias\", \"segformer.encoder.block.2.17.layer_norm_1.weight\", \"segformer.encoder.block.2.17.layer_norm_1.bias\", \"segformer.encoder.block.2.17.attention.self.query.weight\", \"segformer.encoder.block.2.17.attention.self.query.bias\", \"segformer.encoder.block.2.17.attention.self.key.weight\", \"segformer.encoder.block.2.17.attention.self.key.bias\", \"segformer.encoder.block.2.17.attention.self.value.weight\", \"segformer.encoder.block.2.17.attention.self.value.bias\", \"segformer.encoder.block.2.17.attention.self.sr.weight\", \"segformer.encoder.block.2.17.attention.self.sr.bias\", \"segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.17.attention.output.dense.weight\", \"segformer.encoder.block.2.17.attention.output.dense.bias\", \"segformer.encoder.block.2.17.layer_norm_2.weight\", \"segformer.encoder.block.2.17.layer_norm_2.bias\", \"segformer.encoder.block.2.17.mlp.dense1.weight\", \"segformer.encoder.block.2.17.mlp.dense1.bias\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.17.mlp.dense2.weight\", \"segformer.encoder.block.2.17.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.block.3.2.layer_norm_1.weight\", \"segformer.encoder.block.3.2.layer_norm_1.bias\", \"segformer.encoder.block.3.2.attention.self.query.weight\", \"segformer.encoder.block.3.2.attention.self.query.bias\", \"segformer.encoder.block.3.2.attention.self.key.weight\", \"segformer.encoder.block.3.2.attention.self.key.bias\", \"segformer.encoder.block.3.2.attention.self.value.weight\", \"segformer.encoder.block.3.2.attention.self.value.bias\", \"segformer.encoder.block.3.2.attention.output.dense.weight\", \"segformer.encoder.block.3.2.attention.output.dense.bias\", \"segformer.encoder.block.3.2.layer_norm_2.weight\", \"segformer.encoder.block.3.2.layer_norm_2.bias\", \"segformer.encoder.block.3.2.mlp.dense1.weight\", \"segformer.encoder.block.3.2.mlp.dense1.bias\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.2.mlp.dense2.weight\", \"segformer.encoder.block.3.2.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"module.segformer.encoder.patch_embeddings.0.proj.weight\", \"module.segformer.encoder.patch_embeddings.0.proj.bias\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.1.proj.weight\", \"module.segformer.encoder.patch_embeddings.1.proj.bias\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.2.proj.weight\", \"module.segformer.encoder.patch_embeddings.2.proj.bias\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.3.proj.weight\", \"module.segformer.encoder.patch_embeddings.3.proj.bias\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"module.segformer.encoder.block.0.0.layer_norm_1.weight\", \"module.segformer.encoder.block.0.0.layer_norm_1.bias\", \"module.segformer.encoder.block.0.0.attention.self.query.weight\", \"module.segformer.encoder.block.0.0.attention.self.query.bias\", \"module.segformer.encoder.block.0.0.attention.self.key.weight\", \"module.segformer.encoder.block.0.0.attention.self.key.bias\", \"module.segformer.encoder.block.0.0.attention.self.value.weight\", \"module.segformer.encoder.block.0.0.attention.self.value.bias\", \"module.segformer.encoder.block.0.0.attention.self.sr.weight\", \"module.segformer.encoder.block.0.0.attention.self.sr.bias\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.0.attention.output.dense.weight\", \"module.segformer.encoder.block.0.0.attention.output.dense.bias\", \"module.segformer.encoder.block.0.0.layer_norm_2.weight\", \"module.segformer.encoder.block.0.0.layer_norm_2.bias\", \"module.segformer.encoder.block.0.0.mlp.dense1.weight\", \"module.segformer.encoder.block.0.0.mlp.dense1.bias\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.0.mlp.dense2.weight\", \"module.segformer.encoder.block.0.0.mlp.dense2.bias\", \"module.segformer.encoder.block.0.1.layer_norm_1.weight\", \"module.segformer.encoder.block.0.1.layer_norm_1.bias\", \"module.segformer.encoder.block.0.1.attention.self.query.weight\", \"module.segformer.encoder.block.0.1.attention.self.query.bias\", \"module.segformer.encoder.block.0.1.attention.self.key.weight\", \"module.segformer.encoder.block.0.1.attention.self.key.bias\", \"module.segformer.encoder.block.0.1.attention.self.value.weight\", \"module.segformer.encoder.block.0.1.attention.self.value.bias\", \"module.segformer.encoder.block.0.1.attention.self.sr.weight\", \"module.segformer.encoder.block.0.1.attention.self.sr.bias\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.1.attention.output.dense.weight\", \"module.segformer.encoder.block.0.1.attention.output.dense.bias\", \"module.segformer.encoder.block.0.1.layer_norm_2.weight\", \"module.segformer.encoder.block.0.1.layer_norm_2.bias\", \"module.segformer.encoder.block.0.1.mlp.dense1.weight\", \"module.segformer.encoder.block.0.1.mlp.dense1.bias\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.1.mlp.dense2.weight\", \"module.segformer.encoder.block.0.1.mlp.dense2.bias\", \"module.segformer.encoder.block.0.2.layer_norm_1.weight\", \"module.segformer.encoder.block.0.2.layer_norm_1.bias\", \"module.segformer.encoder.block.0.2.attention.self.query.weight\", \"module.segformer.encoder.block.0.2.attention.self.query.bias\", \"module.segformer.encoder.block.0.2.attention.self.key.weight\", \"module.segformer.encoder.block.0.2.attention.self.key.bias\", \"module.segformer.encoder.block.0.2.attention.self.value.weight\", \"module.segformer.encoder.block.0.2.attention.self.value.bias\", \"module.segformer.encoder.block.0.2.attention.self.sr.weight\", \"module.segformer.encoder.block.0.2.attention.self.sr.bias\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.2.attention.output.dense.weight\", \"module.segformer.encoder.block.0.2.attention.output.dense.bias\", \"module.segformer.encoder.block.0.2.layer_norm_2.weight\", \"module.segformer.encoder.block.0.2.layer_norm_2.bias\", \"module.segformer.encoder.block.0.2.mlp.dense1.weight\", \"module.segformer.encoder.block.0.2.mlp.dense1.bias\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.2.mlp.dense2.weight\", \"module.segformer.encoder.block.0.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.0.layer_norm_1.weight\", \"module.segformer.encoder.block.1.0.layer_norm_1.bias\", \"module.segformer.encoder.block.1.0.attention.self.query.weight\", \"module.segformer.encoder.block.1.0.attention.self.query.bias\", \"module.segformer.encoder.block.1.0.attention.self.key.weight\", \"module.segformer.encoder.block.1.0.attention.self.key.bias\", \"module.segformer.encoder.block.1.0.attention.self.value.weight\", \"module.segformer.encoder.block.1.0.attention.self.value.bias\", \"module.segformer.encoder.block.1.0.attention.self.sr.weight\", \"module.segformer.encoder.block.1.0.attention.self.sr.bias\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.0.attention.output.dense.weight\", \"module.segformer.encoder.block.1.0.attention.output.dense.bias\", \"module.segformer.encoder.block.1.0.layer_norm_2.weight\", \"module.segformer.encoder.block.1.0.layer_norm_2.bias\", \"module.segformer.encoder.block.1.0.mlp.dense1.weight\", \"module.segformer.encoder.block.1.0.mlp.dense1.bias\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.0.mlp.dense2.weight\", \"module.segformer.encoder.block.1.0.mlp.dense2.bias\", \"module.segformer.encoder.block.1.1.layer_norm_1.weight\", \"module.segformer.encoder.block.1.1.layer_norm_1.bias\", \"module.segformer.encoder.block.1.1.attention.self.query.weight\", \"module.segformer.encoder.block.1.1.attention.self.query.bias\", \"module.segformer.encoder.block.1.1.attention.self.key.weight\", \"module.segformer.encoder.block.1.1.attention.self.key.bias\", \"module.segformer.encoder.block.1.1.attention.self.value.weight\", \"module.segformer.encoder.block.1.1.attention.self.value.bias\", \"module.segformer.encoder.block.1.1.attention.self.sr.weight\", \"module.segformer.encoder.block.1.1.attention.self.sr.bias\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.1.attention.output.dense.weight\", \"module.segformer.encoder.block.1.1.attention.output.dense.bias\", \"module.segformer.encoder.block.1.1.layer_norm_2.weight\", \"module.segformer.encoder.block.1.1.layer_norm_2.bias\", \"module.segformer.encoder.block.1.1.mlp.dense1.weight\", \"module.segformer.encoder.block.1.1.mlp.dense1.bias\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.1.mlp.dense2.weight\", \"module.segformer.encoder.block.1.1.mlp.dense2.bias\", \"module.segformer.encoder.block.1.2.layer_norm_1.weight\", \"module.segformer.encoder.block.1.2.layer_norm_1.bias\", \"module.segformer.encoder.block.1.2.attention.self.query.weight\", \"module.segformer.encoder.block.1.2.attention.self.query.bias\", \"module.segformer.encoder.block.1.2.attention.self.key.weight\", \"module.segformer.encoder.block.1.2.attention.self.key.bias\", \"module.segformer.encoder.block.1.2.attention.self.value.weight\", \"module.segformer.encoder.block.1.2.attention.self.value.bias\", \"module.segformer.encoder.block.1.2.attention.self.sr.weight\", \"module.segformer.encoder.block.1.2.attention.self.sr.bias\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.2.attention.output.dense.weight\", \"module.segformer.encoder.block.1.2.attention.output.dense.bias\", \"module.segformer.encoder.block.1.2.layer_norm_2.weight\", \"module.segformer.encoder.block.1.2.layer_norm_2.bias\", \"module.segformer.encoder.block.1.2.mlp.dense1.weight\", \"module.segformer.encoder.block.1.2.mlp.dense1.bias\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.2.mlp.dense2.weight\", \"module.segformer.encoder.block.1.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.3.layer_norm_1.weight\", \"module.segformer.encoder.block.1.3.layer_norm_1.bias\", \"module.segformer.encoder.block.1.3.attention.self.query.weight\", \"module.segformer.encoder.block.1.3.attention.self.query.bias\", \"module.segformer.encoder.block.1.3.attention.self.key.weight\", \"module.segformer.encoder.block.1.3.attention.self.key.bias\", \"module.segformer.encoder.block.1.3.attention.self.value.weight\", \"module.segformer.encoder.block.1.3.attention.self.value.bias\", \"module.segformer.encoder.block.1.3.attention.self.sr.weight\", \"module.segformer.encoder.block.1.3.attention.self.sr.bias\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.3.attention.output.dense.weight\", \"module.segformer.encoder.block.1.3.attention.output.dense.bias\", \"module.segformer.encoder.block.1.3.layer_norm_2.weight\", \"module.segformer.encoder.block.1.3.layer_norm_2.bias\", \"module.segformer.encoder.block.1.3.mlp.dense1.weight\", \"module.segformer.encoder.block.1.3.mlp.dense1.bias\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.3.mlp.dense2.weight\", \"module.segformer.encoder.block.1.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.0.layer_norm_1.weight\", \"module.segformer.encoder.block.2.0.layer_norm_1.bias\", \"module.segformer.encoder.block.2.0.attention.self.query.weight\", \"module.segformer.encoder.block.2.0.attention.self.query.bias\", \"module.segformer.encoder.block.2.0.attention.self.key.weight\", \"module.segformer.encoder.block.2.0.attention.self.key.bias\", \"module.segformer.encoder.block.2.0.attention.self.value.weight\", \"module.segformer.encoder.block.2.0.attention.self.value.bias\", \"module.segformer.encoder.block.2.0.attention.self.sr.weight\", \"module.segformer.encoder.block.2.0.attention.self.sr.bias\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.0.attention.output.dense.weight\", \"module.segformer.encoder.block.2.0.attention.output.dense.bias\", \"module.segformer.encoder.block.2.0.layer_norm_2.weight\", \"module.segformer.encoder.block.2.0.layer_norm_2.bias\", \"module.segformer.encoder.block.2.0.mlp.dense1.weight\", \"module.segformer.encoder.block.2.0.mlp.dense1.bias\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.0.mlp.dense2.weight\", \"module.segformer.encoder.block.2.0.mlp.dense2.bias\", \"module.segformer.encoder.block.2.1.layer_norm_1.weight\", \"module.segformer.encoder.block.2.1.layer_norm_1.bias\", \"module.segformer.encoder.block.2.1.attention.self.query.weight\", \"module.segformer.encoder.block.2.1.attention.self.query.bias\", \"module.segformer.encoder.block.2.1.attention.self.key.weight\", \"module.segformer.encoder.block.2.1.attention.self.key.bias\", \"module.segformer.encoder.block.2.1.attention.self.value.weight\", \"module.segformer.encoder.block.2.1.attention.self.value.bias\", \"module.segformer.encoder.block.2.1.attention.self.sr.weight\", \"module.segformer.encoder.block.2.1.attention.self.sr.bias\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.1.attention.output.dense.weight\", \"module.segformer.encoder.block.2.1.attention.output.dense.bias\", \"module.segformer.encoder.block.2.1.layer_norm_2.weight\", \"module.segformer.encoder.block.2.1.layer_norm_2.bias\", \"module.segformer.encoder.block.2.1.mlp.dense1.weight\", \"module.segformer.encoder.block.2.1.mlp.dense1.bias\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.1.mlp.dense2.weight\", \"module.segformer.encoder.block.2.1.mlp.dense2.bias\", \"module.segformer.encoder.block.2.2.layer_norm_1.weight\", \"module.segformer.encoder.block.2.2.layer_norm_1.bias\", \"module.segformer.encoder.block.2.2.attention.self.query.weight\", \"module.segformer.encoder.block.2.2.attention.self.query.bias\", \"module.segformer.encoder.block.2.2.attention.self.key.weight\", \"module.segformer.encoder.block.2.2.attention.self.key.bias\", \"module.segformer.encoder.block.2.2.attention.self.value.weight\", \"module.segformer.encoder.block.2.2.attention.self.value.bias\", \"module.segformer.encoder.block.2.2.attention.self.sr.weight\", \"module.segformer.encoder.block.2.2.attention.self.sr.bias\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.2.attention.output.dense.weight\", \"module.segformer.encoder.block.2.2.attention.output.dense.bias\", \"module.segformer.encoder.block.2.2.layer_norm_2.weight\", \"module.segformer.encoder.block.2.2.layer_norm_2.bias\", \"module.segformer.encoder.block.2.2.mlp.dense1.weight\", \"module.segformer.encoder.block.2.2.mlp.dense1.bias\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.2.mlp.dense2.weight\", \"module.segformer.encoder.block.2.2.mlp.dense2.bias\", \"module.segformer.encoder.block.2.3.layer_norm_1.weight\", \"module.segformer.encoder.block.2.3.layer_norm_1.bias\", \"module.segformer.encoder.block.2.3.attention.self.query.weight\", \"module.segformer.encoder.block.2.3.attention.self.query.bias\", \"module.segformer.encoder.block.2.3.attention.self.key.weight\", \"module.segformer.encoder.block.2.3.attention.self.key.bias\", \"module.segformer.encoder.block.2.3.attention.self.value.weight\", \"module.segformer.encoder.block.2.3.attention.self.value.bias\", \"module.segformer.encoder.block.2.3.attention.self.sr.weight\", \"module.segformer.encoder.block.2.3.attention.self.sr.bias\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.3.attention.output.dense.weight\", \"module.segformer.encoder.block.2.3.attention.output.dense.bias\", \"module.segformer.encoder.block.2.3.layer_norm_2.weight\", \"module.segformer.encoder.block.2.3.layer_norm_2.bias\", \"module.segformer.encoder.block.2.3.mlp.dense1.weight\", \"module.segformer.encoder.block.2.3.mlp.dense1.bias\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.3.mlp.dense2.weight\", \"module.segformer.encoder.block.2.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.4.layer_norm_1.weight\", \"module.segformer.encoder.block.2.4.layer_norm_1.bias\", \"module.segformer.encoder.block.2.4.attention.self.query.weight\", \"module.segformer.encoder.block.2.4.attention.self.query.bias\", \"module.segformer.encoder.block.2.4.attention.self.key.weight\", \"module.segformer.encoder.block.2.4.attention.self.key.bias\", \"module.segformer.encoder.block.2.4.attention.self.value.weight\", \"module.segformer.encoder.block.2.4.attention.self.value.bias\", \"module.segformer.encoder.block.2.4.attention.self.sr.weight\", \"module.segformer.encoder.block.2.4.attention.self.sr.bias\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.4.attention.output.dense.weight\", \"module.segformer.encoder.block.2.4.attention.output.dense.bias\", \"module.segformer.encoder.block.2.4.layer_norm_2.weight\", \"module.segformer.encoder.block.2.4.layer_norm_2.bias\", \"module.segformer.encoder.block.2.4.mlp.dense1.weight\", \"module.segformer.encoder.block.2.4.mlp.dense1.bias\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.4.mlp.dense2.weight\", \"module.segformer.encoder.block.2.4.mlp.dense2.bias\", \"module.segformer.encoder.block.2.5.layer_norm_1.weight\", \"module.segformer.encoder.block.2.5.layer_norm_1.bias\", \"module.segformer.encoder.block.2.5.attention.self.query.weight\", \"module.segformer.encoder.block.2.5.attention.self.query.bias\", \"module.segformer.encoder.block.2.5.attention.self.key.weight\", \"module.segformer.encoder.block.2.5.attention.self.key.bias\", \"module.segformer.encoder.block.2.5.attention.self.value.weight\", \"module.segformer.encoder.block.2.5.attention.self.value.bias\", \"module.segformer.encoder.block.2.5.attention.self.sr.weight\", \"module.segformer.encoder.block.2.5.attention.self.sr.bias\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.5.attention.output.dense.weight\", \"module.segformer.encoder.block.2.5.attention.output.dense.bias\", \"module.segformer.encoder.block.2.5.layer_norm_2.weight\", \"module.segformer.encoder.block.2.5.layer_norm_2.bias\", \"module.segformer.encoder.block.2.5.mlp.dense1.weight\", \"module.segformer.encoder.block.2.5.mlp.dense1.bias\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.5.mlp.dense2.weight\", \"module.segformer.encoder.block.2.5.mlp.dense2.bias\", \"module.segformer.encoder.block.2.6.layer_norm_1.weight\", \"module.segformer.encoder.block.2.6.layer_norm_1.bias\", \"module.segformer.encoder.block.2.6.attention.self.query.weight\", \"module.segformer.encoder.block.2.6.attention.self.query.bias\", \"module.segformer.encoder.block.2.6.attention.self.key.weight\", \"module.segformer.encoder.block.2.6.attention.self.key.bias\", \"module.segformer.encoder.block.2.6.attention.self.value.weight\", \"module.segformer.encoder.block.2.6.attention.self.value.bias\", \"module.segformer.encoder.block.2.6.attention.self.sr.weight\", \"module.segformer.encoder.block.2.6.attention.self.sr.bias\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.6.attention.output.dense.weight\", \"module.segformer.encoder.block.2.6.attention.output.dense.bias\", \"module.segformer.encoder.block.2.6.layer_norm_2.weight\", \"module.segformer.encoder.block.2.6.layer_norm_2.bias\", \"module.segformer.encoder.block.2.6.mlp.dense1.weight\", \"module.segformer.encoder.block.2.6.mlp.dense1.bias\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.6.mlp.dense2.weight\", \"module.segformer.encoder.block.2.6.mlp.dense2.bias\", \"module.segformer.encoder.block.2.7.layer_norm_1.weight\", \"module.segformer.encoder.block.2.7.layer_norm_1.bias\", \"module.segformer.encoder.block.2.7.attention.self.query.weight\", \"module.segformer.encoder.block.2.7.attention.self.query.bias\", \"module.segformer.encoder.block.2.7.attention.self.key.weight\", \"module.segformer.encoder.block.2.7.attention.self.key.bias\", \"module.segformer.encoder.block.2.7.attention.self.value.weight\", \"module.segformer.encoder.block.2.7.attention.self.value.bias\", \"module.segformer.encoder.block.2.7.attention.self.sr.weight\", \"module.segformer.encoder.block.2.7.attention.self.sr.bias\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.7.attention.output.dense.weight\", \"module.segformer.encoder.block.2.7.attention.output.dense.bias\", \"module.segformer.encoder.block.2.7.layer_norm_2.weight\", \"module.segformer.encoder.block.2.7.layer_norm_2.bias\", \"module.segformer.encoder.block.2.7.mlp.dense1.weight\", \"module.segformer.encoder.block.2.7.mlp.dense1.bias\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.7.mlp.dense2.weight\", \"module.segformer.encoder.block.2.7.mlp.dense2.bias\", \"module.segformer.encoder.block.2.8.layer_norm_1.weight\", \"module.segformer.encoder.block.2.8.layer_norm_1.bias\", \"module.segformer.encoder.block.2.8.attention.self.query.weight\", \"module.segformer.encoder.block.2.8.attention.self.query.bias\", \"module.segformer.encoder.block.2.8.attention.self.key.weight\", \"module.segformer.encoder.block.2.8.attention.self.key.bias\", \"module.segformer.encoder.block.2.8.attention.self.value.weight\", \"module.segformer.encoder.block.2.8.attention.self.value.bias\", \"module.segformer.encoder.block.2.8.attention.self.sr.weight\", \"module.segformer.encoder.block.2.8.attention.self.sr.bias\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.8.attention.output.dense.weight\", \"module.segformer.encoder.block.2.8.attention.output.dense.bias\", \"module.segformer.encoder.block.2.8.layer_norm_2.weight\", \"module.segformer.encoder.block.2.8.layer_norm_2.bias\", \"module.segformer.encoder.block.2.8.mlp.dense1.weight\", \"module.segformer.encoder.block.2.8.mlp.dense1.bias\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.8.mlp.dense2.weight\", \"module.segformer.encoder.block.2.8.mlp.dense2.bias\", \"module.segformer.encoder.block.2.9.layer_norm_1.weight\", \"module.segformer.encoder.block.2.9.layer_norm_1.bias\", \"module.segformer.encoder.block.2.9.attention.self.query.weight\", \"module.segformer.encoder.block.2.9.attention.self.query.bias\", \"module.segformer.encoder.block.2.9.attention.self.key.weight\", \"module.segformer.encoder.block.2.9.attention.self.key.bias\", \"module.segformer.encoder.block.2.9.attention.self.value.weight\", \"module.segformer.encoder.block.2.9.attention.self.value.bias\", \"module.segformer.encoder.block.2.9.attention.self.sr.weight\", \"module.segformer.encoder.block.2.9.attention.self.sr.bias\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.9.attention.output.dense.weight\", \"module.segformer.encoder.block.2.9.attention.output.dense.bias\", \"module.segformer.encoder.block.2.9.layer_norm_2.weight\", \"module.segformer.encoder.block.2.9.layer_norm_2.bias\", \"module.segformer.encoder.block.2.9.mlp.dense1.weight\", \"module.segformer.encoder.block.2.9.mlp.dense1.bias\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.9.mlp.dense2.weight\", \"module.segformer.encoder.block.2.9.mlp.dense2.bias\", \"module.segformer.encoder.block.2.10.layer_norm_1.weight\", \"module.segformer.encoder.block.2.10.layer_norm_1.bias\", \"module.segformer.encoder.block.2.10.attention.self.query.weight\", \"module.segformer.encoder.block.2.10.attention.self.query.bias\", \"module.segformer.encoder.block.2.10.attention.self.key.weight\", \"module.segformer.encoder.block.2.10.attention.self.key.bias\", \"module.segformer.encoder.block.2.10.attention.self.value.weight\", \"module.segformer.encoder.block.2.10.attention.self.value.bias\", \"module.segformer.encoder.block.2.10.attention.self.sr.weight\", \"module.segformer.encoder.block.2.10.attention.self.sr.bias\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.10.attention.output.dense.weight\", \"module.segformer.encoder.block.2.10.attention.output.dense.bias\", \"module.segformer.encoder.block.2.10.layer_norm_2.weight\", \"module.segformer.encoder.block.2.10.layer_norm_2.bias\", \"module.segformer.encoder.block.2.10.mlp.dense1.weight\", \"module.segformer.encoder.block.2.10.mlp.dense1.bias\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.10.mlp.dense2.weight\", \"module.segformer.encoder.block.2.10.mlp.dense2.bias\", \"module.segformer.encoder.block.2.11.layer_norm_1.weight\", \"module.segformer.encoder.block.2.11.layer_norm_1.bias\", \"module.segformer.encoder.block.2.11.attention.self.query.weight\", \"module.segformer.encoder.block.2.11.attention.self.query.bias\", \"module.segformer.encoder.block.2.11.attention.self.key.weight\", \"module.segformer.encoder.block.2.11.attention.self.key.bias\", \"module.segformer.encoder.block.2.11.attention.self.value.weight\", \"module.segformer.encoder.block.2.11.attention.self.value.bias\", \"module.segformer.encoder.block.2.11.attention.self.sr.weight\", \"module.segformer.encoder.block.2.11.attention.self.sr.bias\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.11.attention.output.dense.weight\", \"module.segformer.encoder.block.2.11.attention.output.dense.bias\", \"module.segformer.encoder.block.2.11.layer_norm_2.weight\", \"module.segformer.encoder.block.2.11.layer_norm_2.bias\", \"module.segformer.encoder.block.2.11.mlp.dense1.weight\", \"module.segformer.encoder.block.2.11.mlp.dense1.bias\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.11.mlp.dense2.weight\", \"module.segformer.encoder.block.2.11.mlp.dense2.bias\", \"module.segformer.encoder.block.2.12.layer_norm_1.weight\", \"module.segformer.encoder.block.2.12.layer_norm_1.bias\", \"module.segformer.encoder.block.2.12.attention.self.query.weight\", \"module.segformer.encoder.block.2.12.attention.self.query.bias\", \"module.segformer.encoder.block.2.12.attention.self.key.weight\", \"module.segformer.encoder.block.2.12.attention.self.key.bias\", \"module.segformer.encoder.block.2.12.attention.self.value.weight\", \"module.segformer.encoder.block.2.12.attention.self.value.bias\", \"module.segformer.encoder.block.2.12.attention.self.sr.weight\", \"module.segformer.encoder.block.2.12.attention.self.sr.bias\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.12.attention.output.dense.weight\", \"module.segformer.encoder.block.2.12.attention.output.dense.bias\", \"module.segformer.encoder.block.2.12.layer_norm_2.weight\", \"module.segformer.encoder.block.2.12.layer_norm_2.bias\", \"module.segformer.encoder.block.2.12.mlp.dense1.weight\", \"module.segformer.encoder.block.2.12.mlp.dense1.bias\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.12.mlp.dense2.weight\", \"module.segformer.encoder.block.2.12.mlp.dense2.bias\", \"module.segformer.encoder.block.2.13.layer_norm_1.weight\", \"module.segformer.encoder.block.2.13.layer_norm_1.bias\", \"module.segformer.encoder.block.2.13.attention.self.query.weight\", \"module.segformer.encoder.block.2.13.attention.self.query.bias\", \"module.segformer.encoder.block.2.13.attention.self.key.weight\", \"module.segformer.encoder.block.2.13.attention.self.key.bias\", \"module.segformer.encoder.block.2.13.attention.self.value.weight\", \"module.segformer.encoder.block.2.13.attention.self.value.bias\", \"module.segformer.encoder.block.2.13.attention.self.sr.weight\", \"module.segformer.encoder.block.2.13.attention.self.sr.bias\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.13.attention.output.dense.weight\", \"module.segformer.encoder.block.2.13.attention.output.dense.bias\", \"module.segformer.encoder.block.2.13.layer_norm_2.weight\", \"module.segformer.encoder.block.2.13.layer_norm_2.bias\", \"module.segformer.encoder.block.2.13.mlp.dense1.weight\", \"module.segformer.encoder.block.2.13.mlp.dense1.bias\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.13.mlp.dense2.weight\", \"module.segformer.encoder.block.2.13.mlp.dense2.bias\", \"module.segformer.encoder.block.2.14.layer_norm_1.weight\", \"module.segformer.encoder.block.2.14.layer_norm_1.bias\", \"module.segformer.encoder.block.2.14.attention.self.query.weight\", \"module.segformer.encoder.block.2.14.attention.self.query.bias\", \"module.segformer.encoder.block.2.14.attention.self.key.weight\", \"module.segformer.encoder.block.2.14.attention.self.key.bias\", \"module.segformer.encoder.block.2.14.attention.self.value.weight\", \"module.segformer.encoder.block.2.14.attention.self.value.bias\", \"module.segformer.encoder.block.2.14.attention.self.sr.weight\", \"module.segformer.encoder.block.2.14.attention.self.sr.bias\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.14.attention.output.dense.weight\", \"module.segformer.encoder.block.2.14.attention.output.dense.bias\", \"module.segformer.encoder.block.2.14.layer_norm_2.weight\", \"module.segformer.encoder.block.2.14.layer_norm_2.bias\", \"module.segformer.encoder.block.2.14.mlp.dense1.weight\", \"module.segformer.encoder.block.2.14.mlp.dense1.bias\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.14.mlp.dense2.weight\", \"module.segformer.encoder.block.2.14.mlp.dense2.bias\", \"module.segformer.encoder.block.2.15.layer_norm_1.weight\", \"module.segformer.encoder.block.2.15.layer_norm_1.bias\", \"module.segformer.encoder.block.2.15.attention.self.query.weight\", \"module.segformer.encoder.block.2.15.attention.self.query.bias\", \"module.segformer.encoder.block.2.15.attention.self.key.weight\", \"module.segformer.encoder.block.2.15.attention.self.key.bias\", \"module.segformer.encoder.block.2.15.attention.self.value.weight\", \"module.segformer.encoder.block.2.15.attention.self.value.bias\", \"module.segformer.encoder.block.2.15.attention.self.sr.weight\", \"module.segformer.encoder.block.2.15.attention.self.sr.bias\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.15.attention.output.dense.weight\", \"module.segformer.encoder.block.2.15.attention.output.dense.bias\", \"module.segformer.encoder.block.2.15.layer_norm_2.weight\", \"module.segformer.encoder.block.2.15.layer_norm_2.bias\", \"module.segformer.encoder.block.2.15.mlp.dense1.weight\", \"module.segformer.encoder.block.2.15.mlp.dense1.bias\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.15.mlp.dense2.weight\", \"module.segformer.encoder.block.2.15.mlp.dense2.bias\", \"module.segformer.encoder.block.2.16.layer_norm_1.weight\", \"module.segformer.encoder.block.2.16.layer_norm_1.bias\", \"module.segformer.encoder.block.2.16.attention.self.query.weight\", \"module.segformer.encoder.block.2.16.attention.self.query.bias\", \"module.segformer.encoder.block.2.16.attention.self.key.weight\", \"module.segformer.encoder.block.2.16.attention.self.key.bias\", \"module.segformer.encoder.block.2.16.attention.self.value.weight\", \"module.segformer.encoder.block.2.16.attention.self.value.bias\", \"module.segformer.encoder.block.2.16.attention.self.sr.weight\", \"module.segformer.encoder.block.2.16.attention.self.sr.bias\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.16.attention.output.dense.weight\", \"module.segformer.encoder.block.2.16.attention.output.dense.bias\", \"module.segformer.encoder.block.2.16.layer_norm_2.weight\", \"module.segformer.encoder.block.2.16.layer_norm_2.bias\", \"module.segformer.encoder.block.2.16.mlp.dense1.weight\", \"module.segformer.encoder.block.2.16.mlp.dense1.bias\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.16.mlp.dense2.weight\", \"module.segformer.encoder.block.2.16.mlp.dense2.bias\", \"module.segformer.encoder.block.2.17.layer_norm_1.weight\", \"module.segformer.encoder.block.2.17.layer_norm_1.bias\", \"module.segformer.encoder.block.2.17.attention.self.query.weight\", \"module.segformer.encoder.block.2.17.attention.self.query.bias\", \"module.segformer.encoder.block.2.17.attention.self.key.weight\", \"module.segformer.encoder.block.2.17.attention.self.key.bias\", \"module.segformer.encoder.block.2.17.attention.self.value.weight\", \"module.segformer.encoder.block.2.17.attention.self.value.bias\", \"module.segformer.encoder.block.2.17.attention.self.sr.weight\", \"module.segformer.encoder.block.2.17.attention.self.sr.bias\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.17.attention.output.dense.weight\", \"module.segformer.encoder.block.2.17.attention.output.dense.bias\", \"module.segformer.encoder.block.2.17.layer_norm_2.weight\", \"module.segformer.encoder.block.2.17.layer_norm_2.bias\", \"module.segformer.encoder.block.2.17.mlp.dense1.weight\", \"module.segformer.encoder.block.2.17.mlp.dense1.bias\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.17.mlp.dense2.weight\", \"module.segformer.encoder.block.2.17.mlp.dense2.bias\", \"module.segformer.encoder.block.3.0.layer_norm_1.weight\", \"module.segformer.encoder.block.3.0.layer_norm_1.bias\", \"module.segformer.encoder.block.3.0.attention.self.query.weight\", \"module.segformer.encoder.block.3.0.attention.self.query.bias\", \"module.segformer.encoder.block.3.0.attention.self.key.weight\", \"module.segformer.encoder.block.3.0.attention.self.key.bias\", \"module.segformer.encoder.block.3.0.attention.self.value.weight\", \"module.segformer.encoder.block.3.0.attention.self.value.bias\", \"module.segformer.encoder.block.3.0.attention.output.dense.weight\", \"module.segformer.encoder.block.3.0.attention.output.dense.bias\", \"module.segformer.encoder.block.3.0.layer_norm_2.weight\", \"module.segformer.encoder.block.3.0.layer_norm_2.bias\", \"module.segformer.encoder.block.3.0.mlp.dense1.weight\", \"module.segformer.encoder.block.3.0.mlp.dense1.bias\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.0.mlp.dense2.weight\", \"module.segformer.encoder.block.3.0.mlp.dense2.bias\", \"module.segformer.encoder.block.3.1.layer_norm_1.weight\", \"module.segformer.encoder.block.3.1.layer_norm_1.bias\", \"module.segformer.encoder.block.3.1.attention.self.query.weight\", \"module.segformer.encoder.block.3.1.attention.self.query.bias\", \"module.segformer.encoder.block.3.1.attention.self.key.weight\", \"module.segformer.encoder.block.3.1.attention.self.key.bias\", \"module.segformer.encoder.block.3.1.attention.self.value.weight\", \"module.segformer.encoder.block.3.1.attention.self.value.bias\", \"module.segformer.encoder.block.3.1.attention.output.dense.weight\", \"module.segformer.encoder.block.3.1.attention.output.dense.bias\", \"module.segformer.encoder.block.3.1.layer_norm_2.weight\", \"module.segformer.encoder.block.3.1.layer_norm_2.bias\", \"module.segformer.encoder.block.3.1.mlp.dense1.weight\", \"module.segformer.encoder.block.3.1.mlp.dense1.bias\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.1.mlp.dense2.weight\", \"module.segformer.encoder.block.3.1.mlp.dense2.bias\", \"module.segformer.encoder.block.3.2.layer_norm_1.weight\", \"module.segformer.encoder.block.3.2.layer_norm_1.bias\", \"module.segformer.encoder.block.3.2.attention.self.query.weight\", \"module.segformer.encoder.block.3.2.attention.self.query.bias\", \"module.segformer.encoder.block.3.2.attention.self.key.weight\", \"module.segformer.encoder.block.3.2.attention.self.key.bias\", \"module.segformer.encoder.block.3.2.attention.self.value.weight\", \"module.segformer.encoder.block.3.2.attention.self.value.bias\", \"module.segformer.encoder.block.3.2.attention.output.dense.weight\", \"module.segformer.encoder.block.3.2.attention.output.dense.bias\", \"module.segformer.encoder.block.3.2.layer_norm_2.weight\", \"module.segformer.encoder.block.3.2.layer_norm_2.bias\", \"module.segformer.encoder.block.3.2.mlp.dense1.weight\", \"module.segformer.encoder.block.3.2.mlp.dense1.bias\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.2.mlp.dense2.weight\", \"module.segformer.encoder.block.3.2.mlp.dense2.bias\", \"module.segformer.encoder.layer_norm.0.weight\", \"module.segformer.encoder.layer_norm.0.bias\", \"module.segformer.encoder.layer_norm.1.weight\", \"module.segformer.encoder.layer_norm.1.bias\", \"module.segformer.encoder.layer_norm.2.weight\", \"module.segformer.encoder.layer_norm.2.bias\", \"module.segformer.encoder.layer_norm.3.weight\", \"module.segformer.encoder.layer_norm.3.bias\", \"module.decode_head.linear_c.0.proj.weight\", \"module.decode_head.linear_c.0.proj.bias\", \"module.decode_head.linear_c.1.proj.weight\", \"module.decode_head.linear_c.1.proj.bias\", \"module.decode_head.linear_c.2.proj.weight\", \"module.decode_head.linear_c.2.proj.bias\", \"module.decode_head.linear_c.3.proj.weight\", \"module.decode_head.linear_c.3.proj.bias\", \"module.decode_head.linear_fuse.weight\", \"module.decode_head.batch_norm.weight\", \"module.decode_head.batch_norm.bias\", \"module.decode_head.batch_norm.running_mean\", \"module.decode_head.batch_norm.running_var\", \"module.decode_head.batch_norm.num_batches_tracked\", \"module.decode_head.classifier.weight\", \"module.decode_head.classifier.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Load the best checkpoint\u001b[39;00m\n\u001b[1;32m     69\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     73\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model_on_test_set(model, X_test_normalized, y_test, processor)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.0.2.layer_norm_1.weight\", \"segformer.encoder.block.0.2.layer_norm_1.bias\", \"segformer.encoder.block.0.2.attention.self.query.weight\", \"segformer.encoder.block.0.2.attention.self.query.bias\", \"segformer.encoder.block.0.2.attention.self.key.weight\", \"segformer.encoder.block.0.2.attention.self.key.bias\", \"segformer.encoder.block.0.2.attention.self.value.weight\", \"segformer.encoder.block.0.2.attention.self.value.bias\", \"segformer.encoder.block.0.2.attention.self.sr.weight\", \"segformer.encoder.block.0.2.attention.self.sr.bias\", \"segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.2.attention.output.dense.weight\", \"segformer.encoder.block.0.2.attention.output.dense.bias\", \"segformer.encoder.block.0.2.layer_norm_2.weight\", \"segformer.encoder.block.0.2.layer_norm_2.bias\", \"segformer.encoder.block.0.2.mlp.dense1.weight\", \"segformer.encoder.block.0.2.mlp.dense1.bias\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.2.mlp.dense2.weight\", \"segformer.encoder.block.0.2.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.1.2.layer_norm_1.weight\", \"segformer.encoder.block.1.2.layer_norm_1.bias\", \"segformer.encoder.block.1.2.attention.self.query.weight\", \"segformer.encoder.block.1.2.attention.self.query.bias\", \"segformer.encoder.block.1.2.attention.self.key.weight\", \"segformer.encoder.block.1.2.attention.self.key.bias\", \"segformer.encoder.block.1.2.attention.self.value.weight\", \"segformer.encoder.block.1.2.attention.self.value.bias\", \"segformer.encoder.block.1.2.attention.self.sr.weight\", \"segformer.encoder.block.1.2.attention.self.sr.bias\", \"segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.2.attention.output.dense.weight\", \"segformer.encoder.block.1.2.attention.output.dense.bias\", \"segformer.encoder.block.1.2.layer_norm_2.weight\", \"segformer.encoder.block.1.2.layer_norm_2.bias\", \"segformer.encoder.block.1.2.mlp.dense1.weight\", \"segformer.encoder.block.1.2.mlp.dense1.bias\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.2.mlp.dense2.weight\", \"segformer.encoder.block.1.2.mlp.dense2.bias\", \"segformer.encoder.block.1.3.layer_norm_1.weight\", \"segformer.encoder.block.1.3.layer_norm_1.bias\", \"segformer.encoder.block.1.3.attention.self.query.weight\", \"segformer.encoder.block.1.3.attention.self.query.bias\", \"segformer.encoder.block.1.3.attention.self.key.weight\", \"segformer.encoder.block.1.3.attention.self.key.bias\", \"segformer.encoder.block.1.3.attention.self.value.weight\", \"segformer.encoder.block.1.3.attention.self.value.bias\", \"segformer.encoder.block.1.3.attention.self.sr.weight\", \"segformer.encoder.block.1.3.attention.self.sr.bias\", \"segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.3.attention.output.dense.weight\", \"segformer.encoder.block.1.3.attention.output.dense.bias\", \"segformer.encoder.block.1.3.layer_norm_2.weight\", \"segformer.encoder.block.1.3.layer_norm_2.bias\", \"segformer.encoder.block.1.3.mlp.dense1.weight\", \"segformer.encoder.block.1.3.mlp.dense1.bias\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.3.mlp.dense2.weight\", \"segformer.encoder.block.1.3.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.2.2.layer_norm_1.weight\", \"segformer.encoder.block.2.2.layer_norm_1.bias\", \"segformer.encoder.block.2.2.attention.self.query.weight\", \"segformer.encoder.block.2.2.attention.self.query.bias\", \"segformer.encoder.block.2.2.attention.self.key.weight\", \"segformer.encoder.block.2.2.attention.self.key.bias\", \"segformer.encoder.block.2.2.attention.self.value.weight\", \"segformer.encoder.block.2.2.attention.self.value.bias\", \"segformer.encoder.block.2.2.attention.self.sr.weight\", \"segformer.encoder.block.2.2.attention.self.sr.bias\", \"segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.2.attention.output.dense.weight\", \"segformer.encoder.block.2.2.attention.output.dense.bias\", \"segformer.encoder.block.2.2.layer_norm_2.weight\", \"segformer.encoder.block.2.2.layer_norm_2.bias\", \"segformer.encoder.block.2.2.mlp.dense1.weight\", \"segformer.encoder.block.2.2.mlp.dense1.bias\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.2.mlp.dense2.weight\", \"segformer.encoder.block.2.2.mlp.dense2.bias\", \"segformer.encoder.block.2.3.layer_norm_1.weight\", \"segformer.encoder.block.2.3.layer_norm_1.bias\", \"segformer.encoder.block.2.3.attention.self.query.weight\", \"segformer.encoder.block.2.3.attention.self.query.bias\", \"segformer.encoder.block.2.3.attention.self.key.weight\", \"segformer.encoder.block.2.3.attention.self.key.bias\", \"segformer.encoder.block.2.3.attention.self.value.weight\", \"segformer.encoder.block.2.3.attention.self.value.bias\", \"segformer.encoder.block.2.3.attention.self.sr.weight\", \"segformer.encoder.block.2.3.attention.self.sr.bias\", \"segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.3.attention.output.dense.weight\", \"segformer.encoder.block.2.3.attention.output.dense.bias\", \"segformer.encoder.block.2.3.layer_norm_2.weight\", \"segformer.encoder.block.2.3.layer_norm_2.bias\", \"segformer.encoder.block.2.3.mlp.dense1.weight\", \"segformer.encoder.block.2.3.mlp.dense1.bias\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.3.mlp.dense2.weight\", \"segformer.encoder.block.2.3.mlp.dense2.bias\", \"segformer.encoder.block.2.4.layer_norm_1.weight\", \"segformer.encoder.block.2.4.layer_norm_1.bias\", \"segformer.encoder.block.2.4.attention.self.query.weight\", \"segformer.encoder.block.2.4.attention.self.query.bias\", \"segformer.encoder.block.2.4.attention.self.key.weight\", \"segformer.encoder.block.2.4.attention.self.key.bias\", \"segformer.encoder.block.2.4.attention.self.value.weight\", \"segformer.encoder.block.2.4.attention.self.value.bias\", \"segformer.encoder.block.2.4.attention.self.sr.weight\", \"segformer.encoder.block.2.4.attention.self.sr.bias\", \"segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.4.attention.output.dense.weight\", \"segformer.encoder.block.2.4.attention.output.dense.bias\", \"segformer.encoder.block.2.4.layer_norm_2.weight\", \"segformer.encoder.block.2.4.layer_norm_2.bias\", \"segformer.encoder.block.2.4.mlp.dense1.weight\", \"segformer.encoder.block.2.4.mlp.dense1.bias\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.4.mlp.dense2.weight\", \"segformer.encoder.block.2.4.mlp.dense2.bias\", \"segformer.encoder.block.2.5.layer_norm_1.weight\", \"segformer.encoder.block.2.5.layer_norm_1.bias\", \"segformer.encoder.block.2.5.attention.self.query.weight\", \"segformer.encoder.block.2.5.attention.self.query.bias\", \"segformer.encoder.block.2.5.attention.self.key.weight\", \"segformer.encoder.block.2.5.attention.self.key.bias\", \"segformer.encoder.block.2.5.attention.self.value.weight\", \"segformer.encoder.block.2.5.attention.self.value.bias\", \"segformer.encoder.block.2.5.attention.self.sr.weight\", \"segformer.encoder.block.2.5.attention.self.sr.bias\", \"segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.5.attention.output.dense.weight\", \"segformer.encoder.block.2.5.attention.output.dense.bias\", \"segformer.encoder.block.2.5.layer_norm_2.weight\", \"segformer.encoder.block.2.5.layer_norm_2.bias\", \"segformer.encoder.block.2.5.mlp.dense1.weight\", \"segformer.encoder.block.2.5.mlp.dense1.bias\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.5.mlp.dense2.weight\", \"segformer.encoder.block.2.5.mlp.dense2.bias\", \"segformer.encoder.block.2.6.layer_norm_1.weight\", \"segformer.encoder.block.2.6.layer_norm_1.bias\", \"segformer.encoder.block.2.6.attention.self.query.weight\", \"segformer.encoder.block.2.6.attention.self.query.bias\", \"segformer.encoder.block.2.6.attention.self.key.weight\", \"segformer.encoder.block.2.6.attention.self.key.bias\", \"segformer.encoder.block.2.6.attention.self.value.weight\", \"segformer.encoder.block.2.6.attention.self.value.bias\", \"segformer.encoder.block.2.6.attention.self.sr.weight\", \"segformer.encoder.block.2.6.attention.self.sr.bias\", \"segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.6.attention.output.dense.weight\", \"segformer.encoder.block.2.6.attention.output.dense.bias\", \"segformer.encoder.block.2.6.layer_norm_2.weight\", \"segformer.encoder.block.2.6.layer_norm_2.bias\", \"segformer.encoder.block.2.6.mlp.dense1.weight\", \"segformer.encoder.block.2.6.mlp.dense1.bias\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.6.mlp.dense2.weight\", \"segformer.encoder.block.2.6.mlp.dense2.bias\", \"segformer.encoder.block.2.7.layer_norm_1.weight\", \"segformer.encoder.block.2.7.layer_norm_1.bias\", \"segformer.encoder.block.2.7.attention.self.query.weight\", \"segformer.encoder.block.2.7.attention.self.query.bias\", \"segformer.encoder.block.2.7.attention.self.key.weight\", \"segformer.encoder.block.2.7.attention.self.key.bias\", \"segformer.encoder.block.2.7.attention.self.value.weight\", \"segformer.encoder.block.2.7.attention.self.value.bias\", \"segformer.encoder.block.2.7.attention.self.sr.weight\", \"segformer.encoder.block.2.7.attention.self.sr.bias\", \"segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.7.attention.output.dense.weight\", \"segformer.encoder.block.2.7.attention.output.dense.bias\", \"segformer.encoder.block.2.7.layer_norm_2.weight\", \"segformer.encoder.block.2.7.layer_norm_2.bias\", \"segformer.encoder.block.2.7.mlp.dense1.weight\", \"segformer.encoder.block.2.7.mlp.dense1.bias\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.7.mlp.dense2.weight\", \"segformer.encoder.block.2.7.mlp.dense2.bias\", \"segformer.encoder.block.2.8.layer_norm_1.weight\", \"segformer.encoder.block.2.8.layer_norm_1.bias\", \"segformer.encoder.block.2.8.attention.self.query.weight\", \"segformer.encoder.block.2.8.attention.self.query.bias\", \"segformer.encoder.block.2.8.attention.self.key.weight\", \"segformer.encoder.block.2.8.attention.self.key.bias\", \"segformer.encoder.block.2.8.attention.self.value.weight\", \"segformer.encoder.block.2.8.attention.self.value.bias\", \"segformer.encoder.block.2.8.attention.self.sr.weight\", \"segformer.encoder.block.2.8.attention.self.sr.bias\", \"segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.8.attention.output.dense.weight\", \"segformer.encoder.block.2.8.attention.output.dense.bias\", \"segformer.encoder.block.2.8.layer_norm_2.weight\", \"segformer.encoder.block.2.8.layer_norm_2.bias\", \"segformer.encoder.block.2.8.mlp.dense1.weight\", \"segformer.encoder.block.2.8.mlp.dense1.bias\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.8.mlp.dense2.weight\", \"segformer.encoder.block.2.8.mlp.dense2.bias\", \"segformer.encoder.block.2.9.layer_norm_1.weight\", \"segformer.encoder.block.2.9.layer_norm_1.bias\", \"segformer.encoder.block.2.9.attention.self.query.weight\", \"segformer.encoder.block.2.9.attention.self.query.bias\", \"segformer.encoder.block.2.9.attention.self.key.weight\", \"segformer.encoder.block.2.9.attention.self.key.bias\", \"segformer.encoder.block.2.9.attention.self.value.weight\", \"segformer.encoder.block.2.9.attention.self.value.bias\", \"segformer.encoder.block.2.9.attention.self.sr.weight\", \"segformer.encoder.block.2.9.attention.self.sr.bias\", \"segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.9.attention.output.dense.weight\", \"segformer.encoder.block.2.9.attention.output.dense.bias\", \"segformer.encoder.block.2.9.layer_norm_2.weight\", \"segformer.encoder.block.2.9.layer_norm_2.bias\", \"segformer.encoder.block.2.9.mlp.dense1.weight\", \"segformer.encoder.block.2.9.mlp.dense1.bias\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.9.mlp.dense2.weight\", \"segformer.encoder.block.2.9.mlp.dense2.bias\", \"segformer.encoder.block.2.10.layer_norm_1.weight\", \"segformer.encoder.block.2.10.layer_norm_1.bias\", \"segformer.encoder.block.2.10.attention.self.query.weight\", \"segformer.encoder.block.2.10.attention.self.query.bias\", \"segformer.encoder.block.2.10.attention.self.key.weight\", \"segformer.encoder.block.2.10.attention.self.key.bias\", \"segformer.encoder.block.2.10.attention.self.value.weight\", \"segformer.encoder.block.2.10.attention.self.value.bias\", \"segformer.encoder.block.2.10.attention.self.sr.weight\", \"segformer.encoder.block.2.10.attention.self.sr.bias\", \"segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.10.attention.output.dense.weight\", \"segformer.encoder.block.2.10.attention.output.dense.bias\", \"segformer.encoder.block.2.10.layer_norm_2.weight\", \"segformer.encoder.block.2.10.layer_norm_2.bias\", \"segformer.encoder.block.2.10.mlp.dense1.weight\", \"segformer.encoder.block.2.10.mlp.dense1.bias\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.10.mlp.dense2.weight\", \"segformer.encoder.block.2.10.mlp.dense2.bias\", \"segformer.encoder.block.2.11.layer_norm_1.weight\", \"segformer.encoder.block.2.11.layer_norm_1.bias\", \"segformer.encoder.block.2.11.attention.self.query.weight\", \"segformer.encoder.block.2.11.attention.self.query.bias\", \"segformer.encoder.block.2.11.attention.self.key.weight\", \"segformer.encoder.block.2.11.attention.self.key.bias\", \"segformer.encoder.block.2.11.attention.self.value.weight\", \"segformer.encoder.block.2.11.attention.self.value.bias\", \"segformer.encoder.block.2.11.attention.self.sr.weight\", \"segformer.encoder.block.2.11.attention.self.sr.bias\", \"segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.11.attention.output.dense.weight\", \"segformer.encoder.block.2.11.attention.output.dense.bias\", \"segformer.encoder.block.2.11.layer_norm_2.weight\", \"segformer.encoder.block.2.11.layer_norm_2.bias\", \"segformer.encoder.block.2.11.mlp.dense1.weight\", \"segformer.encoder.block.2.11.mlp.dense1.bias\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.11.mlp.dense2.weight\", \"segformer.encoder.block.2.11.mlp.dense2.bias\", \"segformer.encoder.block.2.12.layer_norm_1.weight\", \"segformer.encoder.block.2.12.layer_norm_1.bias\", \"segformer.encoder.block.2.12.attention.self.query.weight\", \"segformer.encoder.block.2.12.attention.self.query.bias\", \"segformer.encoder.block.2.12.attention.self.key.weight\", \"segformer.encoder.block.2.12.attention.self.key.bias\", \"segformer.encoder.block.2.12.attention.self.value.weight\", \"segformer.encoder.block.2.12.attention.self.value.bias\", \"segformer.encoder.block.2.12.attention.self.sr.weight\", \"segformer.encoder.block.2.12.attention.self.sr.bias\", \"segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.12.attention.output.dense.weight\", \"segformer.encoder.block.2.12.attention.output.dense.bias\", \"segformer.encoder.block.2.12.layer_norm_2.weight\", \"segformer.encoder.block.2.12.layer_norm_2.bias\", \"segformer.encoder.block.2.12.mlp.dense1.weight\", \"segformer.encoder.block.2.12.mlp.dense1.bias\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.12.mlp.dense2.weight\", \"segformer.encoder.block.2.12.mlp.dense2.bias\", \"segformer.encoder.block.2.13.layer_norm_1.weight\", \"segformer.encoder.block.2.13.layer_norm_1.bias\", \"segformer.encoder.block.2.13.attention.self.query.weight\", \"segformer.encoder.block.2.13.attention.self.query.bias\", \"segformer.encoder.block.2.13.attention.self.key.weight\", \"segformer.encoder.block.2.13.attention.self.key.bias\", \"segformer.encoder.block.2.13.attention.self.value.weight\", \"segformer.encoder.block.2.13.attention.self.value.bias\", \"segformer.encoder.block.2.13.attention.self.sr.weight\", \"segformer.encoder.block.2.13.attention.self.sr.bias\", \"segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.13.attention.output.dense.weight\", \"segformer.encoder.block.2.13.attention.output.dense.bias\", \"segformer.encoder.block.2.13.layer_norm_2.weight\", \"segformer.encoder.block.2.13.layer_norm_2.bias\", \"segformer.encoder.block.2.13.mlp.dense1.weight\", \"segformer.encoder.block.2.13.mlp.dense1.bias\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.13.mlp.dense2.weight\", \"segformer.encoder.block.2.13.mlp.dense2.bias\", \"segformer.encoder.block.2.14.layer_norm_1.weight\", \"segformer.encoder.block.2.14.layer_norm_1.bias\", \"segformer.encoder.block.2.14.attention.self.query.weight\", \"segformer.encoder.block.2.14.attention.self.query.bias\", \"segformer.encoder.block.2.14.attention.self.key.weight\", \"segformer.encoder.block.2.14.attention.self.key.bias\", \"segformer.encoder.block.2.14.attention.self.value.weight\", \"segformer.encoder.block.2.14.attention.self.value.bias\", \"segformer.encoder.block.2.14.attention.self.sr.weight\", \"segformer.encoder.block.2.14.attention.self.sr.bias\", \"segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.14.attention.output.dense.weight\", \"segformer.encoder.block.2.14.attention.output.dense.bias\", \"segformer.encoder.block.2.14.layer_norm_2.weight\", \"segformer.encoder.block.2.14.layer_norm_2.bias\", \"segformer.encoder.block.2.14.mlp.dense1.weight\", \"segformer.encoder.block.2.14.mlp.dense1.bias\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.14.mlp.dense2.weight\", \"segformer.encoder.block.2.14.mlp.dense2.bias\", \"segformer.encoder.block.2.15.layer_norm_1.weight\", \"segformer.encoder.block.2.15.layer_norm_1.bias\", \"segformer.encoder.block.2.15.attention.self.query.weight\", \"segformer.encoder.block.2.15.attention.self.query.bias\", \"segformer.encoder.block.2.15.attention.self.key.weight\", \"segformer.encoder.block.2.15.attention.self.key.bias\", \"segformer.encoder.block.2.15.attention.self.value.weight\", \"segformer.encoder.block.2.15.attention.self.value.bias\", \"segformer.encoder.block.2.15.attention.self.sr.weight\", \"segformer.encoder.block.2.15.attention.self.sr.bias\", \"segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.15.attention.output.dense.weight\", \"segformer.encoder.block.2.15.attention.output.dense.bias\", \"segformer.encoder.block.2.15.layer_norm_2.weight\", \"segformer.encoder.block.2.15.layer_norm_2.bias\", \"segformer.encoder.block.2.15.mlp.dense1.weight\", \"segformer.encoder.block.2.15.mlp.dense1.bias\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.15.mlp.dense2.weight\", \"segformer.encoder.block.2.15.mlp.dense2.bias\", \"segformer.encoder.block.2.16.layer_norm_1.weight\", \"segformer.encoder.block.2.16.layer_norm_1.bias\", \"segformer.encoder.block.2.16.attention.self.query.weight\", \"segformer.encoder.block.2.16.attention.self.query.bias\", \"segformer.encoder.block.2.16.attention.self.key.weight\", \"segformer.encoder.block.2.16.attention.self.key.bias\", \"segformer.encoder.block.2.16.attention.self.value.weight\", \"segformer.encoder.block.2.16.attention.self.value.bias\", \"segformer.encoder.block.2.16.attention.self.sr.weight\", \"segformer.encoder.block.2.16.attention.self.sr.bias\", \"segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.16.attention.output.dense.weight\", \"segformer.encoder.block.2.16.attention.output.dense.bias\", \"segformer.encoder.block.2.16.layer_norm_2.weight\", \"segformer.encoder.block.2.16.layer_norm_2.bias\", \"segformer.encoder.block.2.16.mlp.dense1.weight\", \"segformer.encoder.block.2.16.mlp.dense1.bias\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.16.mlp.dense2.weight\", \"segformer.encoder.block.2.16.mlp.dense2.bias\", \"segformer.encoder.block.2.17.layer_norm_1.weight\", \"segformer.encoder.block.2.17.layer_norm_1.bias\", \"segformer.encoder.block.2.17.attention.self.query.weight\", \"segformer.encoder.block.2.17.attention.self.query.bias\", \"segformer.encoder.block.2.17.attention.self.key.weight\", \"segformer.encoder.block.2.17.attention.self.key.bias\", \"segformer.encoder.block.2.17.attention.self.value.weight\", \"segformer.encoder.block.2.17.attention.self.value.bias\", \"segformer.encoder.block.2.17.attention.self.sr.weight\", \"segformer.encoder.block.2.17.attention.self.sr.bias\", \"segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.17.attention.output.dense.weight\", \"segformer.encoder.block.2.17.attention.output.dense.bias\", \"segformer.encoder.block.2.17.layer_norm_2.weight\", \"segformer.encoder.block.2.17.layer_norm_2.bias\", \"segformer.encoder.block.2.17.mlp.dense1.weight\", \"segformer.encoder.block.2.17.mlp.dense1.bias\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.17.mlp.dense2.weight\", \"segformer.encoder.block.2.17.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.block.3.2.layer_norm_1.weight\", \"segformer.encoder.block.3.2.layer_norm_1.bias\", \"segformer.encoder.block.3.2.attention.self.query.weight\", \"segformer.encoder.block.3.2.attention.self.query.bias\", \"segformer.encoder.block.3.2.attention.self.key.weight\", \"segformer.encoder.block.3.2.attention.self.key.bias\", \"segformer.encoder.block.3.2.attention.self.value.weight\", \"segformer.encoder.block.3.2.attention.self.value.bias\", \"segformer.encoder.block.3.2.attention.output.dense.weight\", \"segformer.encoder.block.3.2.attention.output.dense.bias\", \"segformer.encoder.block.3.2.layer_norm_2.weight\", \"segformer.encoder.block.3.2.layer_norm_2.bias\", \"segformer.encoder.block.3.2.mlp.dense1.weight\", \"segformer.encoder.block.3.2.mlp.dense1.bias\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.2.mlp.dense2.weight\", \"segformer.encoder.block.3.2.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"module.segformer.encoder.patch_embeddings.0.proj.weight\", \"module.segformer.encoder.patch_embeddings.0.proj.bias\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.1.proj.weight\", \"module.segformer.encoder.patch_embeddings.1.proj.bias\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.2.proj.weight\", \"module.segformer.encoder.patch_embeddings.2.proj.bias\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.3.proj.weight\", \"module.segformer.encoder.patch_embeddings.3.proj.bias\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"module.segformer.encoder.block.0.0.layer_norm_1.weight\", \"module.segformer.encoder.block.0.0.layer_norm_1.bias\", \"module.segformer.encoder.block.0.0.attention.self.query.weight\", \"module.segformer.encoder.block.0.0.attention.self.query.bias\", \"module.segformer.encoder.block.0.0.attention.self.key.weight\", \"module.segformer.encoder.block.0.0.attention.self.key.bias\", \"module.segformer.encoder.block.0.0.attention.self.value.weight\", \"module.segformer.encoder.block.0.0.attention.self.value.bias\", \"module.segformer.encoder.block.0.0.attention.self.sr.weight\", \"module.segformer.encoder.block.0.0.attention.self.sr.bias\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.0.attention.output.dense.weight\", \"module.segformer.encoder.block.0.0.attention.output.dense.bias\", \"module.segformer.encoder.block.0.0.layer_norm_2.weight\", \"module.segformer.encoder.block.0.0.layer_norm_2.bias\", \"module.segformer.encoder.block.0.0.mlp.dense1.weight\", \"module.segformer.encoder.block.0.0.mlp.dense1.bias\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.0.mlp.dense2.weight\", \"module.segformer.encoder.block.0.0.mlp.dense2.bias\", \"module.segformer.encoder.block.0.1.layer_norm_1.weight\", \"module.segformer.encoder.block.0.1.layer_norm_1.bias\", \"module.segformer.encoder.block.0.1.attention.self.query.weight\", \"module.segformer.encoder.block.0.1.attention.self.query.bias\", \"module.segformer.encoder.block.0.1.attention.self.key.weight\", \"module.segformer.encoder.block.0.1.attention.self.key.bias\", \"module.segformer.encoder.block.0.1.attention.self.value.weight\", \"module.segformer.encoder.block.0.1.attention.self.value.bias\", \"module.segformer.encoder.block.0.1.attention.self.sr.weight\", \"module.segformer.encoder.block.0.1.attention.self.sr.bias\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.1.attention.output.dense.weight\", \"module.segformer.encoder.block.0.1.attention.output.dense.bias\", \"module.segformer.encoder.block.0.1.layer_norm_2.weight\", \"module.segformer.encoder.block.0.1.layer_norm_2.bias\", \"module.segformer.encoder.block.0.1.mlp.dense1.weight\", \"module.segformer.encoder.block.0.1.mlp.dense1.bias\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.1.mlp.dense2.weight\", \"module.segformer.encoder.block.0.1.mlp.dense2.bias\", \"module.segformer.encoder.block.0.2.layer_norm_1.weight\", \"module.segformer.encoder.block.0.2.layer_norm_1.bias\", \"module.segformer.encoder.block.0.2.attention.self.query.weight\", \"module.segformer.encoder.block.0.2.attention.self.query.bias\", \"module.segformer.encoder.block.0.2.attention.self.key.weight\", \"module.segformer.encoder.block.0.2.attention.self.key.bias\", \"module.segformer.encoder.block.0.2.attention.self.value.weight\", \"module.segformer.encoder.block.0.2.attention.self.value.bias\", \"module.segformer.encoder.block.0.2.attention.self.sr.weight\", \"module.segformer.encoder.block.0.2.attention.self.sr.bias\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.2.attention.output.dense.weight\", \"module.segformer.encoder.block.0.2.attention.output.dense.bias\", \"module.segformer.encoder.block.0.2.layer_norm_2.weight\", \"module.segformer.encoder.block.0.2.layer_norm_2.bias\", \"module.segformer.encoder.block.0.2.mlp.dense1.weight\", \"module.segformer.encoder.block.0.2.mlp.dense1.bias\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.2.mlp.dense2.weight\", \"module.segformer.encoder.block.0.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.0.layer_norm_1.weight\", \"module.segformer.encoder.block.1.0.layer_norm_1.bias\", \"module.segformer.encoder.block.1.0.attention.self.query.weight\", \"module.segformer.encoder.block.1.0.attention.self.query.bias\", \"module.segformer.encoder.block.1.0.attention.self.key.weight\", \"module.segformer.encoder.block.1.0.attention.self.key.bias\", \"module.segformer.encoder.block.1.0.attention.self.value.weight\", \"module.segformer.encoder.block.1.0.attention.self.value.bias\", \"module.segformer.encoder.block.1.0.attention.self.sr.weight\", \"module.segformer.encoder.block.1.0.attention.self.sr.bias\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.0.attention.output.dense.weight\", \"module.segformer.encoder.block.1.0.attention.output.dense.bias\", \"module.segformer.encoder.block.1.0.layer_norm_2.weight\", \"module.segformer.encoder.block.1.0.layer_norm_2.bias\", \"module.segformer.encoder.block.1.0.mlp.dense1.weight\", \"module.segformer.encoder.block.1.0.mlp.dense1.bias\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.0.mlp.dense2.weight\", \"module.segformer.encoder.block.1.0.mlp.dense2.bias\", \"module.segformer.encoder.block.1.1.layer_norm_1.weight\", \"module.segformer.encoder.block.1.1.layer_norm_1.bias\", \"module.segformer.encoder.block.1.1.attention.self.query.weight\", \"module.segformer.encoder.block.1.1.attention.self.query.bias\", \"module.segformer.encoder.block.1.1.attention.self.key.weight\", \"module.segformer.encoder.block.1.1.attention.self.key.bias\", \"module.segformer.encoder.block.1.1.attention.self.value.weight\", \"module.segformer.encoder.block.1.1.attention.self.value.bias\", \"module.segformer.encoder.block.1.1.attention.self.sr.weight\", \"module.segformer.encoder.block.1.1.attention.self.sr.bias\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.1.attention.output.dense.weight\", \"module.segformer.encoder.block.1.1.attention.output.dense.bias\", \"module.segformer.encoder.block.1.1.layer_norm_2.weight\", \"module.segformer.encoder.block.1.1.layer_norm_2.bias\", \"module.segformer.encoder.block.1.1.mlp.dense1.weight\", \"module.segformer.encoder.block.1.1.mlp.dense1.bias\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.1.mlp.dense2.weight\", \"module.segformer.encoder.block.1.1.mlp.dense2.bias\", \"module.segformer.encoder.block.1.2.layer_norm_1.weight\", \"module.segformer.encoder.block.1.2.layer_norm_1.bias\", \"module.segformer.encoder.block.1.2.attention.self.query.weight\", \"module.segformer.encoder.block.1.2.attention.self.query.bias\", \"module.segformer.encoder.block.1.2.attention.self.key.weight\", \"module.segformer.encoder.block.1.2.attention.self.key.bias\", \"module.segformer.encoder.block.1.2.attention.self.value.weight\", \"module.segformer.encoder.block.1.2.attention.self.value.bias\", \"module.segformer.encoder.block.1.2.attention.self.sr.weight\", \"module.segformer.encoder.block.1.2.attention.self.sr.bias\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.2.attention.output.dense.weight\", \"module.segformer.encoder.block.1.2.attention.output.dense.bias\", \"module.segformer.encoder.block.1.2.layer_norm_2.weight\", \"module.segformer.encoder.block.1.2.layer_norm_2.bias\", \"module.segformer.encoder.block.1.2.mlp.dense1.weight\", \"module.segformer.encoder.block.1.2.mlp.dense1.bias\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.2.mlp.dense2.weight\", \"module.segformer.encoder.block.1.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.3.layer_norm_1.weight\", \"module.segformer.encoder.block.1.3.layer_norm_1.bias\", \"module.segformer.encoder.block.1.3.attention.self.query.weight\", \"module.segformer.encoder.block.1.3.attention.self.query.bias\", \"module.segformer.encoder.block.1.3.attention.self.key.weight\", \"module.segformer.encoder.block.1.3.attention.self.key.bias\", \"module.segformer.encoder.block.1.3.attention.self.value.weight\", \"module.segformer.encoder.block.1.3.attention.self.value.bias\", \"module.segformer.encoder.block.1.3.attention.self.sr.weight\", \"module.segformer.encoder.block.1.3.attention.self.sr.bias\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.3.attention.output.dense.weight\", \"module.segformer.encoder.block.1.3.attention.output.dense.bias\", \"module.segformer.encoder.block.1.3.layer_norm_2.weight\", \"module.segformer.encoder.block.1.3.layer_norm_2.bias\", \"module.segformer.encoder.block.1.3.mlp.dense1.weight\", \"module.segformer.encoder.block.1.3.mlp.dense1.bias\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.3.mlp.dense2.weight\", \"module.segformer.encoder.block.1.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.0.layer_norm_1.weight\", \"module.segformer.encoder.block.2.0.layer_norm_1.bias\", \"module.segformer.encoder.block.2.0.attention.self.query.weight\", \"module.segformer.encoder.block.2.0.attention.self.query.bias\", \"module.segformer.encoder.block.2.0.attention.self.key.weight\", \"module.segformer.encoder.block.2.0.attention.self.key.bias\", \"module.segformer.encoder.block.2.0.attention.self.value.weight\", \"module.segformer.encoder.block.2.0.attention.self.value.bias\", \"module.segformer.encoder.block.2.0.attention.self.sr.weight\", \"module.segformer.encoder.block.2.0.attention.self.sr.bias\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.0.attention.output.dense.weight\", \"module.segformer.encoder.block.2.0.attention.output.dense.bias\", \"module.segformer.encoder.block.2.0.layer_norm_2.weight\", \"module.segformer.encoder.block.2.0.layer_norm_2.bias\", \"module.segformer.encoder.block.2.0.mlp.dense1.weight\", \"module.segformer.encoder.block.2.0.mlp.dense1.bias\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.0.mlp.dense2.weight\", \"module.segformer.encoder.block.2.0.mlp.dense2.bias\", \"module.segformer.encoder.block.2.1.layer_norm_1.weight\", \"module.segformer.encoder.block.2.1.layer_norm_1.bias\", \"module.segformer.encoder.block.2.1.attention.self.query.weight\", \"module.segformer.encoder.block.2.1.attention.self.query.bias\", \"module.segformer.encoder.block.2.1.attention.self.key.weight\", \"module.segformer.encoder.block.2.1.attention.self.key.bias\", \"module.segformer.encoder.block.2.1.attention.self.value.weight\", \"module.segformer.encoder.block.2.1.attention.self.value.bias\", \"module.segformer.encoder.block.2.1.attention.self.sr.weight\", \"module.segformer.encoder.block.2.1.attention.self.sr.bias\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.1.attention.output.dense.weight\", \"module.segformer.encoder.block.2.1.attention.output.dense.bias\", \"module.segformer.encoder.block.2.1.layer_norm_2.weight\", \"module.segformer.encoder.block.2.1.layer_norm_2.bias\", \"module.segformer.encoder.block.2.1.mlp.dense1.weight\", \"module.segformer.encoder.block.2.1.mlp.dense1.bias\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.1.mlp.dense2.weight\", \"module.segformer.encoder.block.2.1.mlp.dense2.bias\", \"module.segformer.encoder.block.2.2.layer_norm_1.weight\", \"module.segformer.encoder.block.2.2.layer_norm_1.bias\", \"module.segformer.encoder.block.2.2.attention.self.query.weight\", \"module.segformer.encoder.block.2.2.attention.self.query.bias\", \"module.segformer.encoder.block.2.2.attention.self.key.weight\", \"module.segformer.encoder.block.2.2.attention.self.key.bias\", \"module.segformer.encoder.block.2.2.attention.self.value.weight\", \"module.segformer.encoder.block.2.2.attention.self.value.bias\", \"module.segformer.encoder.block.2.2.attention.self.sr.weight\", \"module.segformer.encoder.block.2.2.attention.self.sr.bias\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.2.attention.output.dense.weight\", \"module.segformer.encoder.block.2.2.attention.output.dense.bias\", \"module.segformer.encoder.block.2.2.layer_norm_2.weight\", \"module.segformer.encoder.block.2.2.layer_norm_2.bias\", \"module.segformer.encoder.block.2.2.mlp.dense1.weight\", \"module.segformer.encoder.block.2.2.mlp.dense1.bias\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.2.mlp.dense2.weight\", \"module.segformer.encoder.block.2.2.mlp.dense2.bias\", \"module.segformer.encoder.block.2.3.layer_norm_1.weight\", \"module.segformer.encoder.block.2.3.layer_norm_1.bias\", \"module.segformer.encoder.block.2.3.attention.self.query.weight\", \"module.segformer.encoder.block.2.3.attention.self.query.bias\", \"module.segformer.encoder.block.2.3.attention.self.key.weight\", \"module.segformer.encoder.block.2.3.attention.self.key.bias\", \"module.segformer.encoder.block.2.3.attention.self.value.weight\", \"module.segformer.encoder.block.2.3.attention.self.value.bias\", \"module.segformer.encoder.block.2.3.attention.self.sr.weight\", \"module.segformer.encoder.block.2.3.attention.self.sr.bias\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.3.attention.output.dense.weight\", \"module.segformer.encoder.block.2.3.attention.output.dense.bias\", \"module.segformer.encoder.block.2.3.layer_norm_2.weight\", \"module.segformer.encoder.block.2.3.layer_norm_2.bias\", \"module.segformer.encoder.block.2.3.mlp.dense1.weight\", \"module.segformer.encoder.block.2.3.mlp.dense1.bias\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.3.mlp.dense2.weight\", \"module.segformer.encoder.block.2.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.4.layer_norm_1.weight\", \"module.segformer.encoder.block.2.4.layer_norm_1.bias\", \"module.segformer.encoder.block.2.4.attention.self.query.weight\", \"module.segformer.encoder.block.2.4.attention.self.query.bias\", \"module.segformer.encoder.block.2.4.attention.self.key.weight\", \"module.segformer.encoder.block.2.4.attention.self.key.bias\", \"module.segformer.encoder.block.2.4.attention.self.value.weight\", \"module.segformer.encoder.block.2.4.attention.self.value.bias\", \"module.segformer.encoder.block.2.4.attention.self.sr.weight\", \"module.segformer.encoder.block.2.4.attention.self.sr.bias\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.4.attention.output.dense.weight\", \"module.segformer.encoder.block.2.4.attention.output.dense.bias\", \"module.segformer.encoder.block.2.4.layer_norm_2.weight\", \"module.segformer.encoder.block.2.4.layer_norm_2.bias\", \"module.segformer.encoder.block.2.4.mlp.dense1.weight\", \"module.segformer.encoder.block.2.4.mlp.dense1.bias\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.4.mlp.dense2.weight\", \"module.segformer.encoder.block.2.4.mlp.dense2.bias\", \"module.segformer.encoder.block.2.5.layer_norm_1.weight\", \"module.segformer.encoder.block.2.5.layer_norm_1.bias\", \"module.segformer.encoder.block.2.5.attention.self.query.weight\", \"module.segformer.encoder.block.2.5.attention.self.query.bias\", \"module.segformer.encoder.block.2.5.attention.self.key.weight\", \"module.segformer.encoder.block.2.5.attention.self.key.bias\", \"module.segformer.encoder.block.2.5.attention.self.value.weight\", \"module.segformer.encoder.block.2.5.attention.self.value.bias\", \"module.segformer.encoder.block.2.5.attention.self.sr.weight\", \"module.segformer.encoder.block.2.5.attention.self.sr.bias\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.5.attention.output.dense.weight\", \"module.segformer.encoder.block.2.5.attention.output.dense.bias\", \"module.segformer.encoder.block.2.5.layer_norm_2.weight\", \"module.segformer.encoder.block.2.5.layer_norm_2.bias\", \"module.segformer.encoder.block.2.5.mlp.dense1.weight\", \"module.segformer.encoder.block.2.5.mlp.dense1.bias\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.5.mlp.dense2.weight\", \"module.segformer.encoder.block.2.5.mlp.dense2.bias\", \"module.segformer.encoder.block.2.6.layer_norm_1.weight\", \"module.segformer.encoder.block.2.6.layer_norm_1.bias\", \"module.segformer.encoder.block.2.6.attention.self.query.weight\", \"module.segformer.encoder.block.2.6.attention.self.query.bias\", \"module.segformer.encoder.block.2.6.attention.self.key.weight\", \"module.segformer.encoder.block.2.6.attention.self.key.bias\", \"module.segformer.encoder.block.2.6.attention.self.value.weight\", \"module.segformer.encoder.block.2.6.attention.self.value.bias\", \"module.segformer.encoder.block.2.6.attention.self.sr.weight\", \"module.segformer.encoder.block.2.6.attention.self.sr.bias\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.6.attention.output.dense.weight\", \"module.segformer.encoder.block.2.6.attention.output.dense.bias\", \"module.segformer.encoder.block.2.6.layer_norm_2.weight\", \"module.segformer.encoder.block.2.6.layer_norm_2.bias\", \"module.segformer.encoder.block.2.6.mlp.dense1.weight\", \"module.segformer.encoder.block.2.6.mlp.dense1.bias\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.6.mlp.dense2.weight\", \"module.segformer.encoder.block.2.6.mlp.dense2.bias\", \"module.segformer.encoder.block.2.7.layer_norm_1.weight\", \"module.segformer.encoder.block.2.7.layer_norm_1.bias\", \"module.segformer.encoder.block.2.7.attention.self.query.weight\", \"module.segformer.encoder.block.2.7.attention.self.query.bias\", \"module.segformer.encoder.block.2.7.attention.self.key.weight\", \"module.segformer.encoder.block.2.7.attention.self.key.bias\", \"module.segformer.encoder.block.2.7.attention.self.value.weight\", \"module.segformer.encoder.block.2.7.attention.self.value.bias\", \"module.segformer.encoder.block.2.7.attention.self.sr.weight\", \"module.segformer.encoder.block.2.7.attention.self.sr.bias\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.7.attention.output.dense.weight\", \"module.segformer.encoder.block.2.7.attention.output.dense.bias\", \"module.segformer.encoder.block.2.7.layer_norm_2.weight\", \"module.segformer.encoder.block.2.7.layer_norm_2.bias\", \"module.segformer.encoder.block.2.7.mlp.dense1.weight\", \"module.segformer.encoder.block.2.7.mlp.dense1.bias\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.7.mlp.dense2.weight\", \"module.segformer.encoder.block.2.7.mlp.dense2.bias\", \"module.segformer.encoder.block.2.8.layer_norm_1.weight\", \"module.segformer.encoder.block.2.8.layer_norm_1.bias\", \"module.segformer.encoder.block.2.8.attention.self.query.weight\", \"module.segformer.encoder.block.2.8.attention.self.query.bias\", \"module.segformer.encoder.block.2.8.attention.self.key.weight\", \"module.segformer.encoder.block.2.8.attention.self.key.bias\", \"module.segformer.encoder.block.2.8.attention.self.value.weight\", \"module.segformer.encoder.block.2.8.attention.self.value.bias\", \"module.segformer.encoder.block.2.8.attention.self.sr.weight\", \"module.segformer.encoder.block.2.8.attention.self.sr.bias\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.8.attention.output.dense.weight\", \"module.segformer.encoder.block.2.8.attention.output.dense.bias\", \"module.segformer.encoder.block.2.8.layer_norm_2.weight\", \"module.segformer.encoder.block.2.8.layer_norm_2.bias\", \"module.segformer.encoder.block.2.8.mlp.dense1.weight\", \"module.segformer.encoder.block.2.8.mlp.dense1.bias\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.8.mlp.dense2.weight\", \"module.segformer.encoder.block.2.8.mlp.dense2.bias\", \"module.segformer.encoder.block.2.9.layer_norm_1.weight\", \"module.segformer.encoder.block.2.9.layer_norm_1.bias\", \"module.segformer.encoder.block.2.9.attention.self.query.weight\", \"module.segformer.encoder.block.2.9.attention.self.query.bias\", \"module.segformer.encoder.block.2.9.attention.self.key.weight\", \"module.segformer.encoder.block.2.9.attention.self.key.bias\", \"module.segformer.encoder.block.2.9.attention.self.value.weight\", \"module.segformer.encoder.block.2.9.attention.self.value.bias\", \"module.segformer.encoder.block.2.9.attention.self.sr.weight\", \"module.segformer.encoder.block.2.9.attention.self.sr.bias\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.9.attention.output.dense.weight\", \"module.segformer.encoder.block.2.9.attention.output.dense.bias\", \"module.segformer.encoder.block.2.9.layer_norm_2.weight\", \"module.segformer.encoder.block.2.9.layer_norm_2.bias\", \"module.segformer.encoder.block.2.9.mlp.dense1.weight\", \"module.segformer.encoder.block.2.9.mlp.dense1.bias\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.9.mlp.dense2.weight\", \"module.segformer.encoder.block.2.9.mlp.dense2.bias\", \"module.segformer.encoder.block.2.10.layer_norm_1.weight\", \"module.segformer.encoder.block.2.10.layer_norm_1.bias\", \"module.segformer.encoder.block.2.10.attention.self.query.weight\", \"module.segformer.encoder.block.2.10.attention.self.query.bias\", \"module.segformer.encoder.block.2.10.attention.self.key.weight\", \"module.segformer.encoder.block.2.10.attention.self.key.bias\", \"module.segformer.encoder.block.2.10.attention.self.value.weight\", \"module.segformer.encoder.block.2.10.attention.self.value.bias\", \"module.segformer.encoder.block.2.10.attention.self.sr.weight\", \"module.segformer.encoder.block.2.10.attention.self.sr.bias\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.10.attention.output.dense.weight\", \"module.segformer.encoder.block.2.10.attention.output.dense.bias\", \"module.segformer.encoder.block.2.10.layer_norm_2.weight\", \"module.segformer.encoder.block.2.10.layer_norm_2.bias\", \"module.segformer.encoder.block.2.10.mlp.dense1.weight\", \"module.segformer.encoder.block.2.10.mlp.dense1.bias\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.10.mlp.dense2.weight\", \"module.segformer.encoder.block.2.10.mlp.dense2.bias\", \"module.segformer.encoder.block.2.11.layer_norm_1.weight\", \"module.segformer.encoder.block.2.11.layer_norm_1.bias\", \"module.segformer.encoder.block.2.11.attention.self.query.weight\", \"module.segformer.encoder.block.2.11.attention.self.query.bias\", \"module.segformer.encoder.block.2.11.attention.self.key.weight\", \"module.segformer.encoder.block.2.11.attention.self.key.bias\", \"module.segformer.encoder.block.2.11.attention.self.value.weight\", \"module.segformer.encoder.block.2.11.attention.self.value.bias\", \"module.segformer.encoder.block.2.11.attention.self.sr.weight\", \"module.segformer.encoder.block.2.11.attention.self.sr.bias\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.11.attention.output.dense.weight\", \"module.segformer.encoder.block.2.11.attention.output.dense.bias\", \"module.segformer.encoder.block.2.11.layer_norm_2.weight\", \"module.segformer.encoder.block.2.11.layer_norm_2.bias\", \"module.segformer.encoder.block.2.11.mlp.dense1.weight\", \"module.segformer.encoder.block.2.11.mlp.dense1.bias\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.11.mlp.dense2.weight\", \"module.segformer.encoder.block.2.11.mlp.dense2.bias\", \"module.segformer.encoder.block.2.12.layer_norm_1.weight\", \"module.segformer.encoder.block.2.12.layer_norm_1.bias\", \"module.segformer.encoder.block.2.12.attention.self.query.weight\", \"module.segformer.encoder.block.2.12.attention.self.query.bias\", \"module.segformer.encoder.block.2.12.attention.self.key.weight\", \"module.segformer.encoder.block.2.12.attention.self.key.bias\", \"module.segformer.encoder.block.2.12.attention.self.value.weight\", \"module.segformer.encoder.block.2.12.attention.self.value.bias\", \"module.segformer.encoder.block.2.12.attention.self.sr.weight\", \"module.segformer.encoder.block.2.12.attention.self.sr.bias\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.12.attention.output.dense.weight\", \"module.segformer.encoder.block.2.12.attention.output.dense.bias\", \"module.segformer.encoder.block.2.12.layer_norm_2.weight\", \"module.segformer.encoder.block.2.12.layer_norm_2.bias\", \"module.segformer.encoder.block.2.12.mlp.dense1.weight\", \"module.segformer.encoder.block.2.12.mlp.dense1.bias\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.12.mlp.dense2.weight\", \"module.segformer.encoder.block.2.12.mlp.dense2.bias\", \"module.segformer.encoder.block.2.13.layer_norm_1.weight\", \"module.segformer.encoder.block.2.13.layer_norm_1.bias\", \"module.segformer.encoder.block.2.13.attention.self.query.weight\", \"module.segformer.encoder.block.2.13.attention.self.query.bias\", \"module.segformer.encoder.block.2.13.attention.self.key.weight\", \"module.segformer.encoder.block.2.13.attention.self.key.bias\", \"module.segformer.encoder.block.2.13.attention.self.value.weight\", \"module.segformer.encoder.block.2.13.attention.self.value.bias\", \"module.segformer.encoder.block.2.13.attention.self.sr.weight\", \"module.segformer.encoder.block.2.13.attention.self.sr.bias\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.13.attention.output.dense.weight\", \"module.segformer.encoder.block.2.13.attention.output.dense.bias\", \"module.segformer.encoder.block.2.13.layer_norm_2.weight\", \"module.segformer.encoder.block.2.13.layer_norm_2.bias\", \"module.segformer.encoder.block.2.13.mlp.dense1.weight\", \"module.segformer.encoder.block.2.13.mlp.dense1.bias\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.13.mlp.dense2.weight\", \"module.segformer.encoder.block.2.13.mlp.dense2.bias\", \"module.segformer.encoder.block.2.14.layer_norm_1.weight\", \"module.segformer.encoder.block.2.14.layer_norm_1.bias\", \"module.segformer.encoder.block.2.14.attention.self.query.weight\", \"module.segformer.encoder.block.2.14.attention.self.query.bias\", \"module.segformer.encoder.block.2.14.attention.self.key.weight\", \"module.segformer.encoder.block.2.14.attention.self.key.bias\", \"module.segformer.encoder.block.2.14.attention.self.value.weight\", \"module.segformer.encoder.block.2.14.attention.self.value.bias\", \"module.segformer.encoder.block.2.14.attention.self.sr.weight\", \"module.segformer.encoder.block.2.14.attention.self.sr.bias\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.14.attention.output.dense.weight\", \"module.segformer.encoder.block.2.14.attention.output.dense.bias\", \"module.segformer.encoder.block.2.14.layer_norm_2.weight\", \"module.segformer.encoder.block.2.14.layer_norm_2.bias\", \"module.segformer.encoder.block.2.14.mlp.dense1.weight\", \"module.segformer.encoder.block.2.14.mlp.dense1.bias\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.14.mlp.dense2.weight\", \"module.segformer.encoder.block.2.14.mlp.dense2.bias\", \"module.segformer.encoder.block.2.15.layer_norm_1.weight\", \"module.segformer.encoder.block.2.15.layer_norm_1.bias\", \"module.segformer.encoder.block.2.15.attention.self.query.weight\", \"module.segformer.encoder.block.2.15.attention.self.query.bias\", \"module.segformer.encoder.block.2.15.attention.self.key.weight\", \"module.segformer.encoder.block.2.15.attention.self.key.bias\", \"module.segformer.encoder.block.2.15.attention.self.value.weight\", \"module.segformer.encoder.block.2.15.attention.self.value.bias\", \"module.segformer.encoder.block.2.15.attention.self.sr.weight\", \"module.segformer.encoder.block.2.15.attention.self.sr.bias\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.15.attention.output.dense.weight\", \"module.segformer.encoder.block.2.15.attention.output.dense.bias\", \"module.segformer.encoder.block.2.15.layer_norm_2.weight\", \"module.segformer.encoder.block.2.15.layer_norm_2.bias\", \"module.segformer.encoder.block.2.15.mlp.dense1.weight\", \"module.segformer.encoder.block.2.15.mlp.dense1.bias\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.15.mlp.dense2.weight\", \"module.segformer.encoder.block.2.15.mlp.dense2.bias\", \"module.segformer.encoder.block.2.16.layer_norm_1.weight\", \"module.segformer.encoder.block.2.16.layer_norm_1.bias\", \"module.segformer.encoder.block.2.16.attention.self.query.weight\", \"module.segformer.encoder.block.2.16.attention.self.query.bias\", \"module.segformer.encoder.block.2.16.attention.self.key.weight\", \"module.segformer.encoder.block.2.16.attention.self.key.bias\", \"module.segformer.encoder.block.2.16.attention.self.value.weight\", \"module.segformer.encoder.block.2.16.attention.self.value.bias\", \"module.segformer.encoder.block.2.16.attention.self.sr.weight\", \"module.segformer.encoder.block.2.16.attention.self.sr.bias\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.16.attention.output.dense.weight\", \"module.segformer.encoder.block.2.16.attention.output.dense.bias\", \"module.segformer.encoder.block.2.16.layer_norm_2.weight\", \"module.segformer.encoder.block.2.16.layer_norm_2.bias\", \"module.segformer.encoder.block.2.16.mlp.dense1.weight\", \"module.segformer.encoder.block.2.16.mlp.dense1.bias\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.16.mlp.dense2.weight\", \"module.segformer.encoder.block.2.16.mlp.dense2.bias\", \"module.segformer.encoder.block.2.17.layer_norm_1.weight\", \"module.segformer.encoder.block.2.17.layer_norm_1.bias\", \"module.segformer.encoder.block.2.17.attention.self.query.weight\", \"module.segformer.encoder.block.2.17.attention.self.query.bias\", \"module.segformer.encoder.block.2.17.attention.self.key.weight\", \"module.segformer.encoder.block.2.17.attention.self.key.bias\", \"module.segformer.encoder.block.2.17.attention.self.value.weight\", \"module.segformer.encoder.block.2.17.attention.self.value.bias\", \"module.segformer.encoder.block.2.17.attention.self.sr.weight\", \"module.segformer.encoder.block.2.17.attention.self.sr.bias\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.17.attention.output.dense.weight\", \"module.segformer.encoder.block.2.17.attention.output.dense.bias\", \"module.segformer.encoder.block.2.17.layer_norm_2.weight\", \"module.segformer.encoder.block.2.17.layer_norm_2.bias\", \"module.segformer.encoder.block.2.17.mlp.dense1.weight\", \"module.segformer.encoder.block.2.17.mlp.dense1.bias\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.17.mlp.dense2.weight\", \"module.segformer.encoder.block.2.17.mlp.dense2.bias\", \"module.segformer.encoder.block.3.0.layer_norm_1.weight\", \"module.segformer.encoder.block.3.0.layer_norm_1.bias\", \"module.segformer.encoder.block.3.0.attention.self.query.weight\", \"module.segformer.encoder.block.3.0.attention.self.query.bias\", \"module.segformer.encoder.block.3.0.attention.self.key.weight\", \"module.segformer.encoder.block.3.0.attention.self.key.bias\", \"module.segformer.encoder.block.3.0.attention.self.value.weight\", \"module.segformer.encoder.block.3.0.attention.self.value.bias\", \"module.segformer.encoder.block.3.0.attention.output.dense.weight\", \"module.segformer.encoder.block.3.0.attention.output.dense.bias\", \"module.segformer.encoder.block.3.0.layer_norm_2.weight\", \"module.segformer.encoder.block.3.0.layer_norm_2.bias\", \"module.segformer.encoder.block.3.0.mlp.dense1.weight\", \"module.segformer.encoder.block.3.0.mlp.dense1.bias\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.0.mlp.dense2.weight\", \"module.segformer.encoder.block.3.0.mlp.dense2.bias\", \"module.segformer.encoder.block.3.1.layer_norm_1.weight\", \"module.segformer.encoder.block.3.1.layer_norm_1.bias\", \"module.segformer.encoder.block.3.1.attention.self.query.weight\", \"module.segformer.encoder.block.3.1.attention.self.query.bias\", \"module.segformer.encoder.block.3.1.attention.self.key.weight\", \"module.segformer.encoder.block.3.1.attention.self.key.bias\", \"module.segformer.encoder.block.3.1.attention.self.value.weight\", \"module.segformer.encoder.block.3.1.attention.self.value.bias\", \"module.segformer.encoder.block.3.1.attention.output.dense.weight\", \"module.segformer.encoder.block.3.1.attention.output.dense.bias\", \"module.segformer.encoder.block.3.1.layer_norm_2.weight\", \"module.segformer.encoder.block.3.1.layer_norm_2.bias\", \"module.segformer.encoder.block.3.1.mlp.dense1.weight\", \"module.segformer.encoder.block.3.1.mlp.dense1.bias\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.1.mlp.dense2.weight\", \"module.segformer.encoder.block.3.1.mlp.dense2.bias\", \"module.segformer.encoder.block.3.2.layer_norm_1.weight\", \"module.segformer.encoder.block.3.2.layer_norm_1.bias\", \"module.segformer.encoder.block.3.2.attention.self.query.weight\", \"module.segformer.encoder.block.3.2.attention.self.query.bias\", \"module.segformer.encoder.block.3.2.attention.self.key.weight\", \"module.segformer.encoder.block.3.2.attention.self.key.bias\", \"module.segformer.encoder.block.3.2.attention.self.value.weight\", \"module.segformer.encoder.block.3.2.attention.self.value.bias\", \"module.segformer.encoder.block.3.2.attention.output.dense.weight\", \"module.segformer.encoder.block.3.2.attention.output.dense.bias\", \"module.segformer.encoder.block.3.2.layer_norm_2.weight\", \"module.segformer.encoder.block.3.2.layer_norm_2.bias\", \"module.segformer.encoder.block.3.2.mlp.dense1.weight\", \"module.segformer.encoder.block.3.2.mlp.dense1.bias\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.2.mlp.dense2.weight\", \"module.segformer.encoder.block.3.2.mlp.dense2.bias\", \"module.segformer.encoder.layer_norm.0.weight\", \"module.segformer.encoder.layer_norm.0.bias\", \"module.segformer.encoder.layer_norm.1.weight\", \"module.segformer.encoder.layer_norm.1.bias\", \"module.segformer.encoder.layer_norm.2.weight\", \"module.segformer.encoder.layer_norm.2.bias\", \"module.segformer.encoder.layer_norm.3.weight\", \"module.segformer.encoder.layer_norm.3.bias\", \"module.decode_head.linear_c.0.proj.weight\", \"module.decode_head.linear_c.0.proj.bias\", \"module.decode_head.linear_c.1.proj.weight\", \"module.decode_head.linear_c.1.proj.bias\", \"module.decode_head.linear_c.2.proj.weight\", \"module.decode_head.linear_c.2.proj.bias\", \"module.decode_head.linear_c.3.proj.weight\", \"module.decode_head.linear_c.3.proj.bias\", \"module.decode_head.linear_fuse.weight\", \"module.decode_head.batch_norm.weight\", \"module.decode_head.batch_norm.bias\", \"module.decode_head.batch_norm.running_mean\", \"module.decode_head.batch_norm.running_var\", \"module.decode_head.batch_norm.num_batches_tracked\", \"module.decode_head.classifier.weight\", \"module.decode_head.classifier.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "import numpy as np\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "def evaluate_model_on_test_set(model, X_test_normalized, y_test, processor):\n",
    "    resize_transform = Resize(\n",
    "        size=(48, 48),\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "        antialias=True\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = torch.zeros((len(X_test_normalized), 48, 48)).cuda()\n",
    "    y_test_tensor = torch.from_numpy(y_test.reshape(-1, 48, 48)).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, image in enumerate(X_test_normalized):\n",
    "            image_rgb = np.repeat(image.reshape(48, 48, 1), 3, axis=-1)\n",
    "            inputs = processor(\n",
    "                images=image_rgb,\n",
    "                return_tensors=\"pt\",\n",
    "                do_rescale=False\n",
    "            )\n",
    "            pixel_values = inputs['pixel_values'].cuda()\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            pred = probs[0, 1]\n",
    "            \n",
    "            if pred.shape != (48, 48):\n",
    "                pred = resize_transform(pred.unsqueeze(0)).squeeze()\n",
    "            predictions[i] = pred\n",
    "    \n",
    "    predictions = (predictions > 0.5).float()\n",
    "    y_true_flat = y_test_tensor.flatten()\n",
    "    y_pred_flat = predictions.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp = torch.sum((y_true_flat == 1) & (y_pred_flat == 1)).float()\n",
    "    tn = torch.sum((y_true_flat == 0) & (y_pred_flat == 0)).float()\n",
    "    fp = torch.sum((y_true_flat == 0) & (y_pred_flat == 1)).float()\n",
    "    fn = torch.sum((y_true_flat == 1) & (y_pred_flat == 0)).float()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn + 1e-7)\n",
    "    specificity = tn / (tn + fp + 1e-7)\n",
    "    balanced_acc = (sensitivity + specificity) / 2\n",
    "    \n",
    "    return {\n",
    "        'balanced_accuracy': balanced_acc.item(),\n",
    "        'sensitivity': sensitivity.item(),\n",
    "        'specificity': specificity.item(),\n",
    "        'true_positives': tp.item(),\n",
    "        'true_negatives': tn.item(),\n",
    "        'false_positives': fp.item(),\n",
    "        'false_negatives': fn.item()\n",
    "    }\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b3\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").cuda()\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_model_on_test_set(model, X_test_normalized, y_test, processor)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Sensitivity: {metrics['sensitivity']:.4f}\")\n",
    "print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "print(\"\\nDetailed Counts:\")\n",
    "print(f\"True Positives: {metrics['true_positives']}\")\n",
    "print(f\"True Negatives: {metrics['true_negatives']}\")\n",
    "print(f\"False Positives: {metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {metrics['false_negatives']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-26T15:08:41.053885Z",
     "iopub.status.busy": "2024-10-26T15:08:41.053494Z",
     "iopub.status.idle": "2024-10-26T15:25:45.675245Z",
     "shell.execute_reply": "2024-10-26T15:25:45.673998Z",
     "shell.execute_reply.started": "2024-10-26T15:08:41.053846Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n",
      "Starting data preparation...\n",
      "Initializing model and processor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da380f33c8e4f1cba2a0159ba5aa83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f490a6d20242cfaa8b9dd823c027fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/70.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073477440c274acaa9dab977dfd5e376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 84,594,882\n",
      "Using 2 GPUs!\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2693918669.py:176: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/tmp/ipykernel_31/2693918669.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 0.7051\n",
      "Batch: 5, Loss: 0.7081\n",
      "Batch: 10, Loss: 0.7014\n",
      "Batch: 15, Loss: 0.6943\n",
      "Batch: 20, Loss: 0.6492\n",
      "Batch: 25, Loss: 0.6818\n",
      "Batch: 30, Loss: 0.6723\n",
      "Batch: 35, Loss: 0.6643\n",
      "Batch: 40, Loss: 0.6563\n",
      "Batch: 45, Loss: 0.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6776, Val Loss = 0.6120\n",
      "Saved new best model with validation loss: 0.6120\n",
      "\n",
      "Epoch 2/30\n",
      "Batch: 0, Loss: 0.6412\n",
      "Batch: 5, Loss: 0.6391\n",
      "Batch: 10, Loss: 0.6498\n",
      "Batch: 15, Loss: 0.6234\n",
      "Batch: 20, Loss: 0.5952\n",
      "Batch: 25, Loss: 0.6085\n",
      "Batch: 30, Loss: 0.6365\n",
      "Batch: 35, Loss: 0.5877\n",
      "Batch: 40, Loss: 0.6053\n",
      "Batch: 45, Loss: 0.6046\n",
      "Epoch 2: Train Loss = 0.6251, Val Loss = 0.5499\n",
      "Saved new best model with validation loss: 0.5499\n",
      "\n",
      "Epoch 3/30\n",
      "Batch: 0, Loss: 0.5552\n",
      "Batch: 5, Loss: 0.6012\n",
      "Batch: 10, Loss: 0.6128\n",
      "Batch: 15, Loss: 0.5749\n",
      "Batch: 20, Loss: 0.5750\n",
      "Batch: 25, Loss: 0.5832\n",
      "Batch: 30, Loss: 0.5652\n",
      "Batch: 35, Loss: 0.6221\n",
      "Batch: 40, Loss: 0.6072\n",
      "Batch: 45, Loss: 0.5603\n",
      "Epoch 3: Train Loss = 0.6006, Val Loss = 0.5155\n",
      "Saved new best model with validation loss: 0.5155\n",
      "\n",
      "Epoch 4/30\n",
      "Batch: 0, Loss: 0.5317\n",
      "Batch: 5, Loss: 0.5897\n",
      "Batch: 10, Loss: 0.5831\n",
      "Batch: 15, Loss: 0.5833\n",
      "Batch: 20, Loss: 0.5630\n",
      "Batch: 25, Loss: 0.6070\n",
      "Batch: 30, Loss: 0.5715\n",
      "Batch: 35, Loss: 0.5676\n",
      "Batch: 40, Loss: 0.5205\n",
      "Batch: 45, Loss: 0.5915\n",
      "Epoch 4: Train Loss = 0.5759, Val Loss = 0.4867\n",
      "Saved new best model with validation loss: 0.4867\n",
      "\n",
      "Epoch 5/30\n",
      "Batch: 0, Loss: 0.5477\n",
      "Batch: 5, Loss: 0.6059\n",
      "Batch: 10, Loss: 0.5094\n",
      "Batch: 15, Loss: 0.5693\n",
      "Batch: 20, Loss: 0.5558\n",
      "Batch: 25, Loss: 0.5676\n",
      "Batch: 30, Loss: 0.5811\n",
      "Batch: 35, Loss: 0.5791\n",
      "Batch: 40, Loss: 0.5068\n",
      "Batch: 45, Loss: 0.5380\n",
      "Epoch 5: Train Loss = 0.5628, Val Loss = 0.4763\n",
      "Saved new best model with validation loss: 0.4763\n",
      "\n",
      "Epoch 6/30\n",
      "Batch: 0, Loss: 0.5533\n",
      "Batch: 5, Loss: 0.5564\n",
      "Batch: 10, Loss: 0.5875\n",
      "Batch: 15, Loss: 0.5443\n",
      "Batch: 20, Loss: 0.5629\n",
      "Batch: 25, Loss: 0.5199\n",
      "Batch: 30, Loss: 0.5089\n",
      "Batch: 35, Loss: 0.5462\n",
      "Batch: 40, Loss: 0.6030\n",
      "Batch: 45, Loss: 0.5271\n",
      "Epoch 6: Train Loss = 0.5644, Val Loss = 0.4695\n",
      "Saved new best model with validation loss: 0.4695\n",
      "\n",
      "Epoch 7/30\n",
      "Batch: 0, Loss: 0.5822\n",
      "Batch: 5, Loss: 0.5108\n",
      "Batch: 10, Loss: 0.6908\n",
      "Batch: 15, Loss: 0.5840\n",
      "Batch: 20, Loss: 0.5322\n",
      "Batch: 25, Loss: 0.5592\n",
      "Batch: 30, Loss: 0.5969\n",
      "Batch: 35, Loss: 0.5063\n",
      "Batch: 40, Loss: 0.5379\n",
      "Batch: 45, Loss: 0.5438\n",
      "Epoch 7: Train Loss = 0.5621, Val Loss = 0.4680\n",
      "Saved new best model with validation loss: 0.4680\n",
      "\n",
      "Epoch 8/30\n",
      "Batch: 0, Loss: 0.5365\n",
      "Batch: 5, Loss: 0.5447\n",
      "Batch: 10, Loss: 0.5610\n",
      "Batch: 15, Loss: 0.5121\n",
      "Batch: 20, Loss: 0.5003\n",
      "Batch: 25, Loss: 0.4517\n",
      "Batch: 30, Loss: 0.5539\n",
      "Batch: 35, Loss: 0.5693\n",
      "Batch: 40, Loss: 0.5581\n",
      "Batch: 45, Loss: 0.5823\n",
      "Epoch 8: Train Loss = 0.5593, Val Loss = 0.4718\n",
      "\n",
      "Epoch 9/30\n",
      "Batch: 0, Loss: 0.5759\n",
      "Batch: 5, Loss: 0.5759\n",
      "Batch: 10, Loss: 0.4939\n",
      "Batch: 15, Loss: 0.5257\n",
      "Batch: 20, Loss: 0.5513\n",
      "Batch: 25, Loss: 0.5035\n",
      "Batch: 30, Loss: 0.4834\n",
      "Batch: 35, Loss: 0.5994\n",
      "Batch: 40, Loss: 0.5750\n",
      "Batch: 45, Loss: 0.6073\n",
      "Epoch 9: Train Loss = 0.5537, Val Loss = 0.4586\n",
      "Saved new best model with validation loss: 0.4586\n",
      "\n",
      "Epoch 10/30\n",
      "Batch: 0, Loss: 0.4989\n",
      "Batch: 5, Loss: 0.5323\n",
      "Batch: 10, Loss: 0.6864\n",
      "Batch: 15, Loss: 0.5371\n",
      "Batch: 20, Loss: 0.5333\n",
      "Batch: 25, Loss: 0.5195\n",
      "Batch: 30, Loss: 0.5883\n",
      "Batch: 35, Loss: 0.5454\n",
      "Batch: 40, Loss: 0.5445\n",
      "Batch: 45, Loss: 0.5045\n",
      "Epoch 10: Train Loss = 0.5517, Val Loss = 0.4283\n",
      "Saved new best model with validation loss: 0.4283\n",
      "\n",
      "Epoch 11/30\n",
      "Batch: 0, Loss: 0.5208\n",
      "Batch: 5, Loss: 0.5570\n",
      "Batch: 10, Loss: 0.5204\n",
      "Batch: 15, Loss: 0.5735\n",
      "Batch: 20, Loss: 0.4951\n",
      "Batch: 25, Loss: 0.5199\n",
      "Batch: 30, Loss: 0.5216\n",
      "Batch: 35, Loss: 0.4964\n",
      "Batch: 40, Loss: 0.5392\n",
      "Batch: 45, Loss: 0.6188\n",
      "Epoch 11: Train Loss = 0.5602, Val Loss = 0.4689\n",
      "\n",
      "Epoch 12/30\n",
      "Batch: 0, Loss: 0.5035\n",
      "Batch: 5, Loss: 0.6068\n",
      "Batch: 10, Loss: 0.4546\n",
      "Batch: 15, Loss: 0.6677\n",
      "Batch: 20, Loss: 0.6099\n",
      "Batch: 25, Loss: 0.5603\n",
      "Batch: 30, Loss: 0.5187\n",
      "Batch: 35, Loss: 0.5092\n",
      "Batch: 40, Loss: 0.5167\n",
      "Batch: 45, Loss: 0.5561\n",
      "Epoch 12: Train Loss = 0.5466, Val Loss = 0.4714\n",
      "\n",
      "Epoch 13/30\n",
      "Batch: 0, Loss: 0.5333\n",
      "Batch: 5, Loss: 0.5180\n",
      "Batch: 10, Loss: 0.4909\n",
      "Batch: 15, Loss: 0.5699\n",
      "Batch: 20, Loss: 0.5884\n",
      "Batch: 25, Loss: 0.4810\n",
      "Batch: 30, Loss: 0.6377\n",
      "Batch: 35, Loss: 0.5568\n",
      "Batch: 40, Loss: 0.5067\n",
      "Batch: 45, Loss: 0.6310\n",
      "Epoch 13: Train Loss = 0.5438, Val Loss = 0.4352\n",
      "\n",
      "Epoch 14/30\n",
      "Batch: 0, Loss: 0.5876\n",
      "Batch: 5, Loss: 0.5213\n",
      "Batch: 10, Loss: 0.5052\n",
      "Batch: 15, Loss: 0.5421\n",
      "Batch: 20, Loss: 0.5938\n",
      "Batch: 25, Loss: 0.5720\n",
      "Batch: 30, Loss: 0.5544\n",
      "Batch: 35, Loss: 0.4744\n",
      "Batch: 40, Loss: 0.5676\n",
      "Batch: 45, Loss: 0.6214\n",
      "Epoch 14: Train Loss = 0.5388, Val Loss = 0.4237\n",
      "Saved new best model with validation loss: 0.4237\n",
      "\n",
      "Epoch 15/30\n",
      "Batch: 0, Loss: 0.6694\n",
      "Batch: 5, Loss: 0.5945\n",
      "Batch: 10, Loss: 0.4947\n",
      "Batch: 15, Loss: 0.6740\n",
      "Batch: 20, Loss: 0.5333\n",
      "Batch: 25, Loss: 0.5198\n",
      "Batch: 30, Loss: 0.5153\n",
      "Batch: 35, Loss: 0.5265\n",
      "Batch: 40, Loss: 0.5577\n",
      "Batch: 45, Loss: 0.4633\n",
      "Epoch 15: Train Loss = 0.5394, Val Loss = 0.3926\n",
      "Saved new best model with validation loss: 0.3926\n",
      "\n",
      "Epoch 16/30\n",
      "Batch: 0, Loss: 0.4552\n",
      "Batch: 5, Loss: 0.5709\n",
      "Batch: 10, Loss: 0.5534\n",
      "Batch: 15, Loss: 0.4848\n",
      "Batch: 20, Loss: 0.5506\n",
      "Batch: 25, Loss: 0.5791\n",
      "Batch: 30, Loss: 0.5770\n",
      "Batch: 35, Loss: 0.5446\n",
      "Batch: 40, Loss: 0.6088\n",
      "Batch: 45, Loss: 0.5132\n",
      "Epoch 16: Train Loss = 0.5405, Val Loss = 0.3885\n",
      "Saved new best model with validation loss: 0.3885\n",
      "\n",
      "Epoch 17/30\n",
      "Batch: 0, Loss: 0.5676\n",
      "Batch: 5, Loss: 0.6861\n",
      "Batch: 10, Loss: 0.5646\n",
      "Batch: 15, Loss: 0.5274\n",
      "Batch: 20, Loss: 0.5034\n",
      "Batch: 25, Loss: 0.5094\n",
      "Batch: 30, Loss: 0.5104\n",
      "Batch: 35, Loss: 0.5551\n",
      "Batch: 40, Loss: 0.4545\n",
      "Batch: 45, Loss: 0.4667\n",
      "Epoch 17: Train Loss = 0.5447, Val Loss = 0.4178\n",
      "\n",
      "Epoch 18/30\n",
      "Batch: 0, Loss: 0.5316\n",
      "Batch: 5, Loss: 0.4884\n",
      "Batch: 10, Loss: 0.4932\n",
      "Batch: 15, Loss: 0.5384\n",
      "Batch: 20, Loss: 0.4674\n",
      "Batch: 25, Loss: 0.5072\n",
      "Batch: 30, Loss: 0.4548\n",
      "Batch: 35, Loss: 0.4917\n",
      "Batch: 40, Loss: 0.5095\n",
      "Batch: 45, Loss: 0.5274\n",
      "Epoch 18: Train Loss = 0.5297, Val Loss = 0.3779\n",
      "Saved new best model with validation loss: 0.3779\n",
      "\n",
      "Epoch 19/30\n",
      "Batch: 0, Loss: 0.4529\n",
      "Batch: 5, Loss: 0.4916\n",
      "Batch: 10, Loss: 0.4608\n",
      "Batch: 15, Loss: 0.5450\n",
      "Batch: 20, Loss: 0.4660\n",
      "Batch: 25, Loss: 0.6451\n",
      "Batch: 30, Loss: 0.6295\n",
      "Batch: 35, Loss: 0.4862\n",
      "Batch: 40, Loss: 0.5360\n",
      "Batch: 45, Loss: 0.4321\n",
      "Epoch 19: Train Loss = 0.5180, Val Loss = 0.3964\n",
      "\n",
      "Epoch 20/30\n",
      "Batch: 0, Loss: 0.5318\n",
      "Batch: 5, Loss: 0.5999\n",
      "Batch: 10, Loss: 0.5605\n",
      "Batch: 15, Loss: 0.4621\n",
      "Batch: 20, Loss: 0.4869\n",
      "Batch: 25, Loss: 0.5330\n",
      "Batch: 30, Loss: 0.5243\n",
      "Batch: 35, Loss: 0.5217\n",
      "Batch: 40, Loss: 0.4968\n",
      "Batch: 45, Loss: 0.4710\n",
      "Epoch 20: Train Loss = 0.5220, Val Loss = 0.3650\n",
      "Saved new best model with validation loss: 0.3650\n",
      "\n",
      "Epoch 21/30\n",
      "Batch: 0, Loss: 0.6282\n",
      "Batch: 5, Loss: 0.5138\n",
      "Batch: 10, Loss: 0.4184\n",
      "Batch: 15, Loss: 0.5411\n",
      "Batch: 20, Loss: 0.5548\n",
      "Batch: 25, Loss: 0.6274\n",
      "Batch: 30, Loss: 0.4816\n",
      "Batch: 35, Loss: 0.4784\n",
      "Batch: 40, Loss: 0.5069\n",
      "Batch: 45, Loss: 0.6761\n",
      "Epoch 21: Train Loss = 0.5281, Val Loss = 0.3877\n",
      "\n",
      "Epoch 22/30\n",
      "Batch: 0, Loss: 0.4006\n",
      "Batch: 5, Loss: 0.4726\n",
      "Batch: 10, Loss: 0.5517\n",
      "Batch: 15, Loss: 0.4682\n",
      "Batch: 20, Loss: 0.6413\n",
      "Batch: 25, Loss: 0.4765\n",
      "Batch: 30, Loss: 0.5237\n",
      "Batch: 35, Loss: 0.6177\n",
      "Batch: 40, Loss: 0.4507\n",
      "Batch: 45, Loss: 0.6511\n",
      "Epoch 22: Train Loss = 0.5177, Val Loss = 0.3849\n",
      "\n",
      "Epoch 23/30\n",
      "Batch: 0, Loss: 0.5853\n",
      "Batch: 5, Loss: 0.5036\n",
      "Batch: 10, Loss: 0.4671\n",
      "Batch: 15, Loss: 0.5269\n",
      "Batch: 20, Loss: 0.4133\n",
      "Batch: 25, Loss: 0.4553\n",
      "Batch: 30, Loss: 0.6604\n",
      "Batch: 35, Loss: 0.5440\n",
      "Batch: 40, Loss: 0.4631\n",
      "Batch: 45, Loss: 0.5821\n",
      "Epoch 23: Train Loss = 0.5141, Val Loss = 0.4115\n",
      "\n",
      "Epoch 24/30\n",
      "Batch: 0, Loss: 0.4659\n",
      "Batch: 5, Loss: 0.6018\n",
      "Batch: 10, Loss: 0.5347\n",
      "Batch: 15, Loss: 0.5160\n",
      "Batch: 20, Loss: 0.4755\n",
      "Batch: 25, Loss: 0.5633\n",
      "Batch: 30, Loss: 0.4822\n",
      "Batch: 35, Loss: 0.5230\n",
      "Batch: 40, Loss: 0.4638\n",
      "Batch: 45, Loss: 0.4675\n",
      "Epoch 24: Train Loss = 0.5190, Val Loss = 0.4414\n",
      "\n",
      "Epoch 25/30\n",
      "Batch: 0, Loss: 0.5350\n",
      "Batch: 5, Loss: 0.5276\n",
      "Batch: 10, Loss: 0.6139\n",
      "Batch: 15, Loss: 0.5222\n",
      "Batch: 20, Loss: 0.5542\n",
      "Batch: 25, Loss: 0.4829\n",
      "Batch: 30, Loss: 0.5062\n",
      "Batch: 35, Loss: 0.5326\n",
      "Batch: 40, Loss: 0.4480\n",
      "Batch: 45, Loss: 0.5268\n",
      "Epoch 25: Train Loss = 0.5115, Val Loss = 0.3738\n",
      "Early stopping triggered after 25 epochs\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check GPU availability and set device\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "class CraterDataset(Dataset):\n",
    "    def __init__(self, images, masks, processor, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        image_rgb = np.repeat(image.reshape(48, 48, 1), 3, axis=-1)\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = torch.from_numpy(image_rgb).permute(2, 0, 1)\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "            image_rgb = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image_rgb, \n",
    "            segmentation_maps=mask, \n",
    "            return_tensors=\"pt\",\n",
    "            do_rescale=False\n",
    "        )\n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(),\n",
    "            'labels': inputs['labels'].squeeze()\n",
    "        }\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, scaler, device, gradient_accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                pixel_values=batch['pixel_values'].to(device),\n",
    "                labels=batch['labels'].to(device)\n",
    "            )\n",
    "            loss = outputs.loss.mean() / gradient_accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"Batch: {batch_idx}, Loss: {loss.item()*gradient_accumulation_steps:.4f}\")\n",
    "            \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            outputs = model(\n",
    "                pixel_values=batch['pixel_values'].to(device),\n",
    "                labels=batch['labels'].to(device)\n",
    "            )\n",
    "            loss = outputs.loss.mean()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "print(\"Starting data preparation...\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape and normalize data\n",
    "X_train_reshaped = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val_reshaped = X_val.reshape(-1, 48, 48, 1)\n",
    "y_train_reshaped = y_train.reshape(-1, 48, 48)\n",
    "y_val_reshaped = y_val.reshape(-1, 48, 48)\n",
    "\n",
    "X_train_normalized = X_train_reshaped / 255.0\n",
    "X_val_normalized = X_val_reshaped / 255.0\n",
    "\n",
    "print(\"Initializing model and processor...\")\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b5\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b5\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(f\"Model Parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Move model to GPU and enable data parallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "\n",
    "# Data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CraterDataset(X_train_normalized, y_train_reshaped, processor, transform=transform)\n",
    "val_dataset = CraterDataset(X_val_normalized, y_val_reshaped, processor, transform=None)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.module.segformer.parameters(), 'lr': 5e-6},\n",
    "    {'params': model.module.decode_head.parameters(), 'lr': 5e-5}\n",
    "])\n",
    "\n",
    "num_epochs = 30\n",
    "steps_per_epoch = len(train_dataloader)\n",
    "total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=[5e-5, 5e-4],\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "gradient_accumulation_steps = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, \n",
    "                           scaler, device, gradient_accumulation_steps)\n",
    "    val_loss = validate(model, val_dataloader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:00:04.604490Z",
     "iopub.status.busy": "2024-10-26T19:00:04.603842Z",
     "iopub.status.idle": "2024-10-26T19:00:04.664787Z",
     "shell.execute_reply": "2024-10-26T19:00:04.663652Z",
     "shell.execute_reply.started": "2024-10-26T19:00:04.604448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on test set...\n",
      "Predicting image 0/110\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test_normalized)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m(image, model, processor)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Resize prediction using torchvision if necessary\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m48\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Create resize transform\n",
    "resize_transform = Resize(\n",
    "    size=(48, 48),\n",
    "    interpolation=InterpolationMode.BILINEAR,\n",
    "    antialias=True\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "X_test_reshaped = X_test.reshape(-1, 48, 48, 1)\n",
    "X_test_normalized = X_test_reshaped / 255.0\n",
    "\n",
    "print(\"Making predictions on test set...\")\n",
    "predictions = np.zeros((len(X_test_normalized), 48, 48))\n",
    "\n",
    "for i, image in enumerate(X_test_normalized):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Predicting image {i}/{len(X_test_normalized)}\")\n",
    "    pred = predict(image, model, processor)\n",
    "    \n",
    "    # Resize prediction using torchvision if necessary\n",
    "    if pred.shape != (48, 48):\n",
    "        pred_tensor = torch.from_numpy(pred).unsqueeze(0)  # Add batch and channel dims\n",
    "        pred_resized = resize_transform(pred_tensor).squeeze().numpy()\n",
    "        predictions[i] = pred_resized\n",
    "    else:\n",
    "        predictions[i] = pred\n",
    "\n",
    "print(f\"Ground truth shape: {y_test.reshape(-1, 48, 48).shape}\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "y_true_flat = y_test.reshape(-1, 48, 48).flatten()\n",
    "y_pred_flat = predictions.flatten()\n",
    "y_pred_flat = (y_pred_flat > 0.5).astype(int)\n",
    "\n",
    "print(f\"Flattened shapes - True: {y_true_flat.shape}, Pred: {y_pred_flat.shape}\")\n",
    "balanced_acc = balanced_accuracy_score(y_true_flat, y_pred_flat)\n",
    "print(f\"Final Balanced Accuracy: {balanced_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:03:43.290772Z",
     "iopub.status.busy": "2024-10-26T19:03:43.289905Z",
     "iopub.status.idle": "2024-10-26T19:03:45.420875Z",
     "shell.execute_reply": "2024-10-26T19:03:45.418348Z",
     "shell.execute_reply.started": "2024-10-26T19:03:43.290730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29/3824458066.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pt')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.0.2.layer_norm_1.weight\", \"segformer.encoder.block.0.2.layer_norm_1.bias\", \"segformer.encoder.block.0.2.attention.self.query.weight\", \"segformer.encoder.block.0.2.attention.self.query.bias\", \"segformer.encoder.block.0.2.attention.self.key.weight\", \"segformer.encoder.block.0.2.attention.self.key.bias\", \"segformer.encoder.block.0.2.attention.self.value.weight\", \"segformer.encoder.block.0.2.attention.self.value.bias\", \"segformer.encoder.block.0.2.attention.self.sr.weight\", \"segformer.encoder.block.0.2.attention.self.sr.bias\", \"segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.2.attention.output.dense.weight\", \"segformer.encoder.block.0.2.attention.output.dense.bias\", \"segformer.encoder.block.0.2.layer_norm_2.weight\", \"segformer.encoder.block.0.2.layer_norm_2.bias\", \"segformer.encoder.block.0.2.mlp.dense1.weight\", \"segformer.encoder.block.0.2.mlp.dense1.bias\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.2.mlp.dense2.weight\", \"segformer.encoder.block.0.2.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.1.2.layer_norm_1.weight\", \"segformer.encoder.block.1.2.layer_norm_1.bias\", \"segformer.encoder.block.1.2.attention.self.query.weight\", \"segformer.encoder.block.1.2.attention.self.query.bias\", \"segformer.encoder.block.1.2.attention.self.key.weight\", \"segformer.encoder.block.1.2.attention.self.key.bias\", \"segformer.encoder.block.1.2.attention.self.value.weight\", \"segformer.encoder.block.1.2.attention.self.value.bias\", \"segformer.encoder.block.1.2.attention.self.sr.weight\", \"segformer.encoder.block.1.2.attention.self.sr.bias\", \"segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.2.attention.output.dense.weight\", \"segformer.encoder.block.1.2.attention.output.dense.bias\", \"segformer.encoder.block.1.2.layer_norm_2.weight\", \"segformer.encoder.block.1.2.layer_norm_2.bias\", \"segformer.encoder.block.1.2.mlp.dense1.weight\", \"segformer.encoder.block.1.2.mlp.dense1.bias\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.2.mlp.dense2.weight\", \"segformer.encoder.block.1.2.mlp.dense2.bias\", \"segformer.encoder.block.1.3.layer_norm_1.weight\", \"segformer.encoder.block.1.3.layer_norm_1.bias\", \"segformer.encoder.block.1.3.attention.self.query.weight\", \"segformer.encoder.block.1.3.attention.self.query.bias\", \"segformer.encoder.block.1.3.attention.self.key.weight\", \"segformer.encoder.block.1.3.attention.self.key.bias\", \"segformer.encoder.block.1.3.attention.self.value.weight\", \"segformer.encoder.block.1.3.attention.self.value.bias\", \"segformer.encoder.block.1.3.attention.self.sr.weight\", \"segformer.encoder.block.1.3.attention.self.sr.bias\", \"segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.3.attention.output.dense.weight\", \"segformer.encoder.block.1.3.attention.output.dense.bias\", \"segformer.encoder.block.1.3.layer_norm_2.weight\", \"segformer.encoder.block.1.3.layer_norm_2.bias\", \"segformer.encoder.block.1.3.mlp.dense1.weight\", \"segformer.encoder.block.1.3.mlp.dense1.bias\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.3.mlp.dense2.weight\", \"segformer.encoder.block.1.3.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.2.2.layer_norm_1.weight\", \"segformer.encoder.block.2.2.layer_norm_1.bias\", \"segformer.encoder.block.2.2.attention.self.query.weight\", \"segformer.encoder.block.2.2.attention.self.query.bias\", \"segformer.encoder.block.2.2.attention.self.key.weight\", \"segformer.encoder.block.2.2.attention.self.key.bias\", \"segformer.encoder.block.2.2.attention.self.value.weight\", \"segformer.encoder.block.2.2.attention.self.value.bias\", \"segformer.encoder.block.2.2.attention.self.sr.weight\", \"segformer.encoder.block.2.2.attention.self.sr.bias\", \"segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.2.attention.output.dense.weight\", \"segformer.encoder.block.2.2.attention.output.dense.bias\", \"segformer.encoder.block.2.2.layer_norm_2.weight\", \"segformer.encoder.block.2.2.layer_norm_2.bias\", \"segformer.encoder.block.2.2.mlp.dense1.weight\", \"segformer.encoder.block.2.2.mlp.dense1.bias\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.2.mlp.dense2.weight\", \"segformer.encoder.block.2.2.mlp.dense2.bias\", \"segformer.encoder.block.2.3.layer_norm_1.weight\", \"segformer.encoder.block.2.3.layer_norm_1.bias\", \"segformer.encoder.block.2.3.attention.self.query.weight\", \"segformer.encoder.block.2.3.attention.self.query.bias\", \"segformer.encoder.block.2.3.attention.self.key.weight\", \"segformer.encoder.block.2.3.attention.self.key.bias\", \"segformer.encoder.block.2.3.attention.self.value.weight\", \"segformer.encoder.block.2.3.attention.self.value.bias\", \"segformer.encoder.block.2.3.attention.self.sr.weight\", \"segformer.encoder.block.2.3.attention.self.sr.bias\", \"segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.3.attention.output.dense.weight\", \"segformer.encoder.block.2.3.attention.output.dense.bias\", \"segformer.encoder.block.2.3.layer_norm_2.weight\", \"segformer.encoder.block.2.3.layer_norm_2.bias\", \"segformer.encoder.block.2.3.mlp.dense1.weight\", \"segformer.encoder.block.2.3.mlp.dense1.bias\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.3.mlp.dense2.weight\", \"segformer.encoder.block.2.3.mlp.dense2.bias\", \"segformer.encoder.block.2.4.layer_norm_1.weight\", \"segformer.encoder.block.2.4.layer_norm_1.bias\", \"segformer.encoder.block.2.4.attention.self.query.weight\", \"segformer.encoder.block.2.4.attention.self.query.bias\", \"segformer.encoder.block.2.4.attention.self.key.weight\", \"segformer.encoder.block.2.4.attention.self.key.bias\", \"segformer.encoder.block.2.4.attention.self.value.weight\", \"segformer.encoder.block.2.4.attention.self.value.bias\", \"segformer.encoder.block.2.4.attention.self.sr.weight\", \"segformer.encoder.block.2.4.attention.self.sr.bias\", \"segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.4.attention.output.dense.weight\", \"segformer.encoder.block.2.4.attention.output.dense.bias\", \"segformer.encoder.block.2.4.layer_norm_2.weight\", \"segformer.encoder.block.2.4.layer_norm_2.bias\", \"segformer.encoder.block.2.4.mlp.dense1.weight\", \"segformer.encoder.block.2.4.mlp.dense1.bias\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.4.mlp.dense2.weight\", \"segformer.encoder.block.2.4.mlp.dense2.bias\", \"segformer.encoder.block.2.5.layer_norm_1.weight\", \"segformer.encoder.block.2.5.layer_norm_1.bias\", \"segformer.encoder.block.2.5.attention.self.query.weight\", \"segformer.encoder.block.2.5.attention.self.query.bias\", \"segformer.encoder.block.2.5.attention.self.key.weight\", \"segformer.encoder.block.2.5.attention.self.key.bias\", \"segformer.encoder.block.2.5.attention.self.value.weight\", \"segformer.encoder.block.2.5.attention.self.value.bias\", \"segformer.encoder.block.2.5.attention.self.sr.weight\", \"segformer.encoder.block.2.5.attention.self.sr.bias\", \"segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.5.attention.output.dense.weight\", \"segformer.encoder.block.2.5.attention.output.dense.bias\", \"segformer.encoder.block.2.5.layer_norm_2.weight\", \"segformer.encoder.block.2.5.layer_norm_2.bias\", \"segformer.encoder.block.2.5.mlp.dense1.weight\", \"segformer.encoder.block.2.5.mlp.dense1.bias\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.5.mlp.dense2.weight\", \"segformer.encoder.block.2.5.mlp.dense2.bias\", \"segformer.encoder.block.2.6.layer_norm_1.weight\", \"segformer.encoder.block.2.6.layer_norm_1.bias\", \"segformer.encoder.block.2.6.attention.self.query.weight\", \"segformer.encoder.block.2.6.attention.self.query.bias\", \"segformer.encoder.block.2.6.attention.self.key.weight\", \"segformer.encoder.block.2.6.attention.self.key.bias\", \"segformer.encoder.block.2.6.attention.self.value.weight\", \"segformer.encoder.block.2.6.attention.self.value.bias\", \"segformer.encoder.block.2.6.attention.self.sr.weight\", \"segformer.encoder.block.2.6.attention.self.sr.bias\", \"segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.6.attention.output.dense.weight\", \"segformer.encoder.block.2.6.attention.output.dense.bias\", \"segformer.encoder.block.2.6.layer_norm_2.weight\", \"segformer.encoder.block.2.6.layer_norm_2.bias\", \"segformer.encoder.block.2.6.mlp.dense1.weight\", \"segformer.encoder.block.2.6.mlp.dense1.bias\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.6.mlp.dense2.weight\", \"segformer.encoder.block.2.6.mlp.dense2.bias\", \"segformer.encoder.block.2.7.layer_norm_1.weight\", \"segformer.encoder.block.2.7.layer_norm_1.bias\", \"segformer.encoder.block.2.7.attention.self.query.weight\", \"segformer.encoder.block.2.7.attention.self.query.bias\", \"segformer.encoder.block.2.7.attention.self.key.weight\", \"segformer.encoder.block.2.7.attention.self.key.bias\", \"segformer.encoder.block.2.7.attention.self.value.weight\", \"segformer.encoder.block.2.7.attention.self.value.bias\", \"segformer.encoder.block.2.7.attention.self.sr.weight\", \"segformer.encoder.block.2.7.attention.self.sr.bias\", \"segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.7.attention.output.dense.weight\", \"segformer.encoder.block.2.7.attention.output.dense.bias\", \"segformer.encoder.block.2.7.layer_norm_2.weight\", \"segformer.encoder.block.2.7.layer_norm_2.bias\", \"segformer.encoder.block.2.7.mlp.dense1.weight\", \"segformer.encoder.block.2.7.mlp.dense1.bias\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.7.mlp.dense2.weight\", \"segformer.encoder.block.2.7.mlp.dense2.bias\", \"segformer.encoder.block.2.8.layer_norm_1.weight\", \"segformer.encoder.block.2.8.layer_norm_1.bias\", \"segformer.encoder.block.2.8.attention.self.query.weight\", \"segformer.encoder.block.2.8.attention.self.query.bias\", \"segformer.encoder.block.2.8.attention.self.key.weight\", \"segformer.encoder.block.2.8.attention.self.key.bias\", \"segformer.encoder.block.2.8.attention.self.value.weight\", \"segformer.encoder.block.2.8.attention.self.value.bias\", \"segformer.encoder.block.2.8.attention.self.sr.weight\", \"segformer.encoder.block.2.8.attention.self.sr.bias\", \"segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.8.attention.output.dense.weight\", \"segformer.encoder.block.2.8.attention.output.dense.bias\", \"segformer.encoder.block.2.8.layer_norm_2.weight\", \"segformer.encoder.block.2.8.layer_norm_2.bias\", \"segformer.encoder.block.2.8.mlp.dense1.weight\", \"segformer.encoder.block.2.8.mlp.dense1.bias\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.8.mlp.dense2.weight\", \"segformer.encoder.block.2.8.mlp.dense2.bias\", \"segformer.encoder.block.2.9.layer_norm_1.weight\", \"segformer.encoder.block.2.9.layer_norm_1.bias\", \"segformer.encoder.block.2.9.attention.self.query.weight\", \"segformer.encoder.block.2.9.attention.self.query.bias\", \"segformer.encoder.block.2.9.attention.self.key.weight\", \"segformer.encoder.block.2.9.attention.self.key.bias\", \"segformer.encoder.block.2.9.attention.self.value.weight\", \"segformer.encoder.block.2.9.attention.self.value.bias\", \"segformer.encoder.block.2.9.attention.self.sr.weight\", \"segformer.encoder.block.2.9.attention.self.sr.bias\", \"segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.9.attention.output.dense.weight\", \"segformer.encoder.block.2.9.attention.output.dense.bias\", \"segformer.encoder.block.2.9.layer_norm_2.weight\", \"segformer.encoder.block.2.9.layer_norm_2.bias\", \"segformer.encoder.block.2.9.mlp.dense1.weight\", \"segformer.encoder.block.2.9.mlp.dense1.bias\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.9.mlp.dense2.weight\", \"segformer.encoder.block.2.9.mlp.dense2.bias\", \"segformer.encoder.block.2.10.layer_norm_1.weight\", \"segformer.encoder.block.2.10.layer_norm_1.bias\", \"segformer.encoder.block.2.10.attention.self.query.weight\", \"segformer.encoder.block.2.10.attention.self.query.bias\", \"segformer.encoder.block.2.10.attention.self.key.weight\", \"segformer.encoder.block.2.10.attention.self.key.bias\", \"segformer.encoder.block.2.10.attention.self.value.weight\", \"segformer.encoder.block.2.10.attention.self.value.bias\", \"segformer.encoder.block.2.10.attention.self.sr.weight\", \"segformer.encoder.block.2.10.attention.self.sr.bias\", \"segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.10.attention.output.dense.weight\", \"segformer.encoder.block.2.10.attention.output.dense.bias\", \"segformer.encoder.block.2.10.layer_norm_2.weight\", \"segformer.encoder.block.2.10.layer_norm_2.bias\", \"segformer.encoder.block.2.10.mlp.dense1.weight\", \"segformer.encoder.block.2.10.mlp.dense1.bias\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.10.mlp.dense2.weight\", \"segformer.encoder.block.2.10.mlp.dense2.bias\", \"segformer.encoder.block.2.11.layer_norm_1.weight\", \"segformer.encoder.block.2.11.layer_norm_1.bias\", \"segformer.encoder.block.2.11.attention.self.query.weight\", \"segformer.encoder.block.2.11.attention.self.query.bias\", \"segformer.encoder.block.2.11.attention.self.key.weight\", \"segformer.encoder.block.2.11.attention.self.key.bias\", \"segformer.encoder.block.2.11.attention.self.value.weight\", \"segformer.encoder.block.2.11.attention.self.value.bias\", \"segformer.encoder.block.2.11.attention.self.sr.weight\", \"segformer.encoder.block.2.11.attention.self.sr.bias\", \"segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.11.attention.output.dense.weight\", \"segformer.encoder.block.2.11.attention.output.dense.bias\", \"segformer.encoder.block.2.11.layer_norm_2.weight\", \"segformer.encoder.block.2.11.layer_norm_2.bias\", \"segformer.encoder.block.2.11.mlp.dense1.weight\", \"segformer.encoder.block.2.11.mlp.dense1.bias\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.11.mlp.dense2.weight\", \"segformer.encoder.block.2.11.mlp.dense2.bias\", \"segformer.encoder.block.2.12.layer_norm_1.weight\", \"segformer.encoder.block.2.12.layer_norm_1.bias\", \"segformer.encoder.block.2.12.attention.self.query.weight\", \"segformer.encoder.block.2.12.attention.self.query.bias\", \"segformer.encoder.block.2.12.attention.self.key.weight\", \"segformer.encoder.block.2.12.attention.self.key.bias\", \"segformer.encoder.block.2.12.attention.self.value.weight\", \"segformer.encoder.block.2.12.attention.self.value.bias\", \"segformer.encoder.block.2.12.attention.self.sr.weight\", \"segformer.encoder.block.2.12.attention.self.sr.bias\", \"segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.12.attention.output.dense.weight\", \"segformer.encoder.block.2.12.attention.output.dense.bias\", \"segformer.encoder.block.2.12.layer_norm_2.weight\", \"segformer.encoder.block.2.12.layer_norm_2.bias\", \"segformer.encoder.block.2.12.mlp.dense1.weight\", \"segformer.encoder.block.2.12.mlp.dense1.bias\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.12.mlp.dense2.weight\", \"segformer.encoder.block.2.12.mlp.dense2.bias\", \"segformer.encoder.block.2.13.layer_norm_1.weight\", \"segformer.encoder.block.2.13.layer_norm_1.bias\", \"segformer.encoder.block.2.13.attention.self.query.weight\", \"segformer.encoder.block.2.13.attention.self.query.bias\", \"segformer.encoder.block.2.13.attention.self.key.weight\", \"segformer.encoder.block.2.13.attention.self.key.bias\", \"segformer.encoder.block.2.13.attention.self.value.weight\", \"segformer.encoder.block.2.13.attention.self.value.bias\", \"segformer.encoder.block.2.13.attention.self.sr.weight\", \"segformer.encoder.block.2.13.attention.self.sr.bias\", \"segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.13.attention.output.dense.weight\", \"segformer.encoder.block.2.13.attention.output.dense.bias\", \"segformer.encoder.block.2.13.layer_norm_2.weight\", \"segformer.encoder.block.2.13.layer_norm_2.bias\", \"segformer.encoder.block.2.13.mlp.dense1.weight\", \"segformer.encoder.block.2.13.mlp.dense1.bias\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.13.mlp.dense2.weight\", \"segformer.encoder.block.2.13.mlp.dense2.bias\", \"segformer.encoder.block.2.14.layer_norm_1.weight\", \"segformer.encoder.block.2.14.layer_norm_1.bias\", \"segformer.encoder.block.2.14.attention.self.query.weight\", \"segformer.encoder.block.2.14.attention.self.query.bias\", \"segformer.encoder.block.2.14.attention.self.key.weight\", \"segformer.encoder.block.2.14.attention.self.key.bias\", \"segformer.encoder.block.2.14.attention.self.value.weight\", \"segformer.encoder.block.2.14.attention.self.value.bias\", \"segformer.encoder.block.2.14.attention.self.sr.weight\", \"segformer.encoder.block.2.14.attention.self.sr.bias\", \"segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.14.attention.output.dense.weight\", \"segformer.encoder.block.2.14.attention.output.dense.bias\", \"segformer.encoder.block.2.14.layer_norm_2.weight\", \"segformer.encoder.block.2.14.layer_norm_2.bias\", \"segformer.encoder.block.2.14.mlp.dense1.weight\", \"segformer.encoder.block.2.14.mlp.dense1.bias\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.14.mlp.dense2.weight\", \"segformer.encoder.block.2.14.mlp.dense2.bias\", \"segformer.encoder.block.2.15.layer_norm_1.weight\", \"segformer.encoder.block.2.15.layer_norm_1.bias\", \"segformer.encoder.block.2.15.attention.self.query.weight\", \"segformer.encoder.block.2.15.attention.self.query.bias\", \"segformer.encoder.block.2.15.attention.self.key.weight\", \"segformer.encoder.block.2.15.attention.self.key.bias\", \"segformer.encoder.block.2.15.attention.self.value.weight\", \"segformer.encoder.block.2.15.attention.self.value.bias\", \"segformer.encoder.block.2.15.attention.self.sr.weight\", \"segformer.encoder.block.2.15.attention.self.sr.bias\", \"segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.15.attention.output.dense.weight\", \"segformer.encoder.block.2.15.attention.output.dense.bias\", \"segformer.encoder.block.2.15.layer_norm_2.weight\", \"segformer.encoder.block.2.15.layer_norm_2.bias\", \"segformer.encoder.block.2.15.mlp.dense1.weight\", \"segformer.encoder.block.2.15.mlp.dense1.bias\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.15.mlp.dense2.weight\", \"segformer.encoder.block.2.15.mlp.dense2.bias\", \"segformer.encoder.block.2.16.layer_norm_1.weight\", \"segformer.encoder.block.2.16.layer_norm_1.bias\", \"segformer.encoder.block.2.16.attention.self.query.weight\", \"segformer.encoder.block.2.16.attention.self.query.bias\", \"segformer.encoder.block.2.16.attention.self.key.weight\", \"segformer.encoder.block.2.16.attention.self.key.bias\", \"segformer.encoder.block.2.16.attention.self.value.weight\", \"segformer.encoder.block.2.16.attention.self.value.bias\", \"segformer.encoder.block.2.16.attention.self.sr.weight\", \"segformer.encoder.block.2.16.attention.self.sr.bias\", \"segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.16.attention.output.dense.weight\", \"segformer.encoder.block.2.16.attention.output.dense.bias\", \"segformer.encoder.block.2.16.layer_norm_2.weight\", \"segformer.encoder.block.2.16.layer_norm_2.bias\", \"segformer.encoder.block.2.16.mlp.dense1.weight\", \"segformer.encoder.block.2.16.mlp.dense1.bias\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.16.mlp.dense2.weight\", \"segformer.encoder.block.2.16.mlp.dense2.bias\", \"segformer.encoder.block.2.17.layer_norm_1.weight\", \"segformer.encoder.block.2.17.layer_norm_1.bias\", \"segformer.encoder.block.2.17.attention.self.query.weight\", \"segformer.encoder.block.2.17.attention.self.query.bias\", \"segformer.encoder.block.2.17.attention.self.key.weight\", \"segformer.encoder.block.2.17.attention.self.key.bias\", \"segformer.encoder.block.2.17.attention.self.value.weight\", \"segformer.encoder.block.2.17.attention.self.value.bias\", \"segformer.encoder.block.2.17.attention.self.sr.weight\", \"segformer.encoder.block.2.17.attention.self.sr.bias\", \"segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.17.attention.output.dense.weight\", \"segformer.encoder.block.2.17.attention.output.dense.bias\", \"segformer.encoder.block.2.17.layer_norm_2.weight\", \"segformer.encoder.block.2.17.layer_norm_2.bias\", \"segformer.encoder.block.2.17.mlp.dense1.weight\", \"segformer.encoder.block.2.17.mlp.dense1.bias\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.17.mlp.dense2.weight\", \"segformer.encoder.block.2.17.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.block.3.2.layer_norm_1.weight\", \"segformer.encoder.block.3.2.layer_norm_1.bias\", \"segformer.encoder.block.3.2.attention.self.query.weight\", \"segformer.encoder.block.3.2.attention.self.query.bias\", \"segformer.encoder.block.3.2.attention.self.key.weight\", \"segformer.encoder.block.3.2.attention.self.key.bias\", \"segformer.encoder.block.3.2.attention.self.value.weight\", \"segformer.encoder.block.3.2.attention.self.value.bias\", \"segformer.encoder.block.3.2.attention.output.dense.weight\", \"segformer.encoder.block.3.2.attention.output.dense.bias\", \"segformer.encoder.block.3.2.layer_norm_2.weight\", \"segformer.encoder.block.3.2.layer_norm_2.bias\", \"segformer.encoder.block.3.2.mlp.dense1.weight\", \"segformer.encoder.block.3.2.mlp.dense1.bias\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.2.mlp.dense2.weight\", \"segformer.encoder.block.3.2.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"module.segformer.encoder.patch_embeddings.0.proj.weight\", \"module.segformer.encoder.patch_embeddings.0.proj.bias\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.1.proj.weight\", \"module.segformer.encoder.patch_embeddings.1.proj.bias\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.2.proj.weight\", \"module.segformer.encoder.patch_embeddings.2.proj.bias\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.3.proj.weight\", \"module.segformer.encoder.patch_embeddings.3.proj.bias\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"module.segformer.encoder.block.0.0.layer_norm_1.weight\", \"module.segformer.encoder.block.0.0.layer_norm_1.bias\", \"module.segformer.encoder.block.0.0.attention.self.query.weight\", \"module.segformer.encoder.block.0.0.attention.self.query.bias\", \"module.segformer.encoder.block.0.0.attention.self.key.weight\", \"module.segformer.encoder.block.0.0.attention.self.key.bias\", \"module.segformer.encoder.block.0.0.attention.self.value.weight\", \"module.segformer.encoder.block.0.0.attention.self.value.bias\", \"module.segformer.encoder.block.0.0.attention.self.sr.weight\", \"module.segformer.encoder.block.0.0.attention.self.sr.bias\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.0.attention.output.dense.weight\", \"module.segformer.encoder.block.0.0.attention.output.dense.bias\", \"module.segformer.encoder.block.0.0.layer_norm_2.weight\", \"module.segformer.encoder.block.0.0.layer_norm_2.bias\", \"module.segformer.encoder.block.0.0.mlp.dense1.weight\", \"module.segformer.encoder.block.0.0.mlp.dense1.bias\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.0.mlp.dense2.weight\", \"module.segformer.encoder.block.0.0.mlp.dense2.bias\", \"module.segformer.encoder.block.0.1.layer_norm_1.weight\", \"module.segformer.encoder.block.0.1.layer_norm_1.bias\", \"module.segformer.encoder.block.0.1.attention.self.query.weight\", \"module.segformer.encoder.block.0.1.attention.self.query.bias\", \"module.segformer.encoder.block.0.1.attention.self.key.weight\", \"module.segformer.encoder.block.0.1.attention.self.key.bias\", \"module.segformer.encoder.block.0.1.attention.self.value.weight\", \"module.segformer.encoder.block.0.1.attention.self.value.bias\", \"module.segformer.encoder.block.0.1.attention.self.sr.weight\", \"module.segformer.encoder.block.0.1.attention.self.sr.bias\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.1.attention.output.dense.weight\", \"module.segformer.encoder.block.0.1.attention.output.dense.bias\", \"module.segformer.encoder.block.0.1.layer_norm_2.weight\", \"module.segformer.encoder.block.0.1.layer_norm_2.bias\", \"module.segformer.encoder.block.0.1.mlp.dense1.weight\", \"module.segformer.encoder.block.0.1.mlp.dense1.bias\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.1.mlp.dense2.weight\", \"module.segformer.encoder.block.0.1.mlp.dense2.bias\", \"module.segformer.encoder.block.0.2.layer_norm_1.weight\", \"module.segformer.encoder.block.0.2.layer_norm_1.bias\", \"module.segformer.encoder.block.0.2.attention.self.query.weight\", \"module.segformer.encoder.block.0.2.attention.self.query.bias\", \"module.segformer.encoder.block.0.2.attention.self.key.weight\", \"module.segformer.encoder.block.0.2.attention.self.key.bias\", \"module.segformer.encoder.block.0.2.attention.self.value.weight\", \"module.segformer.encoder.block.0.2.attention.self.value.bias\", \"module.segformer.encoder.block.0.2.attention.self.sr.weight\", \"module.segformer.encoder.block.0.2.attention.self.sr.bias\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.2.attention.output.dense.weight\", \"module.segformer.encoder.block.0.2.attention.output.dense.bias\", \"module.segformer.encoder.block.0.2.layer_norm_2.weight\", \"module.segformer.encoder.block.0.2.layer_norm_2.bias\", \"module.segformer.encoder.block.0.2.mlp.dense1.weight\", \"module.segformer.encoder.block.0.2.mlp.dense1.bias\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.2.mlp.dense2.weight\", \"module.segformer.encoder.block.0.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.0.layer_norm_1.weight\", \"module.segformer.encoder.block.1.0.layer_norm_1.bias\", \"module.segformer.encoder.block.1.0.attention.self.query.weight\", \"module.segformer.encoder.block.1.0.attention.self.query.bias\", \"module.segformer.encoder.block.1.0.attention.self.key.weight\", \"module.segformer.encoder.block.1.0.attention.self.key.bias\", \"module.segformer.encoder.block.1.0.attention.self.value.weight\", \"module.segformer.encoder.block.1.0.attention.self.value.bias\", \"module.segformer.encoder.block.1.0.attention.self.sr.weight\", \"module.segformer.encoder.block.1.0.attention.self.sr.bias\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.0.attention.output.dense.weight\", \"module.segformer.encoder.block.1.0.attention.output.dense.bias\", \"module.segformer.encoder.block.1.0.layer_norm_2.weight\", \"module.segformer.encoder.block.1.0.layer_norm_2.bias\", \"module.segformer.encoder.block.1.0.mlp.dense1.weight\", \"module.segformer.encoder.block.1.0.mlp.dense1.bias\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.0.mlp.dense2.weight\", \"module.segformer.encoder.block.1.0.mlp.dense2.bias\", \"module.segformer.encoder.block.1.1.layer_norm_1.weight\", \"module.segformer.encoder.block.1.1.layer_norm_1.bias\", \"module.segformer.encoder.block.1.1.attention.self.query.weight\", \"module.segformer.encoder.block.1.1.attention.self.query.bias\", \"module.segformer.encoder.block.1.1.attention.self.key.weight\", \"module.segformer.encoder.block.1.1.attention.self.key.bias\", \"module.segformer.encoder.block.1.1.attention.self.value.weight\", \"module.segformer.encoder.block.1.1.attention.self.value.bias\", \"module.segformer.encoder.block.1.1.attention.self.sr.weight\", \"module.segformer.encoder.block.1.1.attention.self.sr.bias\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.1.attention.output.dense.weight\", \"module.segformer.encoder.block.1.1.attention.output.dense.bias\", \"module.segformer.encoder.block.1.1.layer_norm_2.weight\", \"module.segformer.encoder.block.1.1.layer_norm_2.bias\", \"module.segformer.encoder.block.1.1.mlp.dense1.weight\", \"module.segformer.encoder.block.1.1.mlp.dense1.bias\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.1.mlp.dense2.weight\", \"module.segformer.encoder.block.1.1.mlp.dense2.bias\", \"module.segformer.encoder.block.1.2.layer_norm_1.weight\", \"module.segformer.encoder.block.1.2.layer_norm_1.bias\", \"module.segformer.encoder.block.1.2.attention.self.query.weight\", \"module.segformer.encoder.block.1.2.attention.self.query.bias\", \"module.segformer.encoder.block.1.2.attention.self.key.weight\", \"module.segformer.encoder.block.1.2.attention.self.key.bias\", \"module.segformer.encoder.block.1.2.attention.self.value.weight\", \"module.segformer.encoder.block.1.2.attention.self.value.bias\", \"module.segformer.encoder.block.1.2.attention.self.sr.weight\", \"module.segformer.encoder.block.1.2.attention.self.sr.bias\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.2.attention.output.dense.weight\", \"module.segformer.encoder.block.1.2.attention.output.dense.bias\", \"module.segformer.encoder.block.1.2.layer_norm_2.weight\", \"module.segformer.encoder.block.1.2.layer_norm_2.bias\", \"module.segformer.encoder.block.1.2.mlp.dense1.weight\", \"module.segformer.encoder.block.1.2.mlp.dense1.bias\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.2.mlp.dense2.weight\", \"module.segformer.encoder.block.1.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.3.layer_norm_1.weight\", \"module.segformer.encoder.block.1.3.layer_norm_1.bias\", \"module.segformer.encoder.block.1.3.attention.self.query.weight\", \"module.segformer.encoder.block.1.3.attention.self.query.bias\", \"module.segformer.encoder.block.1.3.attention.self.key.weight\", \"module.segformer.encoder.block.1.3.attention.self.key.bias\", \"module.segformer.encoder.block.1.3.attention.self.value.weight\", \"module.segformer.encoder.block.1.3.attention.self.value.bias\", \"module.segformer.encoder.block.1.3.attention.self.sr.weight\", \"module.segformer.encoder.block.1.3.attention.self.sr.bias\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.3.attention.output.dense.weight\", \"module.segformer.encoder.block.1.3.attention.output.dense.bias\", \"module.segformer.encoder.block.1.3.layer_norm_2.weight\", \"module.segformer.encoder.block.1.3.layer_norm_2.bias\", \"module.segformer.encoder.block.1.3.mlp.dense1.weight\", \"module.segformer.encoder.block.1.3.mlp.dense1.bias\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.3.mlp.dense2.weight\", \"module.segformer.encoder.block.1.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.0.layer_norm_1.weight\", \"module.segformer.encoder.block.2.0.layer_norm_1.bias\", \"module.segformer.encoder.block.2.0.attention.self.query.weight\", \"module.segformer.encoder.block.2.0.attention.self.query.bias\", \"module.segformer.encoder.block.2.0.attention.self.key.weight\", \"module.segformer.encoder.block.2.0.attention.self.key.bias\", \"module.segformer.encoder.block.2.0.attention.self.value.weight\", \"module.segformer.encoder.block.2.0.attention.self.value.bias\", \"module.segformer.encoder.block.2.0.attention.self.sr.weight\", \"module.segformer.encoder.block.2.0.attention.self.sr.bias\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.0.attention.output.dense.weight\", \"module.segformer.encoder.block.2.0.attention.output.dense.bias\", \"module.segformer.encoder.block.2.0.layer_norm_2.weight\", \"module.segformer.encoder.block.2.0.layer_norm_2.bias\", \"module.segformer.encoder.block.2.0.mlp.dense1.weight\", \"module.segformer.encoder.block.2.0.mlp.dense1.bias\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.0.mlp.dense2.weight\", \"module.segformer.encoder.block.2.0.mlp.dense2.bias\", \"module.segformer.encoder.block.2.1.layer_norm_1.weight\", \"module.segformer.encoder.block.2.1.layer_norm_1.bias\", \"module.segformer.encoder.block.2.1.attention.self.query.weight\", \"module.segformer.encoder.block.2.1.attention.self.query.bias\", \"module.segformer.encoder.block.2.1.attention.self.key.weight\", \"module.segformer.encoder.block.2.1.attention.self.key.bias\", \"module.segformer.encoder.block.2.1.attention.self.value.weight\", \"module.segformer.encoder.block.2.1.attention.self.value.bias\", \"module.segformer.encoder.block.2.1.attention.self.sr.weight\", \"module.segformer.encoder.block.2.1.attention.self.sr.bias\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.1.attention.output.dense.weight\", \"module.segformer.encoder.block.2.1.attention.output.dense.bias\", \"module.segformer.encoder.block.2.1.layer_norm_2.weight\", \"module.segformer.encoder.block.2.1.layer_norm_2.bias\", \"module.segformer.encoder.block.2.1.mlp.dense1.weight\", \"module.segformer.encoder.block.2.1.mlp.dense1.bias\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.1.mlp.dense2.weight\", \"module.segformer.encoder.block.2.1.mlp.dense2.bias\", \"module.segformer.encoder.block.2.2.layer_norm_1.weight\", \"module.segformer.encoder.block.2.2.layer_norm_1.bias\", \"module.segformer.encoder.block.2.2.attention.self.query.weight\", \"module.segformer.encoder.block.2.2.attention.self.query.bias\", \"module.segformer.encoder.block.2.2.attention.self.key.weight\", \"module.segformer.encoder.block.2.2.attention.self.key.bias\", \"module.segformer.encoder.block.2.2.attention.self.value.weight\", \"module.segformer.encoder.block.2.2.attention.self.value.bias\", \"module.segformer.encoder.block.2.2.attention.self.sr.weight\", \"module.segformer.encoder.block.2.2.attention.self.sr.bias\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.2.attention.output.dense.weight\", \"module.segformer.encoder.block.2.2.attention.output.dense.bias\", \"module.segformer.encoder.block.2.2.layer_norm_2.weight\", \"module.segformer.encoder.block.2.2.layer_norm_2.bias\", \"module.segformer.encoder.block.2.2.mlp.dense1.weight\", \"module.segformer.encoder.block.2.2.mlp.dense1.bias\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.2.mlp.dense2.weight\", \"module.segformer.encoder.block.2.2.mlp.dense2.bias\", \"module.segformer.encoder.block.2.3.layer_norm_1.weight\", \"module.segformer.encoder.block.2.3.layer_norm_1.bias\", \"module.segformer.encoder.block.2.3.attention.self.query.weight\", \"module.segformer.encoder.block.2.3.attention.self.query.bias\", \"module.segformer.encoder.block.2.3.attention.self.key.weight\", \"module.segformer.encoder.block.2.3.attention.self.key.bias\", \"module.segformer.encoder.block.2.3.attention.self.value.weight\", \"module.segformer.encoder.block.2.3.attention.self.value.bias\", \"module.segformer.encoder.block.2.3.attention.self.sr.weight\", \"module.segformer.encoder.block.2.3.attention.self.sr.bias\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.3.attention.output.dense.weight\", \"module.segformer.encoder.block.2.3.attention.output.dense.bias\", \"module.segformer.encoder.block.2.3.layer_norm_2.weight\", \"module.segformer.encoder.block.2.3.layer_norm_2.bias\", \"module.segformer.encoder.block.2.3.mlp.dense1.weight\", \"module.segformer.encoder.block.2.3.mlp.dense1.bias\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.3.mlp.dense2.weight\", \"module.segformer.encoder.block.2.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.4.layer_norm_1.weight\", \"module.segformer.encoder.block.2.4.layer_norm_1.bias\", \"module.segformer.encoder.block.2.4.attention.self.query.weight\", \"module.segformer.encoder.block.2.4.attention.self.query.bias\", \"module.segformer.encoder.block.2.4.attention.self.key.weight\", \"module.segformer.encoder.block.2.4.attention.self.key.bias\", \"module.segformer.encoder.block.2.4.attention.self.value.weight\", \"module.segformer.encoder.block.2.4.attention.self.value.bias\", \"module.segformer.encoder.block.2.4.attention.self.sr.weight\", \"module.segformer.encoder.block.2.4.attention.self.sr.bias\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.4.attention.output.dense.weight\", \"module.segformer.encoder.block.2.4.attention.output.dense.bias\", \"module.segformer.encoder.block.2.4.layer_norm_2.weight\", \"module.segformer.encoder.block.2.4.layer_norm_2.bias\", \"module.segformer.encoder.block.2.4.mlp.dense1.weight\", \"module.segformer.encoder.block.2.4.mlp.dense1.bias\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.4.mlp.dense2.weight\", \"module.segformer.encoder.block.2.4.mlp.dense2.bias\", \"module.segformer.encoder.block.2.5.layer_norm_1.weight\", \"module.segformer.encoder.block.2.5.layer_norm_1.bias\", \"module.segformer.encoder.block.2.5.attention.self.query.weight\", \"module.segformer.encoder.block.2.5.attention.self.query.bias\", \"module.segformer.encoder.block.2.5.attention.self.key.weight\", \"module.segformer.encoder.block.2.5.attention.self.key.bias\", \"module.segformer.encoder.block.2.5.attention.self.value.weight\", \"module.segformer.encoder.block.2.5.attention.self.value.bias\", \"module.segformer.encoder.block.2.5.attention.self.sr.weight\", \"module.segformer.encoder.block.2.5.attention.self.sr.bias\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.5.attention.output.dense.weight\", \"module.segformer.encoder.block.2.5.attention.output.dense.bias\", \"module.segformer.encoder.block.2.5.layer_norm_2.weight\", \"module.segformer.encoder.block.2.5.layer_norm_2.bias\", \"module.segformer.encoder.block.2.5.mlp.dense1.weight\", \"module.segformer.encoder.block.2.5.mlp.dense1.bias\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.5.mlp.dense2.weight\", \"module.segformer.encoder.block.2.5.mlp.dense2.bias\", \"module.segformer.encoder.block.2.6.layer_norm_1.weight\", \"module.segformer.encoder.block.2.6.layer_norm_1.bias\", \"module.segformer.encoder.block.2.6.attention.self.query.weight\", \"module.segformer.encoder.block.2.6.attention.self.query.bias\", \"module.segformer.encoder.block.2.6.attention.self.key.weight\", \"module.segformer.encoder.block.2.6.attention.self.key.bias\", \"module.segformer.encoder.block.2.6.attention.self.value.weight\", \"module.segformer.encoder.block.2.6.attention.self.value.bias\", \"module.segformer.encoder.block.2.6.attention.self.sr.weight\", \"module.segformer.encoder.block.2.6.attention.self.sr.bias\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.6.attention.output.dense.weight\", \"module.segformer.encoder.block.2.6.attention.output.dense.bias\", \"module.segformer.encoder.block.2.6.layer_norm_2.weight\", \"module.segformer.encoder.block.2.6.layer_norm_2.bias\", \"module.segformer.encoder.block.2.6.mlp.dense1.weight\", \"module.segformer.encoder.block.2.6.mlp.dense1.bias\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.6.mlp.dense2.weight\", \"module.segformer.encoder.block.2.6.mlp.dense2.bias\", \"module.segformer.encoder.block.2.7.layer_norm_1.weight\", \"module.segformer.encoder.block.2.7.layer_norm_1.bias\", \"module.segformer.encoder.block.2.7.attention.self.query.weight\", \"module.segformer.encoder.block.2.7.attention.self.query.bias\", \"module.segformer.encoder.block.2.7.attention.self.key.weight\", \"module.segformer.encoder.block.2.7.attention.self.key.bias\", \"module.segformer.encoder.block.2.7.attention.self.value.weight\", \"module.segformer.encoder.block.2.7.attention.self.value.bias\", \"module.segformer.encoder.block.2.7.attention.self.sr.weight\", \"module.segformer.encoder.block.2.7.attention.self.sr.bias\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.7.attention.output.dense.weight\", \"module.segformer.encoder.block.2.7.attention.output.dense.bias\", \"module.segformer.encoder.block.2.7.layer_norm_2.weight\", \"module.segformer.encoder.block.2.7.layer_norm_2.bias\", \"module.segformer.encoder.block.2.7.mlp.dense1.weight\", \"module.segformer.encoder.block.2.7.mlp.dense1.bias\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.7.mlp.dense2.weight\", \"module.segformer.encoder.block.2.7.mlp.dense2.bias\", \"module.segformer.encoder.block.2.8.layer_norm_1.weight\", \"module.segformer.encoder.block.2.8.layer_norm_1.bias\", \"module.segformer.encoder.block.2.8.attention.self.query.weight\", \"module.segformer.encoder.block.2.8.attention.self.query.bias\", \"module.segformer.encoder.block.2.8.attention.self.key.weight\", \"module.segformer.encoder.block.2.8.attention.self.key.bias\", \"module.segformer.encoder.block.2.8.attention.self.value.weight\", \"module.segformer.encoder.block.2.8.attention.self.value.bias\", \"module.segformer.encoder.block.2.8.attention.self.sr.weight\", \"module.segformer.encoder.block.2.8.attention.self.sr.bias\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.8.attention.output.dense.weight\", \"module.segformer.encoder.block.2.8.attention.output.dense.bias\", \"module.segformer.encoder.block.2.8.layer_norm_2.weight\", \"module.segformer.encoder.block.2.8.layer_norm_2.bias\", \"module.segformer.encoder.block.2.8.mlp.dense1.weight\", \"module.segformer.encoder.block.2.8.mlp.dense1.bias\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.8.mlp.dense2.weight\", \"module.segformer.encoder.block.2.8.mlp.dense2.bias\", \"module.segformer.encoder.block.2.9.layer_norm_1.weight\", \"module.segformer.encoder.block.2.9.layer_norm_1.bias\", \"module.segformer.encoder.block.2.9.attention.self.query.weight\", \"module.segformer.encoder.block.2.9.attention.self.query.bias\", \"module.segformer.encoder.block.2.9.attention.self.key.weight\", \"module.segformer.encoder.block.2.9.attention.self.key.bias\", \"module.segformer.encoder.block.2.9.attention.self.value.weight\", \"module.segformer.encoder.block.2.9.attention.self.value.bias\", \"module.segformer.encoder.block.2.9.attention.self.sr.weight\", \"module.segformer.encoder.block.2.9.attention.self.sr.bias\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.9.attention.output.dense.weight\", \"module.segformer.encoder.block.2.9.attention.output.dense.bias\", \"module.segformer.encoder.block.2.9.layer_norm_2.weight\", \"module.segformer.encoder.block.2.9.layer_norm_2.bias\", \"module.segformer.encoder.block.2.9.mlp.dense1.weight\", \"module.segformer.encoder.block.2.9.mlp.dense1.bias\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.9.mlp.dense2.weight\", \"module.segformer.encoder.block.2.9.mlp.dense2.bias\", \"module.segformer.encoder.block.2.10.layer_norm_1.weight\", \"module.segformer.encoder.block.2.10.layer_norm_1.bias\", \"module.segformer.encoder.block.2.10.attention.self.query.weight\", \"module.segformer.encoder.block.2.10.attention.self.query.bias\", \"module.segformer.encoder.block.2.10.attention.self.key.weight\", \"module.segformer.encoder.block.2.10.attention.self.key.bias\", \"module.segformer.encoder.block.2.10.attention.self.value.weight\", \"module.segformer.encoder.block.2.10.attention.self.value.bias\", \"module.segformer.encoder.block.2.10.attention.self.sr.weight\", \"module.segformer.encoder.block.2.10.attention.self.sr.bias\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.10.attention.output.dense.weight\", \"module.segformer.encoder.block.2.10.attention.output.dense.bias\", \"module.segformer.encoder.block.2.10.layer_norm_2.weight\", \"module.segformer.encoder.block.2.10.layer_norm_2.bias\", \"module.segformer.encoder.block.2.10.mlp.dense1.weight\", \"module.segformer.encoder.block.2.10.mlp.dense1.bias\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.10.mlp.dense2.weight\", \"module.segformer.encoder.block.2.10.mlp.dense2.bias\", \"module.segformer.encoder.block.2.11.layer_norm_1.weight\", \"module.segformer.encoder.block.2.11.layer_norm_1.bias\", \"module.segformer.encoder.block.2.11.attention.self.query.weight\", \"module.segformer.encoder.block.2.11.attention.self.query.bias\", \"module.segformer.encoder.block.2.11.attention.self.key.weight\", \"module.segformer.encoder.block.2.11.attention.self.key.bias\", \"module.segformer.encoder.block.2.11.attention.self.value.weight\", \"module.segformer.encoder.block.2.11.attention.self.value.bias\", \"module.segformer.encoder.block.2.11.attention.self.sr.weight\", \"module.segformer.encoder.block.2.11.attention.self.sr.bias\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.11.attention.output.dense.weight\", \"module.segformer.encoder.block.2.11.attention.output.dense.bias\", \"module.segformer.encoder.block.2.11.layer_norm_2.weight\", \"module.segformer.encoder.block.2.11.layer_norm_2.bias\", \"module.segformer.encoder.block.2.11.mlp.dense1.weight\", \"module.segformer.encoder.block.2.11.mlp.dense1.bias\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.11.mlp.dense2.weight\", \"module.segformer.encoder.block.2.11.mlp.dense2.bias\", \"module.segformer.encoder.block.2.12.layer_norm_1.weight\", \"module.segformer.encoder.block.2.12.layer_norm_1.bias\", \"module.segformer.encoder.block.2.12.attention.self.query.weight\", \"module.segformer.encoder.block.2.12.attention.self.query.bias\", \"module.segformer.encoder.block.2.12.attention.self.key.weight\", \"module.segformer.encoder.block.2.12.attention.self.key.bias\", \"module.segformer.encoder.block.2.12.attention.self.value.weight\", \"module.segformer.encoder.block.2.12.attention.self.value.bias\", \"module.segformer.encoder.block.2.12.attention.self.sr.weight\", \"module.segformer.encoder.block.2.12.attention.self.sr.bias\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.12.attention.output.dense.weight\", \"module.segformer.encoder.block.2.12.attention.output.dense.bias\", \"module.segformer.encoder.block.2.12.layer_norm_2.weight\", \"module.segformer.encoder.block.2.12.layer_norm_2.bias\", \"module.segformer.encoder.block.2.12.mlp.dense1.weight\", \"module.segformer.encoder.block.2.12.mlp.dense1.bias\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.12.mlp.dense2.weight\", \"module.segformer.encoder.block.2.12.mlp.dense2.bias\", \"module.segformer.encoder.block.2.13.layer_norm_1.weight\", \"module.segformer.encoder.block.2.13.layer_norm_1.bias\", \"module.segformer.encoder.block.2.13.attention.self.query.weight\", \"module.segformer.encoder.block.2.13.attention.self.query.bias\", \"module.segformer.encoder.block.2.13.attention.self.key.weight\", \"module.segformer.encoder.block.2.13.attention.self.key.bias\", \"module.segformer.encoder.block.2.13.attention.self.value.weight\", \"module.segformer.encoder.block.2.13.attention.self.value.bias\", \"module.segformer.encoder.block.2.13.attention.self.sr.weight\", \"module.segformer.encoder.block.2.13.attention.self.sr.bias\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.13.attention.output.dense.weight\", \"module.segformer.encoder.block.2.13.attention.output.dense.bias\", \"module.segformer.encoder.block.2.13.layer_norm_2.weight\", \"module.segformer.encoder.block.2.13.layer_norm_2.bias\", \"module.segformer.encoder.block.2.13.mlp.dense1.weight\", \"module.segformer.encoder.block.2.13.mlp.dense1.bias\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.13.mlp.dense2.weight\", \"module.segformer.encoder.block.2.13.mlp.dense2.bias\", \"module.segformer.encoder.block.2.14.layer_norm_1.weight\", \"module.segformer.encoder.block.2.14.layer_norm_1.bias\", \"module.segformer.encoder.block.2.14.attention.self.query.weight\", \"module.segformer.encoder.block.2.14.attention.self.query.bias\", \"module.segformer.encoder.block.2.14.attention.self.key.weight\", \"module.segformer.encoder.block.2.14.attention.self.key.bias\", \"module.segformer.encoder.block.2.14.attention.self.value.weight\", \"module.segformer.encoder.block.2.14.attention.self.value.bias\", \"module.segformer.encoder.block.2.14.attention.self.sr.weight\", \"module.segformer.encoder.block.2.14.attention.self.sr.bias\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.14.attention.output.dense.weight\", \"module.segformer.encoder.block.2.14.attention.output.dense.bias\", \"module.segformer.encoder.block.2.14.layer_norm_2.weight\", \"module.segformer.encoder.block.2.14.layer_norm_2.bias\", \"module.segformer.encoder.block.2.14.mlp.dense1.weight\", \"module.segformer.encoder.block.2.14.mlp.dense1.bias\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.14.mlp.dense2.weight\", \"module.segformer.encoder.block.2.14.mlp.dense2.bias\", \"module.segformer.encoder.block.2.15.layer_norm_1.weight\", \"module.segformer.encoder.block.2.15.layer_norm_1.bias\", \"module.segformer.encoder.block.2.15.attention.self.query.weight\", \"module.segformer.encoder.block.2.15.attention.self.query.bias\", \"module.segformer.encoder.block.2.15.attention.self.key.weight\", \"module.segformer.encoder.block.2.15.attention.self.key.bias\", \"module.segformer.encoder.block.2.15.attention.self.value.weight\", \"module.segformer.encoder.block.2.15.attention.self.value.bias\", \"module.segformer.encoder.block.2.15.attention.self.sr.weight\", \"module.segformer.encoder.block.2.15.attention.self.sr.bias\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.15.attention.output.dense.weight\", \"module.segformer.encoder.block.2.15.attention.output.dense.bias\", \"module.segformer.encoder.block.2.15.layer_norm_2.weight\", \"module.segformer.encoder.block.2.15.layer_norm_2.bias\", \"module.segformer.encoder.block.2.15.mlp.dense1.weight\", \"module.segformer.encoder.block.2.15.mlp.dense1.bias\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.15.mlp.dense2.weight\", \"module.segformer.encoder.block.2.15.mlp.dense2.bias\", \"module.segformer.encoder.block.2.16.layer_norm_1.weight\", \"module.segformer.encoder.block.2.16.layer_norm_1.bias\", \"module.segformer.encoder.block.2.16.attention.self.query.weight\", \"module.segformer.encoder.block.2.16.attention.self.query.bias\", \"module.segformer.encoder.block.2.16.attention.self.key.weight\", \"module.segformer.encoder.block.2.16.attention.self.key.bias\", \"module.segformer.encoder.block.2.16.attention.self.value.weight\", \"module.segformer.encoder.block.2.16.attention.self.value.bias\", \"module.segformer.encoder.block.2.16.attention.self.sr.weight\", \"module.segformer.encoder.block.2.16.attention.self.sr.bias\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.16.attention.output.dense.weight\", \"module.segformer.encoder.block.2.16.attention.output.dense.bias\", \"module.segformer.encoder.block.2.16.layer_norm_2.weight\", \"module.segformer.encoder.block.2.16.layer_norm_2.bias\", \"module.segformer.encoder.block.2.16.mlp.dense1.weight\", \"module.segformer.encoder.block.2.16.mlp.dense1.bias\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.16.mlp.dense2.weight\", \"module.segformer.encoder.block.2.16.mlp.dense2.bias\", \"module.segformer.encoder.block.2.17.layer_norm_1.weight\", \"module.segformer.encoder.block.2.17.layer_norm_1.bias\", \"module.segformer.encoder.block.2.17.attention.self.query.weight\", \"module.segformer.encoder.block.2.17.attention.self.query.bias\", \"module.segformer.encoder.block.2.17.attention.self.key.weight\", \"module.segformer.encoder.block.2.17.attention.self.key.bias\", \"module.segformer.encoder.block.2.17.attention.self.value.weight\", \"module.segformer.encoder.block.2.17.attention.self.value.bias\", \"module.segformer.encoder.block.2.17.attention.self.sr.weight\", \"module.segformer.encoder.block.2.17.attention.self.sr.bias\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.17.attention.output.dense.weight\", \"module.segformer.encoder.block.2.17.attention.output.dense.bias\", \"module.segformer.encoder.block.2.17.layer_norm_2.weight\", \"module.segformer.encoder.block.2.17.layer_norm_2.bias\", \"module.segformer.encoder.block.2.17.mlp.dense1.weight\", \"module.segformer.encoder.block.2.17.mlp.dense1.bias\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.17.mlp.dense2.weight\", \"module.segformer.encoder.block.2.17.mlp.dense2.bias\", \"module.segformer.encoder.block.3.0.layer_norm_1.weight\", \"module.segformer.encoder.block.3.0.layer_norm_1.bias\", \"module.segformer.encoder.block.3.0.attention.self.query.weight\", \"module.segformer.encoder.block.3.0.attention.self.query.bias\", \"module.segformer.encoder.block.3.0.attention.self.key.weight\", \"module.segformer.encoder.block.3.0.attention.self.key.bias\", \"module.segformer.encoder.block.3.0.attention.self.value.weight\", \"module.segformer.encoder.block.3.0.attention.self.value.bias\", \"module.segformer.encoder.block.3.0.attention.output.dense.weight\", \"module.segformer.encoder.block.3.0.attention.output.dense.bias\", \"module.segformer.encoder.block.3.0.layer_norm_2.weight\", \"module.segformer.encoder.block.3.0.layer_norm_2.bias\", \"module.segformer.encoder.block.3.0.mlp.dense1.weight\", \"module.segformer.encoder.block.3.0.mlp.dense1.bias\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.0.mlp.dense2.weight\", \"module.segformer.encoder.block.3.0.mlp.dense2.bias\", \"module.segformer.encoder.block.3.1.layer_norm_1.weight\", \"module.segformer.encoder.block.3.1.layer_norm_1.bias\", \"module.segformer.encoder.block.3.1.attention.self.query.weight\", \"module.segformer.encoder.block.3.1.attention.self.query.bias\", \"module.segformer.encoder.block.3.1.attention.self.key.weight\", \"module.segformer.encoder.block.3.1.attention.self.key.bias\", \"module.segformer.encoder.block.3.1.attention.self.value.weight\", \"module.segformer.encoder.block.3.1.attention.self.value.bias\", \"module.segformer.encoder.block.3.1.attention.output.dense.weight\", \"module.segformer.encoder.block.3.1.attention.output.dense.bias\", \"module.segformer.encoder.block.3.1.layer_norm_2.weight\", \"module.segformer.encoder.block.3.1.layer_norm_2.bias\", \"module.segformer.encoder.block.3.1.mlp.dense1.weight\", \"module.segformer.encoder.block.3.1.mlp.dense1.bias\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.1.mlp.dense2.weight\", \"module.segformer.encoder.block.3.1.mlp.dense2.bias\", \"module.segformer.encoder.block.3.2.layer_norm_1.weight\", \"module.segformer.encoder.block.3.2.layer_norm_1.bias\", \"module.segformer.encoder.block.3.2.attention.self.query.weight\", \"module.segformer.encoder.block.3.2.attention.self.query.bias\", \"module.segformer.encoder.block.3.2.attention.self.key.weight\", \"module.segformer.encoder.block.3.2.attention.self.key.bias\", \"module.segformer.encoder.block.3.2.attention.self.value.weight\", \"module.segformer.encoder.block.3.2.attention.self.value.bias\", \"module.segformer.encoder.block.3.2.attention.output.dense.weight\", \"module.segformer.encoder.block.3.2.attention.output.dense.bias\", \"module.segformer.encoder.block.3.2.layer_norm_2.weight\", \"module.segformer.encoder.block.3.2.layer_norm_2.bias\", \"module.segformer.encoder.block.3.2.mlp.dense1.weight\", \"module.segformer.encoder.block.3.2.mlp.dense1.bias\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.2.mlp.dense2.weight\", \"module.segformer.encoder.block.3.2.mlp.dense2.bias\", \"module.segformer.encoder.layer_norm.0.weight\", \"module.segformer.encoder.layer_norm.0.bias\", \"module.segformer.encoder.layer_norm.1.weight\", \"module.segformer.encoder.layer_norm.1.bias\", \"module.segformer.encoder.layer_norm.2.weight\", \"module.segformer.encoder.layer_norm.2.bias\", \"module.segformer.encoder.layer_norm.3.weight\", \"module.segformer.encoder.layer_norm.3.bias\", \"module.decode_head.linear_c.0.proj.weight\", \"module.decode_head.linear_c.0.proj.bias\", \"module.decode_head.linear_c.1.proj.weight\", \"module.decode_head.linear_c.1.proj.bias\", \"module.decode_head.linear_c.2.proj.weight\", \"module.decode_head.linear_c.2.proj.bias\", \"module.decode_head.linear_c.3.proj.weight\", \"module.decode_head.linear_c.3.proj.bias\", \"module.decode_head.linear_fuse.weight\", \"module.decode_head.batch_norm.weight\", \"module.decode_head.batch_norm.bias\", \"module.decode_head.batch_norm.running_mean\", \"module.decode_head.batch_norm.running_var\", \"module.decode_head.batch_norm.num_batches_tracked\", \"module.decode_head.classifier.weight\", \"module.decode_head.classifier.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Load the best checkpoint\u001b[39;00m\n\u001b[1;32m     54\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create resize transform\u001b[39;00m\n\u001b[1;32m     58\u001b[0m resize_transform \u001b[38;5;241m=\u001b[39m Resize(\n\u001b[1;32m     59\u001b[0m     size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m48\u001b[39m),\n\u001b[1;32m     60\u001b[0m     interpolation\u001b[38;5;241m=\u001b[39mInterpolationMode\u001b[38;5;241m.\u001b[39mBILINEAR,\n\u001b[1;32m     61\u001b[0m     antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     62\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SegformerForSemanticSegmentation:\n\tMissing key(s) in state_dict: \"segformer.encoder.patch_embeddings.0.proj.weight\", \"segformer.encoder.patch_embeddings.0.proj.bias\", \"segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"segformer.encoder.patch_embeddings.1.proj.weight\", \"segformer.encoder.patch_embeddings.1.proj.bias\", \"segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"segformer.encoder.patch_embeddings.2.proj.weight\", \"segformer.encoder.patch_embeddings.2.proj.bias\", \"segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"segformer.encoder.patch_embeddings.3.proj.weight\", \"segformer.encoder.patch_embeddings.3.proj.bias\", \"segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"segformer.encoder.block.0.0.layer_norm_1.weight\", \"segformer.encoder.block.0.0.layer_norm_1.bias\", \"segformer.encoder.block.0.0.attention.self.query.weight\", \"segformer.encoder.block.0.0.attention.self.query.bias\", \"segformer.encoder.block.0.0.attention.self.key.weight\", \"segformer.encoder.block.0.0.attention.self.key.bias\", \"segformer.encoder.block.0.0.attention.self.value.weight\", \"segformer.encoder.block.0.0.attention.self.value.bias\", \"segformer.encoder.block.0.0.attention.self.sr.weight\", \"segformer.encoder.block.0.0.attention.self.sr.bias\", \"segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.0.attention.output.dense.weight\", \"segformer.encoder.block.0.0.attention.output.dense.bias\", \"segformer.encoder.block.0.0.layer_norm_2.weight\", \"segformer.encoder.block.0.0.layer_norm_2.bias\", \"segformer.encoder.block.0.0.mlp.dense1.weight\", \"segformer.encoder.block.0.0.mlp.dense1.bias\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.0.mlp.dense2.weight\", \"segformer.encoder.block.0.0.mlp.dense2.bias\", \"segformer.encoder.block.0.1.layer_norm_1.weight\", \"segformer.encoder.block.0.1.layer_norm_1.bias\", \"segformer.encoder.block.0.1.attention.self.query.weight\", \"segformer.encoder.block.0.1.attention.self.query.bias\", \"segformer.encoder.block.0.1.attention.self.key.weight\", \"segformer.encoder.block.0.1.attention.self.key.bias\", \"segformer.encoder.block.0.1.attention.self.value.weight\", \"segformer.encoder.block.0.1.attention.self.value.bias\", \"segformer.encoder.block.0.1.attention.self.sr.weight\", \"segformer.encoder.block.0.1.attention.self.sr.bias\", \"segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.1.attention.output.dense.weight\", \"segformer.encoder.block.0.1.attention.output.dense.bias\", \"segformer.encoder.block.0.1.layer_norm_2.weight\", \"segformer.encoder.block.0.1.layer_norm_2.bias\", \"segformer.encoder.block.0.1.mlp.dense1.weight\", \"segformer.encoder.block.0.1.mlp.dense1.bias\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.1.mlp.dense2.weight\", \"segformer.encoder.block.0.1.mlp.dense2.bias\", \"segformer.encoder.block.0.2.layer_norm_1.weight\", \"segformer.encoder.block.0.2.layer_norm_1.bias\", \"segformer.encoder.block.0.2.attention.self.query.weight\", \"segformer.encoder.block.0.2.attention.self.query.bias\", \"segformer.encoder.block.0.2.attention.self.key.weight\", \"segformer.encoder.block.0.2.attention.self.key.bias\", \"segformer.encoder.block.0.2.attention.self.value.weight\", \"segformer.encoder.block.0.2.attention.self.value.bias\", \"segformer.encoder.block.0.2.attention.self.sr.weight\", \"segformer.encoder.block.0.2.attention.self.sr.bias\", \"segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.0.2.attention.output.dense.weight\", \"segformer.encoder.block.0.2.attention.output.dense.bias\", \"segformer.encoder.block.0.2.layer_norm_2.weight\", \"segformer.encoder.block.0.2.layer_norm_2.bias\", \"segformer.encoder.block.0.2.mlp.dense1.weight\", \"segformer.encoder.block.0.2.mlp.dense1.bias\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.0.2.mlp.dense2.weight\", \"segformer.encoder.block.0.2.mlp.dense2.bias\", \"segformer.encoder.block.1.0.layer_norm_1.weight\", \"segformer.encoder.block.1.0.layer_norm_1.bias\", \"segformer.encoder.block.1.0.attention.self.query.weight\", \"segformer.encoder.block.1.0.attention.self.query.bias\", \"segformer.encoder.block.1.0.attention.self.key.weight\", \"segformer.encoder.block.1.0.attention.self.key.bias\", \"segformer.encoder.block.1.0.attention.self.value.weight\", \"segformer.encoder.block.1.0.attention.self.value.bias\", \"segformer.encoder.block.1.0.attention.self.sr.weight\", \"segformer.encoder.block.1.0.attention.self.sr.bias\", \"segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.0.attention.output.dense.weight\", \"segformer.encoder.block.1.0.attention.output.dense.bias\", \"segformer.encoder.block.1.0.layer_norm_2.weight\", \"segformer.encoder.block.1.0.layer_norm_2.bias\", \"segformer.encoder.block.1.0.mlp.dense1.weight\", \"segformer.encoder.block.1.0.mlp.dense1.bias\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.0.mlp.dense2.weight\", \"segformer.encoder.block.1.0.mlp.dense2.bias\", \"segformer.encoder.block.1.1.layer_norm_1.weight\", \"segformer.encoder.block.1.1.layer_norm_1.bias\", \"segformer.encoder.block.1.1.attention.self.query.weight\", \"segformer.encoder.block.1.1.attention.self.query.bias\", \"segformer.encoder.block.1.1.attention.self.key.weight\", \"segformer.encoder.block.1.1.attention.self.key.bias\", \"segformer.encoder.block.1.1.attention.self.value.weight\", \"segformer.encoder.block.1.1.attention.self.value.bias\", \"segformer.encoder.block.1.1.attention.self.sr.weight\", \"segformer.encoder.block.1.1.attention.self.sr.bias\", \"segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.1.attention.output.dense.weight\", \"segformer.encoder.block.1.1.attention.output.dense.bias\", \"segformer.encoder.block.1.1.layer_norm_2.weight\", \"segformer.encoder.block.1.1.layer_norm_2.bias\", \"segformer.encoder.block.1.1.mlp.dense1.weight\", \"segformer.encoder.block.1.1.mlp.dense1.bias\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.1.mlp.dense2.weight\", \"segformer.encoder.block.1.1.mlp.dense2.bias\", \"segformer.encoder.block.1.2.layer_norm_1.weight\", \"segformer.encoder.block.1.2.layer_norm_1.bias\", \"segformer.encoder.block.1.2.attention.self.query.weight\", \"segformer.encoder.block.1.2.attention.self.query.bias\", \"segformer.encoder.block.1.2.attention.self.key.weight\", \"segformer.encoder.block.1.2.attention.self.key.bias\", \"segformer.encoder.block.1.2.attention.self.value.weight\", \"segformer.encoder.block.1.2.attention.self.value.bias\", \"segformer.encoder.block.1.2.attention.self.sr.weight\", \"segformer.encoder.block.1.2.attention.self.sr.bias\", \"segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.2.attention.output.dense.weight\", \"segformer.encoder.block.1.2.attention.output.dense.bias\", \"segformer.encoder.block.1.2.layer_norm_2.weight\", \"segformer.encoder.block.1.2.layer_norm_2.bias\", \"segformer.encoder.block.1.2.mlp.dense1.weight\", \"segformer.encoder.block.1.2.mlp.dense1.bias\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.2.mlp.dense2.weight\", \"segformer.encoder.block.1.2.mlp.dense2.bias\", \"segformer.encoder.block.1.3.layer_norm_1.weight\", \"segformer.encoder.block.1.3.layer_norm_1.bias\", \"segformer.encoder.block.1.3.attention.self.query.weight\", \"segformer.encoder.block.1.3.attention.self.query.bias\", \"segformer.encoder.block.1.3.attention.self.key.weight\", \"segformer.encoder.block.1.3.attention.self.key.bias\", \"segformer.encoder.block.1.3.attention.self.value.weight\", \"segformer.encoder.block.1.3.attention.self.value.bias\", \"segformer.encoder.block.1.3.attention.self.sr.weight\", \"segformer.encoder.block.1.3.attention.self.sr.bias\", \"segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.1.3.attention.output.dense.weight\", \"segformer.encoder.block.1.3.attention.output.dense.bias\", \"segformer.encoder.block.1.3.layer_norm_2.weight\", \"segformer.encoder.block.1.3.layer_norm_2.bias\", \"segformer.encoder.block.1.3.mlp.dense1.weight\", \"segformer.encoder.block.1.3.mlp.dense1.bias\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.1.3.mlp.dense2.weight\", \"segformer.encoder.block.1.3.mlp.dense2.bias\", \"segformer.encoder.block.2.0.layer_norm_1.weight\", \"segformer.encoder.block.2.0.layer_norm_1.bias\", \"segformer.encoder.block.2.0.attention.self.query.weight\", \"segformer.encoder.block.2.0.attention.self.query.bias\", \"segformer.encoder.block.2.0.attention.self.key.weight\", \"segformer.encoder.block.2.0.attention.self.key.bias\", \"segformer.encoder.block.2.0.attention.self.value.weight\", \"segformer.encoder.block.2.0.attention.self.value.bias\", \"segformer.encoder.block.2.0.attention.self.sr.weight\", \"segformer.encoder.block.2.0.attention.self.sr.bias\", \"segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.0.attention.output.dense.weight\", \"segformer.encoder.block.2.0.attention.output.dense.bias\", \"segformer.encoder.block.2.0.layer_norm_2.weight\", \"segformer.encoder.block.2.0.layer_norm_2.bias\", \"segformer.encoder.block.2.0.mlp.dense1.weight\", \"segformer.encoder.block.2.0.mlp.dense1.bias\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.0.mlp.dense2.weight\", \"segformer.encoder.block.2.0.mlp.dense2.bias\", \"segformer.encoder.block.2.1.layer_norm_1.weight\", \"segformer.encoder.block.2.1.layer_norm_1.bias\", \"segformer.encoder.block.2.1.attention.self.query.weight\", \"segformer.encoder.block.2.1.attention.self.query.bias\", \"segformer.encoder.block.2.1.attention.self.key.weight\", \"segformer.encoder.block.2.1.attention.self.key.bias\", \"segformer.encoder.block.2.1.attention.self.value.weight\", \"segformer.encoder.block.2.1.attention.self.value.bias\", \"segformer.encoder.block.2.1.attention.self.sr.weight\", \"segformer.encoder.block.2.1.attention.self.sr.bias\", \"segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.1.attention.output.dense.weight\", \"segformer.encoder.block.2.1.attention.output.dense.bias\", \"segformer.encoder.block.2.1.layer_norm_2.weight\", \"segformer.encoder.block.2.1.layer_norm_2.bias\", \"segformer.encoder.block.2.1.mlp.dense1.weight\", \"segformer.encoder.block.2.1.mlp.dense1.bias\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.1.mlp.dense2.weight\", \"segformer.encoder.block.2.1.mlp.dense2.bias\", \"segformer.encoder.block.2.2.layer_norm_1.weight\", \"segformer.encoder.block.2.2.layer_norm_1.bias\", \"segformer.encoder.block.2.2.attention.self.query.weight\", \"segformer.encoder.block.2.2.attention.self.query.bias\", \"segformer.encoder.block.2.2.attention.self.key.weight\", \"segformer.encoder.block.2.2.attention.self.key.bias\", \"segformer.encoder.block.2.2.attention.self.value.weight\", \"segformer.encoder.block.2.2.attention.self.value.bias\", \"segformer.encoder.block.2.2.attention.self.sr.weight\", \"segformer.encoder.block.2.2.attention.self.sr.bias\", \"segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.2.attention.output.dense.weight\", \"segformer.encoder.block.2.2.attention.output.dense.bias\", \"segformer.encoder.block.2.2.layer_norm_2.weight\", \"segformer.encoder.block.2.2.layer_norm_2.bias\", \"segformer.encoder.block.2.2.mlp.dense1.weight\", \"segformer.encoder.block.2.2.mlp.dense1.bias\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.2.mlp.dense2.weight\", \"segformer.encoder.block.2.2.mlp.dense2.bias\", \"segformer.encoder.block.2.3.layer_norm_1.weight\", \"segformer.encoder.block.2.3.layer_norm_1.bias\", \"segformer.encoder.block.2.3.attention.self.query.weight\", \"segformer.encoder.block.2.3.attention.self.query.bias\", \"segformer.encoder.block.2.3.attention.self.key.weight\", \"segformer.encoder.block.2.3.attention.self.key.bias\", \"segformer.encoder.block.2.3.attention.self.value.weight\", \"segformer.encoder.block.2.3.attention.self.value.bias\", \"segformer.encoder.block.2.3.attention.self.sr.weight\", \"segformer.encoder.block.2.3.attention.self.sr.bias\", \"segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.3.attention.output.dense.weight\", \"segformer.encoder.block.2.3.attention.output.dense.bias\", \"segformer.encoder.block.2.3.layer_norm_2.weight\", \"segformer.encoder.block.2.3.layer_norm_2.bias\", \"segformer.encoder.block.2.3.mlp.dense1.weight\", \"segformer.encoder.block.2.3.mlp.dense1.bias\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.3.mlp.dense2.weight\", \"segformer.encoder.block.2.3.mlp.dense2.bias\", \"segformer.encoder.block.2.4.layer_norm_1.weight\", \"segformer.encoder.block.2.4.layer_norm_1.bias\", \"segformer.encoder.block.2.4.attention.self.query.weight\", \"segformer.encoder.block.2.4.attention.self.query.bias\", \"segformer.encoder.block.2.4.attention.self.key.weight\", \"segformer.encoder.block.2.4.attention.self.key.bias\", \"segformer.encoder.block.2.4.attention.self.value.weight\", \"segformer.encoder.block.2.4.attention.self.value.bias\", \"segformer.encoder.block.2.4.attention.self.sr.weight\", \"segformer.encoder.block.2.4.attention.self.sr.bias\", \"segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.4.attention.output.dense.weight\", \"segformer.encoder.block.2.4.attention.output.dense.bias\", \"segformer.encoder.block.2.4.layer_norm_2.weight\", \"segformer.encoder.block.2.4.layer_norm_2.bias\", \"segformer.encoder.block.2.4.mlp.dense1.weight\", \"segformer.encoder.block.2.4.mlp.dense1.bias\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.4.mlp.dense2.weight\", \"segformer.encoder.block.2.4.mlp.dense2.bias\", \"segformer.encoder.block.2.5.layer_norm_1.weight\", \"segformer.encoder.block.2.5.layer_norm_1.bias\", \"segformer.encoder.block.2.5.attention.self.query.weight\", \"segformer.encoder.block.2.5.attention.self.query.bias\", \"segformer.encoder.block.2.5.attention.self.key.weight\", \"segformer.encoder.block.2.5.attention.self.key.bias\", \"segformer.encoder.block.2.5.attention.self.value.weight\", \"segformer.encoder.block.2.5.attention.self.value.bias\", \"segformer.encoder.block.2.5.attention.self.sr.weight\", \"segformer.encoder.block.2.5.attention.self.sr.bias\", \"segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.5.attention.output.dense.weight\", \"segformer.encoder.block.2.5.attention.output.dense.bias\", \"segformer.encoder.block.2.5.layer_norm_2.weight\", \"segformer.encoder.block.2.5.layer_norm_2.bias\", \"segformer.encoder.block.2.5.mlp.dense1.weight\", \"segformer.encoder.block.2.5.mlp.dense1.bias\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.5.mlp.dense2.weight\", \"segformer.encoder.block.2.5.mlp.dense2.bias\", \"segformer.encoder.block.2.6.layer_norm_1.weight\", \"segformer.encoder.block.2.6.layer_norm_1.bias\", \"segformer.encoder.block.2.6.attention.self.query.weight\", \"segformer.encoder.block.2.6.attention.self.query.bias\", \"segformer.encoder.block.2.6.attention.self.key.weight\", \"segformer.encoder.block.2.6.attention.self.key.bias\", \"segformer.encoder.block.2.6.attention.self.value.weight\", \"segformer.encoder.block.2.6.attention.self.value.bias\", \"segformer.encoder.block.2.6.attention.self.sr.weight\", \"segformer.encoder.block.2.6.attention.self.sr.bias\", \"segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.6.attention.output.dense.weight\", \"segformer.encoder.block.2.6.attention.output.dense.bias\", \"segformer.encoder.block.2.6.layer_norm_2.weight\", \"segformer.encoder.block.2.6.layer_norm_2.bias\", \"segformer.encoder.block.2.6.mlp.dense1.weight\", \"segformer.encoder.block.2.6.mlp.dense1.bias\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.6.mlp.dense2.weight\", \"segformer.encoder.block.2.6.mlp.dense2.bias\", \"segformer.encoder.block.2.7.layer_norm_1.weight\", \"segformer.encoder.block.2.7.layer_norm_1.bias\", \"segformer.encoder.block.2.7.attention.self.query.weight\", \"segformer.encoder.block.2.7.attention.self.query.bias\", \"segformer.encoder.block.2.7.attention.self.key.weight\", \"segformer.encoder.block.2.7.attention.self.key.bias\", \"segformer.encoder.block.2.7.attention.self.value.weight\", \"segformer.encoder.block.2.7.attention.self.value.bias\", \"segformer.encoder.block.2.7.attention.self.sr.weight\", \"segformer.encoder.block.2.7.attention.self.sr.bias\", \"segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.7.attention.output.dense.weight\", \"segformer.encoder.block.2.7.attention.output.dense.bias\", \"segformer.encoder.block.2.7.layer_norm_2.weight\", \"segformer.encoder.block.2.7.layer_norm_2.bias\", \"segformer.encoder.block.2.7.mlp.dense1.weight\", \"segformer.encoder.block.2.7.mlp.dense1.bias\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.7.mlp.dense2.weight\", \"segformer.encoder.block.2.7.mlp.dense2.bias\", \"segformer.encoder.block.2.8.layer_norm_1.weight\", \"segformer.encoder.block.2.8.layer_norm_1.bias\", \"segformer.encoder.block.2.8.attention.self.query.weight\", \"segformer.encoder.block.2.8.attention.self.query.bias\", \"segformer.encoder.block.2.8.attention.self.key.weight\", \"segformer.encoder.block.2.8.attention.self.key.bias\", \"segformer.encoder.block.2.8.attention.self.value.weight\", \"segformer.encoder.block.2.8.attention.self.value.bias\", \"segformer.encoder.block.2.8.attention.self.sr.weight\", \"segformer.encoder.block.2.8.attention.self.sr.bias\", \"segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.8.attention.output.dense.weight\", \"segformer.encoder.block.2.8.attention.output.dense.bias\", \"segformer.encoder.block.2.8.layer_norm_2.weight\", \"segformer.encoder.block.2.8.layer_norm_2.bias\", \"segformer.encoder.block.2.8.mlp.dense1.weight\", \"segformer.encoder.block.2.8.mlp.dense1.bias\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.8.mlp.dense2.weight\", \"segformer.encoder.block.2.8.mlp.dense2.bias\", \"segformer.encoder.block.2.9.layer_norm_1.weight\", \"segformer.encoder.block.2.9.layer_norm_1.bias\", \"segformer.encoder.block.2.9.attention.self.query.weight\", \"segformer.encoder.block.2.9.attention.self.query.bias\", \"segformer.encoder.block.2.9.attention.self.key.weight\", \"segformer.encoder.block.2.9.attention.self.key.bias\", \"segformer.encoder.block.2.9.attention.self.value.weight\", \"segformer.encoder.block.2.9.attention.self.value.bias\", \"segformer.encoder.block.2.9.attention.self.sr.weight\", \"segformer.encoder.block.2.9.attention.self.sr.bias\", \"segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.9.attention.output.dense.weight\", \"segformer.encoder.block.2.9.attention.output.dense.bias\", \"segformer.encoder.block.2.9.layer_norm_2.weight\", \"segformer.encoder.block.2.9.layer_norm_2.bias\", \"segformer.encoder.block.2.9.mlp.dense1.weight\", \"segformer.encoder.block.2.9.mlp.dense1.bias\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.9.mlp.dense2.weight\", \"segformer.encoder.block.2.9.mlp.dense2.bias\", \"segformer.encoder.block.2.10.layer_norm_1.weight\", \"segformer.encoder.block.2.10.layer_norm_1.bias\", \"segformer.encoder.block.2.10.attention.self.query.weight\", \"segformer.encoder.block.2.10.attention.self.query.bias\", \"segformer.encoder.block.2.10.attention.self.key.weight\", \"segformer.encoder.block.2.10.attention.self.key.bias\", \"segformer.encoder.block.2.10.attention.self.value.weight\", \"segformer.encoder.block.2.10.attention.self.value.bias\", \"segformer.encoder.block.2.10.attention.self.sr.weight\", \"segformer.encoder.block.2.10.attention.self.sr.bias\", \"segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.10.attention.output.dense.weight\", \"segformer.encoder.block.2.10.attention.output.dense.bias\", \"segformer.encoder.block.2.10.layer_norm_2.weight\", \"segformer.encoder.block.2.10.layer_norm_2.bias\", \"segformer.encoder.block.2.10.mlp.dense1.weight\", \"segformer.encoder.block.2.10.mlp.dense1.bias\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.10.mlp.dense2.weight\", \"segformer.encoder.block.2.10.mlp.dense2.bias\", \"segformer.encoder.block.2.11.layer_norm_1.weight\", \"segformer.encoder.block.2.11.layer_norm_1.bias\", \"segformer.encoder.block.2.11.attention.self.query.weight\", \"segformer.encoder.block.2.11.attention.self.query.bias\", \"segformer.encoder.block.2.11.attention.self.key.weight\", \"segformer.encoder.block.2.11.attention.self.key.bias\", \"segformer.encoder.block.2.11.attention.self.value.weight\", \"segformer.encoder.block.2.11.attention.self.value.bias\", \"segformer.encoder.block.2.11.attention.self.sr.weight\", \"segformer.encoder.block.2.11.attention.self.sr.bias\", \"segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.11.attention.output.dense.weight\", \"segformer.encoder.block.2.11.attention.output.dense.bias\", \"segformer.encoder.block.2.11.layer_norm_2.weight\", \"segformer.encoder.block.2.11.layer_norm_2.bias\", \"segformer.encoder.block.2.11.mlp.dense1.weight\", \"segformer.encoder.block.2.11.mlp.dense1.bias\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.11.mlp.dense2.weight\", \"segformer.encoder.block.2.11.mlp.dense2.bias\", \"segformer.encoder.block.2.12.layer_norm_1.weight\", \"segformer.encoder.block.2.12.layer_norm_1.bias\", \"segformer.encoder.block.2.12.attention.self.query.weight\", \"segformer.encoder.block.2.12.attention.self.query.bias\", \"segformer.encoder.block.2.12.attention.self.key.weight\", \"segformer.encoder.block.2.12.attention.self.key.bias\", \"segformer.encoder.block.2.12.attention.self.value.weight\", \"segformer.encoder.block.2.12.attention.self.value.bias\", \"segformer.encoder.block.2.12.attention.self.sr.weight\", \"segformer.encoder.block.2.12.attention.self.sr.bias\", \"segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.12.attention.output.dense.weight\", \"segformer.encoder.block.2.12.attention.output.dense.bias\", \"segformer.encoder.block.2.12.layer_norm_2.weight\", \"segformer.encoder.block.2.12.layer_norm_2.bias\", \"segformer.encoder.block.2.12.mlp.dense1.weight\", \"segformer.encoder.block.2.12.mlp.dense1.bias\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.12.mlp.dense2.weight\", \"segformer.encoder.block.2.12.mlp.dense2.bias\", \"segformer.encoder.block.2.13.layer_norm_1.weight\", \"segformer.encoder.block.2.13.layer_norm_1.bias\", \"segformer.encoder.block.2.13.attention.self.query.weight\", \"segformer.encoder.block.2.13.attention.self.query.bias\", \"segformer.encoder.block.2.13.attention.self.key.weight\", \"segformer.encoder.block.2.13.attention.self.key.bias\", \"segformer.encoder.block.2.13.attention.self.value.weight\", \"segformer.encoder.block.2.13.attention.self.value.bias\", \"segformer.encoder.block.2.13.attention.self.sr.weight\", \"segformer.encoder.block.2.13.attention.self.sr.bias\", \"segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.13.attention.output.dense.weight\", \"segformer.encoder.block.2.13.attention.output.dense.bias\", \"segformer.encoder.block.2.13.layer_norm_2.weight\", \"segformer.encoder.block.2.13.layer_norm_2.bias\", \"segformer.encoder.block.2.13.mlp.dense1.weight\", \"segformer.encoder.block.2.13.mlp.dense1.bias\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.13.mlp.dense2.weight\", \"segformer.encoder.block.2.13.mlp.dense2.bias\", \"segformer.encoder.block.2.14.layer_norm_1.weight\", \"segformer.encoder.block.2.14.layer_norm_1.bias\", \"segformer.encoder.block.2.14.attention.self.query.weight\", \"segformer.encoder.block.2.14.attention.self.query.bias\", \"segformer.encoder.block.2.14.attention.self.key.weight\", \"segformer.encoder.block.2.14.attention.self.key.bias\", \"segformer.encoder.block.2.14.attention.self.value.weight\", \"segformer.encoder.block.2.14.attention.self.value.bias\", \"segformer.encoder.block.2.14.attention.self.sr.weight\", \"segformer.encoder.block.2.14.attention.self.sr.bias\", \"segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.14.attention.output.dense.weight\", \"segformer.encoder.block.2.14.attention.output.dense.bias\", \"segformer.encoder.block.2.14.layer_norm_2.weight\", \"segformer.encoder.block.2.14.layer_norm_2.bias\", \"segformer.encoder.block.2.14.mlp.dense1.weight\", \"segformer.encoder.block.2.14.mlp.dense1.bias\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.14.mlp.dense2.weight\", \"segformer.encoder.block.2.14.mlp.dense2.bias\", \"segformer.encoder.block.2.15.layer_norm_1.weight\", \"segformer.encoder.block.2.15.layer_norm_1.bias\", \"segformer.encoder.block.2.15.attention.self.query.weight\", \"segformer.encoder.block.2.15.attention.self.query.bias\", \"segformer.encoder.block.2.15.attention.self.key.weight\", \"segformer.encoder.block.2.15.attention.self.key.bias\", \"segformer.encoder.block.2.15.attention.self.value.weight\", \"segformer.encoder.block.2.15.attention.self.value.bias\", \"segformer.encoder.block.2.15.attention.self.sr.weight\", \"segformer.encoder.block.2.15.attention.self.sr.bias\", \"segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.15.attention.output.dense.weight\", \"segformer.encoder.block.2.15.attention.output.dense.bias\", \"segformer.encoder.block.2.15.layer_norm_2.weight\", \"segformer.encoder.block.2.15.layer_norm_2.bias\", \"segformer.encoder.block.2.15.mlp.dense1.weight\", \"segformer.encoder.block.2.15.mlp.dense1.bias\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.15.mlp.dense2.weight\", \"segformer.encoder.block.2.15.mlp.dense2.bias\", \"segformer.encoder.block.2.16.layer_norm_1.weight\", \"segformer.encoder.block.2.16.layer_norm_1.bias\", \"segformer.encoder.block.2.16.attention.self.query.weight\", \"segformer.encoder.block.2.16.attention.self.query.bias\", \"segformer.encoder.block.2.16.attention.self.key.weight\", \"segformer.encoder.block.2.16.attention.self.key.bias\", \"segformer.encoder.block.2.16.attention.self.value.weight\", \"segformer.encoder.block.2.16.attention.self.value.bias\", \"segformer.encoder.block.2.16.attention.self.sr.weight\", \"segformer.encoder.block.2.16.attention.self.sr.bias\", \"segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.16.attention.output.dense.weight\", \"segformer.encoder.block.2.16.attention.output.dense.bias\", \"segformer.encoder.block.2.16.layer_norm_2.weight\", \"segformer.encoder.block.2.16.layer_norm_2.bias\", \"segformer.encoder.block.2.16.mlp.dense1.weight\", \"segformer.encoder.block.2.16.mlp.dense1.bias\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.16.mlp.dense2.weight\", \"segformer.encoder.block.2.16.mlp.dense2.bias\", \"segformer.encoder.block.2.17.layer_norm_1.weight\", \"segformer.encoder.block.2.17.layer_norm_1.bias\", \"segformer.encoder.block.2.17.attention.self.query.weight\", \"segformer.encoder.block.2.17.attention.self.query.bias\", \"segformer.encoder.block.2.17.attention.self.key.weight\", \"segformer.encoder.block.2.17.attention.self.key.bias\", \"segformer.encoder.block.2.17.attention.self.value.weight\", \"segformer.encoder.block.2.17.attention.self.value.bias\", \"segformer.encoder.block.2.17.attention.self.sr.weight\", \"segformer.encoder.block.2.17.attention.self.sr.bias\", \"segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"segformer.encoder.block.2.17.attention.output.dense.weight\", \"segformer.encoder.block.2.17.attention.output.dense.bias\", \"segformer.encoder.block.2.17.layer_norm_2.weight\", \"segformer.encoder.block.2.17.layer_norm_2.bias\", \"segformer.encoder.block.2.17.mlp.dense1.weight\", \"segformer.encoder.block.2.17.mlp.dense1.bias\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.2.17.mlp.dense2.weight\", \"segformer.encoder.block.2.17.mlp.dense2.bias\", \"segformer.encoder.block.3.0.layer_norm_1.weight\", \"segformer.encoder.block.3.0.layer_norm_1.bias\", \"segformer.encoder.block.3.0.attention.self.query.weight\", \"segformer.encoder.block.3.0.attention.self.query.bias\", \"segformer.encoder.block.3.0.attention.self.key.weight\", \"segformer.encoder.block.3.0.attention.self.key.bias\", \"segformer.encoder.block.3.0.attention.self.value.weight\", \"segformer.encoder.block.3.0.attention.self.value.bias\", \"segformer.encoder.block.3.0.attention.output.dense.weight\", \"segformer.encoder.block.3.0.attention.output.dense.bias\", \"segformer.encoder.block.3.0.layer_norm_2.weight\", \"segformer.encoder.block.3.0.layer_norm_2.bias\", \"segformer.encoder.block.3.0.mlp.dense1.weight\", \"segformer.encoder.block.3.0.mlp.dense1.bias\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.0.mlp.dense2.weight\", \"segformer.encoder.block.3.0.mlp.dense2.bias\", \"segformer.encoder.block.3.1.layer_norm_1.weight\", \"segformer.encoder.block.3.1.layer_norm_1.bias\", \"segformer.encoder.block.3.1.attention.self.query.weight\", \"segformer.encoder.block.3.1.attention.self.query.bias\", \"segformer.encoder.block.3.1.attention.self.key.weight\", \"segformer.encoder.block.3.1.attention.self.key.bias\", \"segformer.encoder.block.3.1.attention.self.value.weight\", \"segformer.encoder.block.3.1.attention.self.value.bias\", \"segformer.encoder.block.3.1.attention.output.dense.weight\", \"segformer.encoder.block.3.1.attention.output.dense.bias\", \"segformer.encoder.block.3.1.layer_norm_2.weight\", \"segformer.encoder.block.3.1.layer_norm_2.bias\", \"segformer.encoder.block.3.1.mlp.dense1.weight\", \"segformer.encoder.block.3.1.mlp.dense1.bias\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.1.mlp.dense2.weight\", \"segformer.encoder.block.3.1.mlp.dense2.bias\", \"segformer.encoder.block.3.2.layer_norm_1.weight\", \"segformer.encoder.block.3.2.layer_norm_1.bias\", \"segformer.encoder.block.3.2.attention.self.query.weight\", \"segformer.encoder.block.3.2.attention.self.query.bias\", \"segformer.encoder.block.3.2.attention.self.key.weight\", \"segformer.encoder.block.3.2.attention.self.key.bias\", \"segformer.encoder.block.3.2.attention.self.value.weight\", \"segformer.encoder.block.3.2.attention.self.value.bias\", \"segformer.encoder.block.3.2.attention.output.dense.weight\", \"segformer.encoder.block.3.2.attention.output.dense.bias\", \"segformer.encoder.block.3.2.layer_norm_2.weight\", \"segformer.encoder.block.3.2.layer_norm_2.bias\", \"segformer.encoder.block.3.2.mlp.dense1.weight\", \"segformer.encoder.block.3.2.mlp.dense1.bias\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"segformer.encoder.block.3.2.mlp.dense2.weight\", \"segformer.encoder.block.3.2.mlp.dense2.bias\", \"segformer.encoder.layer_norm.0.weight\", \"segformer.encoder.layer_norm.0.bias\", \"segformer.encoder.layer_norm.1.weight\", \"segformer.encoder.layer_norm.1.bias\", \"segformer.encoder.layer_norm.2.weight\", \"segformer.encoder.layer_norm.2.bias\", \"segformer.encoder.layer_norm.3.weight\", \"segformer.encoder.layer_norm.3.bias\", \"decode_head.linear_c.0.proj.weight\", \"decode_head.linear_c.0.proj.bias\", \"decode_head.linear_c.1.proj.weight\", \"decode_head.linear_c.1.proj.bias\", \"decode_head.linear_c.2.proj.weight\", \"decode_head.linear_c.2.proj.bias\", \"decode_head.linear_c.3.proj.weight\", \"decode_head.linear_c.3.proj.bias\", \"decode_head.linear_fuse.weight\", \"decode_head.batch_norm.weight\", \"decode_head.batch_norm.bias\", \"decode_head.batch_norm.running_mean\", \"decode_head.batch_norm.running_var\", \"decode_head.classifier.weight\", \"decode_head.classifier.bias\". \n\tUnexpected key(s) in state_dict: \"module.segformer.encoder.patch_embeddings.0.proj.weight\", \"module.segformer.encoder.patch_embeddings.0.proj.bias\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.0.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.1.proj.weight\", \"module.segformer.encoder.patch_embeddings.1.proj.bias\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.1.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.2.proj.weight\", \"module.segformer.encoder.patch_embeddings.2.proj.bias\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.2.layer_norm.bias\", \"module.segformer.encoder.patch_embeddings.3.proj.weight\", \"module.segformer.encoder.patch_embeddings.3.proj.bias\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.weight\", \"module.segformer.encoder.patch_embeddings.3.layer_norm.bias\", \"module.segformer.encoder.block.0.0.layer_norm_1.weight\", \"module.segformer.encoder.block.0.0.layer_norm_1.bias\", \"module.segformer.encoder.block.0.0.attention.self.query.weight\", \"module.segformer.encoder.block.0.0.attention.self.query.bias\", \"module.segformer.encoder.block.0.0.attention.self.key.weight\", \"module.segformer.encoder.block.0.0.attention.self.key.bias\", \"module.segformer.encoder.block.0.0.attention.self.value.weight\", \"module.segformer.encoder.block.0.0.attention.self.value.bias\", \"module.segformer.encoder.block.0.0.attention.self.sr.weight\", \"module.segformer.encoder.block.0.0.attention.self.sr.bias\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.0.attention.output.dense.weight\", \"module.segformer.encoder.block.0.0.attention.output.dense.bias\", \"module.segformer.encoder.block.0.0.layer_norm_2.weight\", \"module.segformer.encoder.block.0.0.layer_norm_2.bias\", \"module.segformer.encoder.block.0.0.mlp.dense1.weight\", \"module.segformer.encoder.block.0.0.mlp.dense1.bias\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.0.mlp.dense2.weight\", \"module.segformer.encoder.block.0.0.mlp.dense2.bias\", \"module.segformer.encoder.block.0.1.layer_norm_1.weight\", \"module.segformer.encoder.block.0.1.layer_norm_1.bias\", \"module.segformer.encoder.block.0.1.attention.self.query.weight\", \"module.segformer.encoder.block.0.1.attention.self.query.bias\", \"module.segformer.encoder.block.0.1.attention.self.key.weight\", \"module.segformer.encoder.block.0.1.attention.self.key.bias\", \"module.segformer.encoder.block.0.1.attention.self.value.weight\", \"module.segformer.encoder.block.0.1.attention.self.value.bias\", \"module.segformer.encoder.block.0.1.attention.self.sr.weight\", \"module.segformer.encoder.block.0.1.attention.self.sr.bias\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.1.attention.output.dense.weight\", \"module.segformer.encoder.block.0.1.attention.output.dense.bias\", \"module.segformer.encoder.block.0.1.layer_norm_2.weight\", \"module.segformer.encoder.block.0.1.layer_norm_2.bias\", \"module.segformer.encoder.block.0.1.mlp.dense1.weight\", \"module.segformer.encoder.block.0.1.mlp.dense1.bias\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.1.mlp.dense2.weight\", \"module.segformer.encoder.block.0.1.mlp.dense2.bias\", \"module.segformer.encoder.block.0.2.layer_norm_1.weight\", \"module.segformer.encoder.block.0.2.layer_norm_1.bias\", \"module.segformer.encoder.block.0.2.attention.self.query.weight\", \"module.segformer.encoder.block.0.2.attention.self.query.bias\", \"module.segformer.encoder.block.0.2.attention.self.key.weight\", \"module.segformer.encoder.block.0.2.attention.self.key.bias\", \"module.segformer.encoder.block.0.2.attention.self.value.weight\", \"module.segformer.encoder.block.0.2.attention.self.value.bias\", \"module.segformer.encoder.block.0.2.attention.self.sr.weight\", \"module.segformer.encoder.block.0.2.attention.self.sr.bias\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.0.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.0.2.attention.output.dense.weight\", \"module.segformer.encoder.block.0.2.attention.output.dense.bias\", \"module.segformer.encoder.block.0.2.layer_norm_2.weight\", \"module.segformer.encoder.block.0.2.layer_norm_2.bias\", \"module.segformer.encoder.block.0.2.mlp.dense1.weight\", \"module.segformer.encoder.block.0.2.mlp.dense1.bias\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.0.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.0.2.mlp.dense2.weight\", \"module.segformer.encoder.block.0.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.0.layer_norm_1.weight\", \"module.segformer.encoder.block.1.0.layer_norm_1.bias\", \"module.segformer.encoder.block.1.0.attention.self.query.weight\", \"module.segformer.encoder.block.1.0.attention.self.query.bias\", \"module.segformer.encoder.block.1.0.attention.self.key.weight\", \"module.segformer.encoder.block.1.0.attention.self.key.bias\", \"module.segformer.encoder.block.1.0.attention.self.value.weight\", \"module.segformer.encoder.block.1.0.attention.self.value.bias\", \"module.segformer.encoder.block.1.0.attention.self.sr.weight\", \"module.segformer.encoder.block.1.0.attention.self.sr.bias\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.0.attention.output.dense.weight\", \"module.segformer.encoder.block.1.0.attention.output.dense.bias\", \"module.segformer.encoder.block.1.0.layer_norm_2.weight\", \"module.segformer.encoder.block.1.0.layer_norm_2.bias\", \"module.segformer.encoder.block.1.0.mlp.dense1.weight\", \"module.segformer.encoder.block.1.0.mlp.dense1.bias\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.0.mlp.dense2.weight\", \"module.segformer.encoder.block.1.0.mlp.dense2.bias\", \"module.segformer.encoder.block.1.1.layer_norm_1.weight\", \"module.segformer.encoder.block.1.1.layer_norm_1.bias\", \"module.segformer.encoder.block.1.1.attention.self.query.weight\", \"module.segformer.encoder.block.1.1.attention.self.query.bias\", \"module.segformer.encoder.block.1.1.attention.self.key.weight\", \"module.segformer.encoder.block.1.1.attention.self.key.bias\", \"module.segformer.encoder.block.1.1.attention.self.value.weight\", \"module.segformer.encoder.block.1.1.attention.self.value.bias\", \"module.segformer.encoder.block.1.1.attention.self.sr.weight\", \"module.segformer.encoder.block.1.1.attention.self.sr.bias\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.1.attention.output.dense.weight\", \"module.segformer.encoder.block.1.1.attention.output.dense.bias\", \"module.segformer.encoder.block.1.1.layer_norm_2.weight\", \"module.segformer.encoder.block.1.1.layer_norm_2.bias\", \"module.segformer.encoder.block.1.1.mlp.dense1.weight\", \"module.segformer.encoder.block.1.1.mlp.dense1.bias\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.1.mlp.dense2.weight\", \"module.segformer.encoder.block.1.1.mlp.dense2.bias\", \"module.segformer.encoder.block.1.2.layer_norm_1.weight\", \"module.segformer.encoder.block.1.2.layer_norm_1.bias\", \"module.segformer.encoder.block.1.2.attention.self.query.weight\", \"module.segformer.encoder.block.1.2.attention.self.query.bias\", \"module.segformer.encoder.block.1.2.attention.self.key.weight\", \"module.segformer.encoder.block.1.2.attention.self.key.bias\", \"module.segformer.encoder.block.1.2.attention.self.value.weight\", \"module.segformer.encoder.block.1.2.attention.self.value.bias\", \"module.segformer.encoder.block.1.2.attention.self.sr.weight\", \"module.segformer.encoder.block.1.2.attention.self.sr.bias\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.2.attention.output.dense.weight\", \"module.segformer.encoder.block.1.2.attention.output.dense.bias\", \"module.segformer.encoder.block.1.2.layer_norm_2.weight\", \"module.segformer.encoder.block.1.2.layer_norm_2.bias\", \"module.segformer.encoder.block.1.2.mlp.dense1.weight\", \"module.segformer.encoder.block.1.2.mlp.dense1.bias\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.2.mlp.dense2.weight\", \"module.segformer.encoder.block.1.2.mlp.dense2.bias\", \"module.segformer.encoder.block.1.3.layer_norm_1.weight\", \"module.segformer.encoder.block.1.3.layer_norm_1.bias\", \"module.segformer.encoder.block.1.3.attention.self.query.weight\", \"module.segformer.encoder.block.1.3.attention.self.query.bias\", \"module.segformer.encoder.block.1.3.attention.self.key.weight\", \"module.segformer.encoder.block.1.3.attention.self.key.bias\", \"module.segformer.encoder.block.1.3.attention.self.value.weight\", \"module.segformer.encoder.block.1.3.attention.self.value.bias\", \"module.segformer.encoder.block.1.3.attention.self.sr.weight\", \"module.segformer.encoder.block.1.3.attention.self.sr.bias\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.1.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.1.3.attention.output.dense.weight\", \"module.segformer.encoder.block.1.3.attention.output.dense.bias\", \"module.segformer.encoder.block.1.3.layer_norm_2.weight\", \"module.segformer.encoder.block.1.3.layer_norm_2.bias\", \"module.segformer.encoder.block.1.3.mlp.dense1.weight\", \"module.segformer.encoder.block.1.3.mlp.dense1.bias\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.1.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.1.3.mlp.dense2.weight\", \"module.segformer.encoder.block.1.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.0.layer_norm_1.weight\", \"module.segformer.encoder.block.2.0.layer_norm_1.bias\", \"module.segformer.encoder.block.2.0.attention.self.query.weight\", \"module.segformer.encoder.block.2.0.attention.self.query.bias\", \"module.segformer.encoder.block.2.0.attention.self.key.weight\", \"module.segformer.encoder.block.2.0.attention.self.key.bias\", \"module.segformer.encoder.block.2.0.attention.self.value.weight\", \"module.segformer.encoder.block.2.0.attention.self.value.bias\", \"module.segformer.encoder.block.2.0.attention.self.sr.weight\", \"module.segformer.encoder.block.2.0.attention.self.sr.bias\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.0.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.0.attention.output.dense.weight\", \"module.segformer.encoder.block.2.0.attention.output.dense.bias\", \"module.segformer.encoder.block.2.0.layer_norm_2.weight\", \"module.segformer.encoder.block.2.0.layer_norm_2.bias\", \"module.segformer.encoder.block.2.0.mlp.dense1.weight\", \"module.segformer.encoder.block.2.0.mlp.dense1.bias\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.0.mlp.dense2.weight\", \"module.segformer.encoder.block.2.0.mlp.dense2.bias\", \"module.segformer.encoder.block.2.1.layer_norm_1.weight\", \"module.segformer.encoder.block.2.1.layer_norm_1.bias\", \"module.segformer.encoder.block.2.1.attention.self.query.weight\", \"module.segformer.encoder.block.2.1.attention.self.query.bias\", \"module.segformer.encoder.block.2.1.attention.self.key.weight\", \"module.segformer.encoder.block.2.1.attention.self.key.bias\", \"module.segformer.encoder.block.2.1.attention.self.value.weight\", \"module.segformer.encoder.block.2.1.attention.self.value.bias\", \"module.segformer.encoder.block.2.1.attention.self.sr.weight\", \"module.segformer.encoder.block.2.1.attention.self.sr.bias\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.1.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.1.attention.output.dense.weight\", \"module.segformer.encoder.block.2.1.attention.output.dense.bias\", \"module.segformer.encoder.block.2.1.layer_norm_2.weight\", \"module.segformer.encoder.block.2.1.layer_norm_2.bias\", \"module.segformer.encoder.block.2.1.mlp.dense1.weight\", \"module.segformer.encoder.block.2.1.mlp.dense1.bias\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.1.mlp.dense2.weight\", \"module.segformer.encoder.block.2.1.mlp.dense2.bias\", \"module.segformer.encoder.block.2.2.layer_norm_1.weight\", \"module.segformer.encoder.block.2.2.layer_norm_1.bias\", \"module.segformer.encoder.block.2.2.attention.self.query.weight\", \"module.segformer.encoder.block.2.2.attention.self.query.bias\", \"module.segformer.encoder.block.2.2.attention.self.key.weight\", \"module.segformer.encoder.block.2.2.attention.self.key.bias\", \"module.segformer.encoder.block.2.2.attention.self.value.weight\", \"module.segformer.encoder.block.2.2.attention.self.value.bias\", \"module.segformer.encoder.block.2.2.attention.self.sr.weight\", \"module.segformer.encoder.block.2.2.attention.self.sr.bias\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.2.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.2.attention.output.dense.weight\", \"module.segformer.encoder.block.2.2.attention.output.dense.bias\", \"module.segformer.encoder.block.2.2.layer_norm_2.weight\", \"module.segformer.encoder.block.2.2.layer_norm_2.bias\", \"module.segformer.encoder.block.2.2.mlp.dense1.weight\", \"module.segformer.encoder.block.2.2.mlp.dense1.bias\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.2.mlp.dense2.weight\", \"module.segformer.encoder.block.2.2.mlp.dense2.bias\", \"module.segformer.encoder.block.2.3.layer_norm_1.weight\", \"module.segformer.encoder.block.2.3.layer_norm_1.bias\", \"module.segformer.encoder.block.2.3.attention.self.query.weight\", \"module.segformer.encoder.block.2.3.attention.self.query.bias\", \"module.segformer.encoder.block.2.3.attention.self.key.weight\", \"module.segformer.encoder.block.2.3.attention.self.key.bias\", \"module.segformer.encoder.block.2.3.attention.self.value.weight\", \"module.segformer.encoder.block.2.3.attention.self.value.bias\", \"module.segformer.encoder.block.2.3.attention.self.sr.weight\", \"module.segformer.encoder.block.2.3.attention.self.sr.bias\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.3.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.3.attention.output.dense.weight\", \"module.segformer.encoder.block.2.3.attention.output.dense.bias\", \"module.segformer.encoder.block.2.3.layer_norm_2.weight\", \"module.segformer.encoder.block.2.3.layer_norm_2.bias\", \"module.segformer.encoder.block.2.3.mlp.dense1.weight\", \"module.segformer.encoder.block.2.3.mlp.dense1.bias\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.3.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.3.mlp.dense2.weight\", \"module.segformer.encoder.block.2.3.mlp.dense2.bias\", \"module.segformer.encoder.block.2.4.layer_norm_1.weight\", \"module.segformer.encoder.block.2.4.layer_norm_1.bias\", \"module.segformer.encoder.block.2.4.attention.self.query.weight\", \"module.segformer.encoder.block.2.4.attention.self.query.bias\", \"module.segformer.encoder.block.2.4.attention.self.key.weight\", \"module.segformer.encoder.block.2.4.attention.self.key.bias\", \"module.segformer.encoder.block.2.4.attention.self.value.weight\", \"module.segformer.encoder.block.2.4.attention.self.value.bias\", \"module.segformer.encoder.block.2.4.attention.self.sr.weight\", \"module.segformer.encoder.block.2.4.attention.self.sr.bias\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.4.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.4.attention.output.dense.weight\", \"module.segformer.encoder.block.2.4.attention.output.dense.bias\", \"module.segformer.encoder.block.2.4.layer_norm_2.weight\", \"module.segformer.encoder.block.2.4.layer_norm_2.bias\", \"module.segformer.encoder.block.2.4.mlp.dense1.weight\", \"module.segformer.encoder.block.2.4.mlp.dense1.bias\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.4.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.4.mlp.dense2.weight\", \"module.segformer.encoder.block.2.4.mlp.dense2.bias\", \"module.segformer.encoder.block.2.5.layer_norm_1.weight\", \"module.segformer.encoder.block.2.5.layer_norm_1.bias\", \"module.segformer.encoder.block.2.5.attention.self.query.weight\", \"module.segformer.encoder.block.2.5.attention.self.query.bias\", \"module.segformer.encoder.block.2.5.attention.self.key.weight\", \"module.segformer.encoder.block.2.5.attention.self.key.bias\", \"module.segformer.encoder.block.2.5.attention.self.value.weight\", \"module.segformer.encoder.block.2.5.attention.self.value.bias\", \"module.segformer.encoder.block.2.5.attention.self.sr.weight\", \"module.segformer.encoder.block.2.5.attention.self.sr.bias\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.5.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.5.attention.output.dense.weight\", \"module.segformer.encoder.block.2.5.attention.output.dense.bias\", \"module.segformer.encoder.block.2.5.layer_norm_2.weight\", \"module.segformer.encoder.block.2.5.layer_norm_2.bias\", \"module.segformer.encoder.block.2.5.mlp.dense1.weight\", \"module.segformer.encoder.block.2.5.mlp.dense1.bias\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.5.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.5.mlp.dense2.weight\", \"module.segformer.encoder.block.2.5.mlp.dense2.bias\", \"module.segformer.encoder.block.2.6.layer_norm_1.weight\", \"module.segformer.encoder.block.2.6.layer_norm_1.bias\", \"module.segformer.encoder.block.2.6.attention.self.query.weight\", \"module.segformer.encoder.block.2.6.attention.self.query.bias\", \"module.segformer.encoder.block.2.6.attention.self.key.weight\", \"module.segformer.encoder.block.2.6.attention.self.key.bias\", \"module.segformer.encoder.block.2.6.attention.self.value.weight\", \"module.segformer.encoder.block.2.6.attention.self.value.bias\", \"module.segformer.encoder.block.2.6.attention.self.sr.weight\", \"module.segformer.encoder.block.2.6.attention.self.sr.bias\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.6.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.6.attention.output.dense.weight\", \"module.segformer.encoder.block.2.6.attention.output.dense.bias\", \"module.segformer.encoder.block.2.6.layer_norm_2.weight\", \"module.segformer.encoder.block.2.6.layer_norm_2.bias\", \"module.segformer.encoder.block.2.6.mlp.dense1.weight\", \"module.segformer.encoder.block.2.6.mlp.dense1.bias\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.6.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.6.mlp.dense2.weight\", \"module.segformer.encoder.block.2.6.mlp.dense2.bias\", \"module.segformer.encoder.block.2.7.layer_norm_1.weight\", \"module.segformer.encoder.block.2.7.layer_norm_1.bias\", \"module.segformer.encoder.block.2.7.attention.self.query.weight\", \"module.segformer.encoder.block.2.7.attention.self.query.bias\", \"module.segformer.encoder.block.2.7.attention.self.key.weight\", \"module.segformer.encoder.block.2.7.attention.self.key.bias\", \"module.segformer.encoder.block.2.7.attention.self.value.weight\", \"module.segformer.encoder.block.2.7.attention.self.value.bias\", \"module.segformer.encoder.block.2.7.attention.self.sr.weight\", \"module.segformer.encoder.block.2.7.attention.self.sr.bias\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.7.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.7.attention.output.dense.weight\", \"module.segformer.encoder.block.2.7.attention.output.dense.bias\", \"module.segformer.encoder.block.2.7.layer_norm_2.weight\", \"module.segformer.encoder.block.2.7.layer_norm_2.bias\", \"module.segformer.encoder.block.2.7.mlp.dense1.weight\", \"module.segformer.encoder.block.2.7.mlp.dense1.bias\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.7.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.7.mlp.dense2.weight\", \"module.segformer.encoder.block.2.7.mlp.dense2.bias\", \"module.segformer.encoder.block.2.8.layer_norm_1.weight\", \"module.segformer.encoder.block.2.8.layer_norm_1.bias\", \"module.segformer.encoder.block.2.8.attention.self.query.weight\", \"module.segformer.encoder.block.2.8.attention.self.query.bias\", \"module.segformer.encoder.block.2.8.attention.self.key.weight\", \"module.segformer.encoder.block.2.8.attention.self.key.bias\", \"module.segformer.encoder.block.2.8.attention.self.value.weight\", \"module.segformer.encoder.block.2.8.attention.self.value.bias\", \"module.segformer.encoder.block.2.8.attention.self.sr.weight\", \"module.segformer.encoder.block.2.8.attention.self.sr.bias\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.8.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.8.attention.output.dense.weight\", \"module.segformer.encoder.block.2.8.attention.output.dense.bias\", \"module.segformer.encoder.block.2.8.layer_norm_2.weight\", \"module.segformer.encoder.block.2.8.layer_norm_2.bias\", \"module.segformer.encoder.block.2.8.mlp.dense1.weight\", \"module.segformer.encoder.block.2.8.mlp.dense1.bias\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.8.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.8.mlp.dense2.weight\", \"module.segformer.encoder.block.2.8.mlp.dense2.bias\", \"module.segformer.encoder.block.2.9.layer_norm_1.weight\", \"module.segformer.encoder.block.2.9.layer_norm_1.bias\", \"module.segformer.encoder.block.2.9.attention.self.query.weight\", \"module.segformer.encoder.block.2.9.attention.self.query.bias\", \"module.segformer.encoder.block.2.9.attention.self.key.weight\", \"module.segformer.encoder.block.2.9.attention.self.key.bias\", \"module.segformer.encoder.block.2.9.attention.self.value.weight\", \"module.segformer.encoder.block.2.9.attention.self.value.bias\", \"module.segformer.encoder.block.2.9.attention.self.sr.weight\", \"module.segformer.encoder.block.2.9.attention.self.sr.bias\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.9.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.9.attention.output.dense.weight\", \"module.segformer.encoder.block.2.9.attention.output.dense.bias\", \"module.segformer.encoder.block.2.9.layer_norm_2.weight\", \"module.segformer.encoder.block.2.9.layer_norm_2.bias\", \"module.segformer.encoder.block.2.9.mlp.dense1.weight\", \"module.segformer.encoder.block.2.9.mlp.dense1.bias\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.9.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.9.mlp.dense2.weight\", \"module.segformer.encoder.block.2.9.mlp.dense2.bias\", \"module.segformer.encoder.block.2.10.layer_norm_1.weight\", \"module.segformer.encoder.block.2.10.layer_norm_1.bias\", \"module.segformer.encoder.block.2.10.attention.self.query.weight\", \"module.segformer.encoder.block.2.10.attention.self.query.bias\", \"module.segformer.encoder.block.2.10.attention.self.key.weight\", \"module.segformer.encoder.block.2.10.attention.self.key.bias\", \"module.segformer.encoder.block.2.10.attention.self.value.weight\", \"module.segformer.encoder.block.2.10.attention.self.value.bias\", \"module.segformer.encoder.block.2.10.attention.self.sr.weight\", \"module.segformer.encoder.block.2.10.attention.self.sr.bias\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.10.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.10.attention.output.dense.weight\", \"module.segformer.encoder.block.2.10.attention.output.dense.bias\", \"module.segformer.encoder.block.2.10.layer_norm_2.weight\", \"module.segformer.encoder.block.2.10.layer_norm_2.bias\", \"module.segformer.encoder.block.2.10.mlp.dense1.weight\", \"module.segformer.encoder.block.2.10.mlp.dense1.bias\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.10.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.10.mlp.dense2.weight\", \"module.segformer.encoder.block.2.10.mlp.dense2.bias\", \"module.segformer.encoder.block.2.11.layer_norm_1.weight\", \"module.segformer.encoder.block.2.11.layer_norm_1.bias\", \"module.segformer.encoder.block.2.11.attention.self.query.weight\", \"module.segformer.encoder.block.2.11.attention.self.query.bias\", \"module.segformer.encoder.block.2.11.attention.self.key.weight\", \"module.segformer.encoder.block.2.11.attention.self.key.bias\", \"module.segformer.encoder.block.2.11.attention.self.value.weight\", \"module.segformer.encoder.block.2.11.attention.self.value.bias\", \"module.segformer.encoder.block.2.11.attention.self.sr.weight\", \"module.segformer.encoder.block.2.11.attention.self.sr.bias\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.11.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.11.attention.output.dense.weight\", \"module.segformer.encoder.block.2.11.attention.output.dense.bias\", \"module.segformer.encoder.block.2.11.layer_norm_2.weight\", \"module.segformer.encoder.block.2.11.layer_norm_2.bias\", \"module.segformer.encoder.block.2.11.mlp.dense1.weight\", \"module.segformer.encoder.block.2.11.mlp.dense1.bias\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.11.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.11.mlp.dense2.weight\", \"module.segformer.encoder.block.2.11.mlp.dense2.bias\", \"module.segformer.encoder.block.2.12.layer_norm_1.weight\", \"module.segformer.encoder.block.2.12.layer_norm_1.bias\", \"module.segformer.encoder.block.2.12.attention.self.query.weight\", \"module.segformer.encoder.block.2.12.attention.self.query.bias\", \"module.segformer.encoder.block.2.12.attention.self.key.weight\", \"module.segformer.encoder.block.2.12.attention.self.key.bias\", \"module.segformer.encoder.block.2.12.attention.self.value.weight\", \"module.segformer.encoder.block.2.12.attention.self.value.bias\", \"module.segformer.encoder.block.2.12.attention.self.sr.weight\", \"module.segformer.encoder.block.2.12.attention.self.sr.bias\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.12.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.12.attention.output.dense.weight\", \"module.segformer.encoder.block.2.12.attention.output.dense.bias\", \"module.segformer.encoder.block.2.12.layer_norm_2.weight\", \"module.segformer.encoder.block.2.12.layer_norm_2.bias\", \"module.segformer.encoder.block.2.12.mlp.dense1.weight\", \"module.segformer.encoder.block.2.12.mlp.dense1.bias\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.12.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.12.mlp.dense2.weight\", \"module.segformer.encoder.block.2.12.mlp.dense2.bias\", \"module.segformer.encoder.block.2.13.layer_norm_1.weight\", \"module.segformer.encoder.block.2.13.layer_norm_1.bias\", \"module.segformer.encoder.block.2.13.attention.self.query.weight\", \"module.segformer.encoder.block.2.13.attention.self.query.bias\", \"module.segformer.encoder.block.2.13.attention.self.key.weight\", \"module.segformer.encoder.block.2.13.attention.self.key.bias\", \"module.segformer.encoder.block.2.13.attention.self.value.weight\", \"module.segformer.encoder.block.2.13.attention.self.value.bias\", \"module.segformer.encoder.block.2.13.attention.self.sr.weight\", \"module.segformer.encoder.block.2.13.attention.self.sr.bias\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.13.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.13.attention.output.dense.weight\", \"module.segformer.encoder.block.2.13.attention.output.dense.bias\", \"module.segformer.encoder.block.2.13.layer_norm_2.weight\", \"module.segformer.encoder.block.2.13.layer_norm_2.bias\", \"module.segformer.encoder.block.2.13.mlp.dense1.weight\", \"module.segformer.encoder.block.2.13.mlp.dense1.bias\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.13.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.13.mlp.dense2.weight\", \"module.segformer.encoder.block.2.13.mlp.dense2.bias\", \"module.segformer.encoder.block.2.14.layer_norm_1.weight\", \"module.segformer.encoder.block.2.14.layer_norm_1.bias\", \"module.segformer.encoder.block.2.14.attention.self.query.weight\", \"module.segformer.encoder.block.2.14.attention.self.query.bias\", \"module.segformer.encoder.block.2.14.attention.self.key.weight\", \"module.segformer.encoder.block.2.14.attention.self.key.bias\", \"module.segformer.encoder.block.2.14.attention.self.value.weight\", \"module.segformer.encoder.block.2.14.attention.self.value.bias\", \"module.segformer.encoder.block.2.14.attention.self.sr.weight\", \"module.segformer.encoder.block.2.14.attention.self.sr.bias\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.14.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.14.attention.output.dense.weight\", \"module.segformer.encoder.block.2.14.attention.output.dense.bias\", \"module.segformer.encoder.block.2.14.layer_norm_2.weight\", \"module.segformer.encoder.block.2.14.layer_norm_2.bias\", \"module.segformer.encoder.block.2.14.mlp.dense1.weight\", \"module.segformer.encoder.block.2.14.mlp.dense1.bias\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.14.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.14.mlp.dense2.weight\", \"module.segformer.encoder.block.2.14.mlp.dense2.bias\", \"module.segformer.encoder.block.2.15.layer_norm_1.weight\", \"module.segformer.encoder.block.2.15.layer_norm_1.bias\", \"module.segformer.encoder.block.2.15.attention.self.query.weight\", \"module.segformer.encoder.block.2.15.attention.self.query.bias\", \"module.segformer.encoder.block.2.15.attention.self.key.weight\", \"module.segformer.encoder.block.2.15.attention.self.key.bias\", \"module.segformer.encoder.block.2.15.attention.self.value.weight\", \"module.segformer.encoder.block.2.15.attention.self.value.bias\", \"module.segformer.encoder.block.2.15.attention.self.sr.weight\", \"module.segformer.encoder.block.2.15.attention.self.sr.bias\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.15.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.15.attention.output.dense.weight\", \"module.segformer.encoder.block.2.15.attention.output.dense.bias\", \"module.segformer.encoder.block.2.15.layer_norm_2.weight\", \"module.segformer.encoder.block.2.15.layer_norm_2.bias\", \"module.segformer.encoder.block.2.15.mlp.dense1.weight\", \"module.segformer.encoder.block.2.15.mlp.dense1.bias\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.15.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.15.mlp.dense2.weight\", \"module.segformer.encoder.block.2.15.mlp.dense2.bias\", \"module.segformer.encoder.block.2.16.layer_norm_1.weight\", \"module.segformer.encoder.block.2.16.layer_norm_1.bias\", \"module.segformer.encoder.block.2.16.attention.self.query.weight\", \"module.segformer.encoder.block.2.16.attention.self.query.bias\", \"module.segformer.encoder.block.2.16.attention.self.key.weight\", \"module.segformer.encoder.block.2.16.attention.self.key.bias\", \"module.segformer.encoder.block.2.16.attention.self.value.weight\", \"module.segformer.encoder.block.2.16.attention.self.value.bias\", \"module.segformer.encoder.block.2.16.attention.self.sr.weight\", \"module.segformer.encoder.block.2.16.attention.self.sr.bias\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.16.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.16.attention.output.dense.weight\", \"module.segformer.encoder.block.2.16.attention.output.dense.bias\", \"module.segformer.encoder.block.2.16.layer_norm_2.weight\", \"module.segformer.encoder.block.2.16.layer_norm_2.bias\", \"module.segformer.encoder.block.2.16.mlp.dense1.weight\", \"module.segformer.encoder.block.2.16.mlp.dense1.bias\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.16.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.16.mlp.dense2.weight\", \"module.segformer.encoder.block.2.16.mlp.dense2.bias\", \"module.segformer.encoder.block.2.17.layer_norm_1.weight\", \"module.segformer.encoder.block.2.17.layer_norm_1.bias\", \"module.segformer.encoder.block.2.17.attention.self.query.weight\", \"module.segformer.encoder.block.2.17.attention.self.query.bias\", \"module.segformer.encoder.block.2.17.attention.self.key.weight\", \"module.segformer.encoder.block.2.17.attention.self.key.bias\", \"module.segformer.encoder.block.2.17.attention.self.value.weight\", \"module.segformer.encoder.block.2.17.attention.self.value.bias\", \"module.segformer.encoder.block.2.17.attention.self.sr.weight\", \"module.segformer.encoder.block.2.17.attention.self.sr.bias\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.weight\", \"module.segformer.encoder.block.2.17.attention.self.layer_norm.bias\", \"module.segformer.encoder.block.2.17.attention.output.dense.weight\", \"module.segformer.encoder.block.2.17.attention.output.dense.bias\", \"module.segformer.encoder.block.2.17.layer_norm_2.weight\", \"module.segformer.encoder.block.2.17.layer_norm_2.bias\", \"module.segformer.encoder.block.2.17.mlp.dense1.weight\", \"module.segformer.encoder.block.2.17.mlp.dense1.bias\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.2.17.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.2.17.mlp.dense2.weight\", \"module.segformer.encoder.block.2.17.mlp.dense2.bias\", \"module.segformer.encoder.block.3.0.layer_norm_1.weight\", \"module.segformer.encoder.block.3.0.layer_norm_1.bias\", \"module.segformer.encoder.block.3.0.attention.self.query.weight\", \"module.segformer.encoder.block.3.0.attention.self.query.bias\", \"module.segformer.encoder.block.3.0.attention.self.key.weight\", \"module.segformer.encoder.block.3.0.attention.self.key.bias\", \"module.segformer.encoder.block.3.0.attention.self.value.weight\", \"module.segformer.encoder.block.3.0.attention.self.value.bias\", \"module.segformer.encoder.block.3.0.attention.output.dense.weight\", \"module.segformer.encoder.block.3.0.attention.output.dense.bias\", \"module.segformer.encoder.block.3.0.layer_norm_2.weight\", \"module.segformer.encoder.block.3.0.layer_norm_2.bias\", \"module.segformer.encoder.block.3.0.mlp.dense1.weight\", \"module.segformer.encoder.block.3.0.mlp.dense1.bias\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.0.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.0.mlp.dense2.weight\", \"module.segformer.encoder.block.3.0.mlp.dense2.bias\", \"module.segformer.encoder.block.3.1.layer_norm_1.weight\", \"module.segformer.encoder.block.3.1.layer_norm_1.bias\", \"module.segformer.encoder.block.3.1.attention.self.query.weight\", \"module.segformer.encoder.block.3.1.attention.self.query.bias\", \"module.segformer.encoder.block.3.1.attention.self.key.weight\", \"module.segformer.encoder.block.3.1.attention.self.key.bias\", \"module.segformer.encoder.block.3.1.attention.self.value.weight\", \"module.segformer.encoder.block.3.1.attention.self.value.bias\", \"module.segformer.encoder.block.3.1.attention.output.dense.weight\", \"module.segformer.encoder.block.3.1.attention.output.dense.bias\", \"module.segformer.encoder.block.3.1.layer_norm_2.weight\", \"module.segformer.encoder.block.3.1.layer_norm_2.bias\", \"module.segformer.encoder.block.3.1.mlp.dense1.weight\", \"module.segformer.encoder.block.3.1.mlp.dense1.bias\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.1.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.1.mlp.dense2.weight\", \"module.segformer.encoder.block.3.1.mlp.dense2.bias\", \"module.segformer.encoder.block.3.2.layer_norm_1.weight\", \"module.segformer.encoder.block.3.2.layer_norm_1.bias\", \"module.segformer.encoder.block.3.2.attention.self.query.weight\", \"module.segformer.encoder.block.3.2.attention.self.query.bias\", \"module.segformer.encoder.block.3.2.attention.self.key.weight\", \"module.segformer.encoder.block.3.2.attention.self.key.bias\", \"module.segformer.encoder.block.3.2.attention.self.value.weight\", \"module.segformer.encoder.block.3.2.attention.self.value.bias\", \"module.segformer.encoder.block.3.2.attention.output.dense.weight\", \"module.segformer.encoder.block.3.2.attention.output.dense.bias\", \"module.segformer.encoder.block.3.2.layer_norm_2.weight\", \"module.segformer.encoder.block.3.2.layer_norm_2.bias\", \"module.segformer.encoder.block.3.2.mlp.dense1.weight\", \"module.segformer.encoder.block.3.2.mlp.dense1.bias\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.weight\", \"module.segformer.encoder.block.3.2.mlp.dwconv.dwconv.bias\", \"module.segformer.encoder.block.3.2.mlp.dense2.weight\", \"module.segformer.encoder.block.3.2.mlp.dense2.bias\", \"module.segformer.encoder.layer_norm.0.weight\", \"module.segformer.encoder.layer_norm.0.bias\", \"module.segformer.encoder.layer_norm.1.weight\", \"module.segformer.encoder.layer_norm.1.bias\", \"module.segformer.encoder.layer_norm.2.weight\", \"module.segformer.encoder.layer_norm.2.bias\", \"module.segformer.encoder.layer_norm.3.weight\", \"module.segformer.encoder.layer_norm.3.bias\", \"module.decode_head.linear_c.0.proj.weight\", \"module.decode_head.linear_c.0.proj.bias\", \"module.decode_head.linear_c.1.proj.weight\", \"module.decode_head.linear_c.1.proj.bias\", \"module.decode_head.linear_c.2.proj.weight\", \"module.decode_head.linear_c.2.proj.bias\", \"module.decode_head.linear_c.3.proj.weight\", \"module.decode_head.linear_c.3.proj.bias\", \"module.decode_head.linear_fuse.weight\", \"module.decode_head.batch_norm.weight\", \"module.decode_head.batch_norm.bias\", \"module.decode_head.batch_norm.running_mean\", \"module.decode_head.batch_norm.running_var\", \"module.decode_head.batch_norm.num_batches_tracked\", \"module.decode_head.classifier.weight\", \"module.decode_head.classifier.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def predict(image, model, processor):\n",
    "    \"\"\"\n",
    "    Make prediction for a single image\n",
    "    \n",
    "    Args:\n",
    "        image: numpy array of shape (48, 48, 1)\n",
    "        model: trained SegFormer model\n",
    "        processor: SegFormer processor\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (48, 48) with probability predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert image to RGB\n",
    "        image_rgb = np.repeat(image.reshape(48, 48, 1), 3, axis=-1)\n",
    "        \n",
    "        # Process image\n",
    "        inputs = processor(\n",
    "            images=image_rgb,\n",
    "            return_tensors=\"pt\",\n",
    "            do_rescale=False\n",
    "        )\n",
    "        \n",
    "        # Move to GPU\n",
    "        pixel_values = inputs['pixel_values'].cuda()\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert to probabilities and move to CPU\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = probs[0, 1].cpu().numpy()  # Take class 1 probability\n",
    "        \n",
    "        return pred\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b3\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").cuda()\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Create resize transform\n",
    "resize_transform = Resize(\n",
    "    size=(48, 48),\n",
    "    interpolation=InterpolationMode.BILINEAR,\n",
    "    antialias=True\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "X_test_reshaped = X_test.reshape(-1, 48, 48, 1)\n",
    "X_test_normalized = X_test_reshaped / 255.0\n",
    "\n",
    "print(\"Making predictions on test set...\")\n",
    "predictions = np.zeros((len(X_test_normalized), 48, 48))\n",
    "\n",
    "for i, image in enumerate(X_test_normalized):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Predicting image {i}/{len(X_test_normalized)}\")\n",
    "    pred = predict(image, model, processor)\n",
    "    \n",
    "    # Resize prediction using torchvision if necessary\n",
    "    if pred.shape != (48, 48):\n",
    "        pred_tensor = torch.from_numpy(pred).unsqueeze(0)  # Add batch and channel dims\n",
    "        pred_resized = resize_transform(pred_tensor).squeeze().numpy()\n",
    "        predictions[i] = pred_resized\n",
    "    else:\n",
    "        predictions[i] = pred\n",
    "\n",
    "print(f\"Ground truth shape: {y_test.reshape(-1, 48, 48).shape}\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "y_true_flat = y_test.reshape(-1, 48, 48).flatten()\n",
    "y_pred_flat = predictions.flatten()\n",
    "y_pred_flat = (y_pred_flat > 0.5).astype(int)\n",
    "\n",
    "print(f\"Flattened shapes - True: {y_true_flat.shape}, Pred: {y_pred_flat.shape}\")\n",
    "balanced_acc = balanced_accuracy_score(y_true_flat, y_pred_flat)\n",
    "print(f\"Final Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "tp = np.sum((y_true_flat == 1) & (y_pred_flat == 1))\n",
    "tn = np.sum((y_true_flat == 0) & (y_pred_flat == 0))\n",
    "fp = np.sum((y_true_flat == 0) & (y_pred_flat == 1))\n",
    "fn = np.sum((y_true_flat == 1) & (y_pred_flat == 0))\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:05:04.660592Z",
     "iopub.status.busy": "2024-10-26T19:05:04.660185Z",
     "iopub.status.idle": "2024-10-26T19:05:15.158500Z",
     "shell.execute_reply": "2024-10-26T19:05:15.157457Z",
     "shell.execute_reply.started": "2024-10-26T19:05:04.660553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29/2240498336.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on test set...\n",
      "Predicting image 0/110\n",
      "Predicting image 20/110\n",
      "Predicting image 40/110\n",
      "Predicting image 60/110\n",
      "Predicting image 80/110\n",
      "Predicting image 100/110\n",
      "Ground truth shape: (110, 48, 48)\n",
      "Predictions shape: (110, 48, 48)\n",
      "Flattened shapes - True: (253440,), Pred: (253440,)\n",
      "Final Balanced Accuracy: 0.8258\n",
      "\n",
      "Detailed Metrics:\n",
      "Sensitivity (True Positive Rate): 0.7271\n",
      "Specificity (True Negative Rate): 0.9246\n",
      "True Positives: 54176\n",
      "True Negatives: 165435\n",
      "False Positives: 13495\n",
      "False Negatives: 20334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def remove_module_prefix(state_dict):\n",
    "    \"\"\"Remove 'module.' prefix from state_dict keys if present\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('module.'):\n",
    "            new_key = key[7:]  # Remove 'module.' prefix\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "def predict(image, model, processor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_rgb = np.repeat(image.reshape(48, 48, 1), 3, axis=-1)\n",
    "        inputs = processor(\n",
    "            images=image_rgb,\n",
    "            return_tensors=\"pt\",\n",
    "            do_rescale=False\n",
    "        )\n",
    "        pixel_values = inputs['pixel_values'].cuda()\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = probs[0, 1].cpu().numpy()\n",
    "        return pred\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b3\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").cuda()\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "state_dict = remove_module_prefix(checkpoint['model_state_dict'])\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Create resize transform\n",
    "resize_transform = Resize(\n",
    "    size=(48, 48),\n",
    "    interpolation=InterpolationMode.BILINEAR,\n",
    "    antialias=True\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "X_test_reshaped = X_test.reshape(-1, 48, 48, 1)\n",
    "X_test_normalized = X_test_reshaped / 255.0\n",
    "\n",
    "print(\"Making predictions on test set...\")\n",
    "predictions = np.zeros((len(X_test_normalized), 48, 48))\n",
    "\n",
    "for i, image in enumerate(X_test_normalized):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Predicting image {i}/{len(X_test_normalized)}\")\n",
    "    pred = predict(image, model, processor)\n",
    "    \n",
    "    if pred.shape != (48, 48):\n",
    "        pred_tensor = torch.from_numpy(pred).unsqueeze(0)\n",
    "        pred_resized = resize_transform(pred_tensor).squeeze().numpy()\n",
    "        predictions[i] = pred_resized\n",
    "    else:\n",
    "        predictions[i] = pred\n",
    "\n",
    "print(f\"Ground truth shape: {y_test.reshape(-1, 48, 48).shape}\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Calculate metrics\n",
    "y_true_flat = y_test.reshape(-1, 48, 48).flatten()\n",
    "y_pred_flat = predictions.flatten()\n",
    "y_pred_flat = (y_pred_flat > 0.5).astype(int)\n",
    "\n",
    "print(f\"Flattened shapes - True: {y_true_flat.shape}, Pred: {y_pred_flat.shape}\")\n",
    "balanced_acc = balanced_accuracy_score(y_true_flat, y_pred_flat)\n",
    "print(f\"Final Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "tp = np.sum((y_true_flat == 1) & (y_pred_flat == 1))\n",
    "tn = np.sum((y_true_flat == 0) & (y_pred_flat == 0))\n",
    "fp = np.sum((y_true_flat == 0) & (y_pred_flat == 1))\n",
    "fn = np.sum((y_true_flat == 1) & (y_pred_flat == 0))\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:12:52.010247Z",
     "iopub.status.busy": "2024-10-26T19:12:52.009559Z",
     "iopub.status.idle": "2024-10-26T19:12:52.051056Z",
     "shell.execute_reply": "2024-10-26T19:12:52.050062Z",
     "shell.execute_reply.started": "2024-10-26T19:12:52.010208Z"
    }
   },
   "outputs": [],
   "source": [
    "X_final = np.load('/kaggle/input/crater-segmentation/Xtest2_b.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:13:17.189218Z",
     "iopub.status.busy": "2024-10-26T19:13:17.188831Z",
     "iopub.status.idle": "2024-10-26T19:13:34.352979Z",
     "shell.execute_reply": "2024-10-26T19:13:34.351734Z",
     "shell.execute_reply.started": "2024-10-26T19:13:17.189180Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29/2996944383.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on X_final...\n",
      "Predicting image 0/196\n",
      "Predicting image 20/196\n",
      "Predicting image 40/196\n",
      "Predicting image 60/196\n",
      "Predicting image 80/196\n",
      "Predicting image 100/196\n",
      "Predicting image 120/196\n",
      "Predicting image 140/196\n",
      "Predicting image 160/196\n",
      "Predicting image 180/196\n",
      "Predictions saved to predictions_final.npy\n",
      "\n",
      "Prediction Statistics:\n",
      "Shape: (196, 48, 48)\n",
      "Number of positive predictions: 114312\n",
      "Number of negative predictions: 337272\n",
      "Percentage of positive predictions: 25.31%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "def remove_module_prefix(state_dict):\n",
    "    \"\"\"Remove 'module.' prefix from state_dict keys if present\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('module.'):\n",
    "            new_key = key[7:]  # Remove 'module.' prefix\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "def predict(image, model, processor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_rgb = np.repeat(image.reshape(48, 48, 1), 3, axis=-1)\n",
    "        inputs = processor(\n",
    "            images=image_rgb,\n",
    "            return_tensors=\"pt\",\n",
    "            do_rescale=False\n",
    "        )\n",
    "        pixel_values = inputs['pixel_values'].cuda()\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = probs[0, 1].cpu().numpy()\n",
    "        return pred\n",
    "\n",
    "# Load and prepare X_final\n",
    "X_final = np.load('/kaggle/input/crater-segmentation/Xtest2_b.npy')\n",
    "X_final_normalized = X_final / 255.0\n",
    "\n",
    "# Initialize model and processor\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b3\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").cuda()\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "state_dict = remove_module_prefix(checkpoint['model_state_dict'])\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Create resize transform\n",
    "resize_transform = Resize(\n",
    "    size=(48, 48),\n",
    "    interpolation=InterpolationMode.BILINEAR,\n",
    "    antialias=True\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions on X_final...\")\n",
    "predictions = np.zeros((len(X_final_normalized), 48, 48))\n",
    "\n",
    "for i, image in enumerate(X_final_normalized):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Predicting image {i}/{len(X_final_normalized)}\")\n",
    "    pred = predict(image, model, processor)\n",
    "    \n",
    "    if pred.shape != (48, 48):\n",
    "        pred_tensor = torch.from_numpy(pred).unsqueeze(0)\n",
    "        pred_resized = resize_transform(pred_tensor).squeeze().numpy()\n",
    "        predictions[i] = pred_resized\n",
    "    else:\n",
    "        predictions[i] = pred\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.5).astype(np.uint8)\n",
    "\n",
    "# Save predictions\n",
    "#np.save('predictions_final.npy', predictions_binary)\n",
    "print(\"Predictions saved to predictions_final.npy\")\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(f\"Shape: {predictions_binary.shape}\")\n",
    "print(f\"Number of positive predictions: {np.sum(predictions_binary == 1)}\")\n",
    "print(f\"Number of negative predictions: {np.sum(predictions_binary == 0)}\")\n",
    "print(f\"Percentage of positive predictions: {100 * np.mean(predictions_binary):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:14:30.694888Z",
     "iopub.status.busy": "2024-10-26T19:14:30.694513Z",
     "iopub.status.idle": "2024-10-26T19:14:30.699635Z",
     "shell.execute_reply": "2024-10-26T19:14:30.698615Z",
     "shell.execute_reply.started": "2024-10-26T19:14:30.694853Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_binary = predictions_binary.reshape(196,48**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:14:39.701700Z",
     "iopub.status.busy": "2024-10-26T19:14:39.700812Z",
     "iopub.status.idle": "2024-10-26T19:14:39.707210Z",
     "shell.execute_reply": "2024-10-26T19:14:39.706317Z",
     "shell.execute_reply.started": "2024-10-26T19:14:39.701658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 2304)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T19:15:12.184775Z",
     "iopub.status.busy": "2024-10-26T19:15:12.184387Z",
     "iopub.status.idle": "2024-10-26T19:15:12.189829Z",
     "shell.execute_reply": "2024-10-26T19:15:12.188768Z",
     "shell.execute_reply.started": "2024-10-26T19:15:12.184739Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('final_predictions.npy',predictions_binary) ##FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "Xtrain2_a = np.load('/kaggle/input/datasetmachinelearning4/Xtrain2_a.npy')\n",
    "Ytrain2_a = np.load('/kaggle/input/datasetmachinelearning4/Ytrain2_a.npy')\n",
    "Xtrain2_b = np.load('/kaggle/input/datasetmachinelearning4/Xtrain2_b.npy')\n",
    "Ytrain2_b = np.load('/kaggle/input/datasetmachinelearning4/Ytrain2_b.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "# Simple CNN model\n",
    "model_a = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(7, 7, 1)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_a.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def unet_model(input_size=(48, 48, 1)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    \n",
    "    # Decoder\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    merge4 = concatenate([conv2, up4], axis=3)\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge4)\n",
    "    \n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    merge5 = concatenate([conv1, up5], axis=3)\n",
    "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge5)\n",
    "    \n",
    "    conv6 = Conv2D(1, (1, 1), activation='sigmoid')(conv5)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=conv6)\n",
    "\n",
    "model_b = unet_model()\n",
    "model_b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reshape Xtrain2_a to have the shape (num_samples, 7, 7, 1)\n",
    "Xtrain2_a = Xtrain2_a.reshape(-1, 7, 7, 1)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "# Simple CNN model for format a\n",
    "model_a = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(7, 7, 1)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_a.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "model_a.fit(Xtrain2_a, Ytrain2_a, epochs=10, batch_size=32)\n",
    "model_b.fit(Xtrain2_b, Ytrain2_b, epochs=10, batch_size=32)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Predict\n",
    "preds_a = model_a.predict(Xtrain2_a).flatten()\n",
    "preds_b = model_b.predict(Xtrain2_b).flatten()\n",
    "\n",
    "# Round predictions to get binary outputs\n",
    "preds_a_rounded = np.round(preds_a)\n",
    "preds_b_rounded = np.round(preds_b)\n",
    "\n",
    "# Compute balanced accuracy\n",
    "balanced_acc_a = balanced_accuracy_score(Ytrain2_a.flatten(), preds_a_rounded)\n",
    "balanced_acc_b = balanced_accuracy_score(Ytrain2_b.flatten(), preds_b_rounded)\n",
    "\n",
    "print(\"Balanced Accuracy for Format A:\", balanced_acc_a)\n",
    "print(\"Balanced Accuracy for Format B:\", balanced_acc_b)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5953135,
     "sourceId": 9730184,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
